<!doctype html><html lang="en"><head><title data-rh="true">How To Train Your Siamese Neural Network | by Cameron Trotter | Towards Data Science</title><meta data-rh="true" charset="utf-8"/><meta data-rh="true" name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1,maximum-scale=1"/><meta data-rh="true" name="theme-color" content="#000000"/><meta data-rh="true" name="twitter:app:name:iphone" content="Medium"/><meta data-rh="true" name="twitter:app:id:iphone" content="828256236"/><meta data-rh="true" property="al:ios:app_name" content="Medium"/><meta data-rh="true" property="al:ios:app_store_id" content="828256236"/><meta data-rh="true" property="al:android:package" content="com.medium.reader"/><meta data-rh="true" property="fb:app_id" content="542599432471018"/><meta data-rh="true" property="og:site_name" content="Medium"/><meta data-rh="true" property="og:type" content="article"/><meta data-rh="true" property="article:published_time" content="2021-03-03T20:49:53.162Z"/><meta data-rh="true" name="title" content="How To Train Your Siamese Neural Network | by Cameron Trotter | Towards Data Science"/><meta data-rh="true" property="og:title" content="How To Train Your Siamese Neural Network"/><meta data-rh="true" property="al:android:url" content="medium://p/4c6da3259463"/><meta data-rh="true" property="al:ios:url" content="medium://p/4c6da3259463"/><meta data-rh="true" property="al:android:app_name" content="Medium"/><meta data-rh="true" name="description" content="When you first start out with machine learning, it becomes clear from the offset that massive amounts of data are required for accurate and robust model training. And whilst this is true, when…"/><meta data-rh="true" property="og:description" content="When you first start out with machine learning, it becomes clear from the offset that massive amounts of data are required for accurate…"/><meta data-rh="true" property="og:url" content="https://towardsdatascience.com/how-to-train-your-siamese-neural-network-4c6da3259463"/><meta data-rh="true" property="al:web:url" content="https://towardsdatascience.com/how-to-train-your-siamese-neural-network-4c6da3259463"/><meta data-rh="true" property="og:image" content="https://miro.medium.com/v2/resize:fill:903:475/g:fp:0.47:0.49/1*bJABur9wzFNACosQkim8kw.png"/><meta data-rh="true" property="og:image:alt" content="Left: 3 input imgs labelled anchor, positive, negative. Centre: An SNN embedding the imgs. Right: The images embedded, arrows showing positive and negative are pushing away, positive and centre are getting closer."/><meta data-rh="true" property="article:author" content="https://medium.com/@c.trotter2"/><meta data-rh="true" name="author" content="Cameron Trotter"/><meta data-rh="true" name="robots" content="index,follow,max-image-preview:large"/><meta data-rh="true" name="referrer" content="unsafe-url"/><meta data-rh="true" property="twitter:title" content="How To Train Your Siamese Neural Network"/><meta data-rh="true" name="twitter:site" content="@TDataScience"/><meta data-rh="true" name="twitter:app:url:iphone" content="medium://p/4c6da3259463"/><meta data-rh="true" property="twitter:description" content="When you first start out with machine learning, it becomes clear from the offset that massive amounts of data are required for accurate…"/><meta data-rh="true" name="twitter:image:src" content="https://miro.medium.com/v2/resize:fill:903:475/g:fp:0.47:0.49/1*bJABur9wzFNACosQkim8kw.png"/><meta data-rh="true" name="twitter:image:alt" content="Left: 3 input imgs labelled anchor, positive, negative. Centre: An SNN embedding the imgs. Right: The images embedded, arrows showing positive and negative are pushing away, positive and centre are getting closer."/><meta data-rh="true" name="twitter:card" content="summary_large_image"/><meta data-rh="true" name="twitter:label1" content="Reading time"/><meta data-rh="true" name="twitter:data1" content="20 min read"/><meta data-rh="true" name="twitter:tile:template:testing" content="2"/><meta data-rh="true" name="twitter:tile:image" content="https://miro.medium.com/v2/resize:fill:903:475/g:fp:0.47:0.49/1*bJABur9wzFNACosQkim8kw.png"/><meta data-rh="true" name="twitter:tile:image:alt" content="Left: 3 input imgs labelled anchor, positive, negative. Centre: An SNN embedding the imgs. Right: The images embedded, arrows showing positive and negative are pushing away, positive and centre are getting closer."/><meta data-rh="true" name="twitter:tile:info1:icon" content="Person"/><meta data-rh="true" name="twitter:tile:info1:text" content="Cameron Trotter"/><meta data-rh="true" name="twitter:tile:info2:icon" content="Calendar"/><meta data-rh="true" name="twitter:tile:info2:text" content="Mar 3, 2021"/><meta data-rh="true" name="twitter:cta" content="Read on Medium"/><link data-rh="true" rel="icon" href="https://miro.medium.com/v2/resize:fill:256:256/1*VzTUkfeGymHP4Bvav-T-lA.png"/><link data-rh="true" rel="search" type="application/opensearchdescription+xml" title="Medium" href="/osd.xml"/><link data-rh="true" rel="apple-touch-icon" sizes="152x152" href="https://miro.medium.com/v2/resize:fill:152:152/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"/><link data-rh="true" rel="apple-touch-icon" sizes="120x120" href="https://miro.medium.com/v2/resize:fill:120:120/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"/><link data-rh="true" rel="apple-touch-icon" sizes="76x76" href="https://miro.medium.com/v2/resize:fill:76:76/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"/><link data-rh="true" rel="apple-touch-icon" sizes="60x60" href="https://miro.medium.com/v2/resize:fill:60:60/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"/><link data-rh="true" rel="mask-icon" href="https://cdn-static-1.medium.com/_/fp/icons/Medium-Avatar-500x500.svg" color="#171717"/><link data-rh="true" rel="preconnect" href="https://glyph.medium.com" crossOrigin=""/><link data-rh="true" id="glyph_preload_link" rel="preload" as="style" type="text/css" href="https://glyph.medium.com/css/unbound.css"/><link data-rh="true" id="glyph_link" rel="stylesheet" type="text/css" href="https://glyph.medium.com/css/unbound.css"/><link data-rh="true" rel="author" href="https://medium.com/@c.trotter2"/><link data-rh="true" rel="canonical" href="https://towardsdatascience.com/how-to-train-your-siamese-neural-network-4c6da3259463"/><link data-rh="true" rel="alternate" href="android-app://com.medium.reader/https/medium.com/p/4c6da3259463"/><script data-rh="true" type="application/ld+json">{"@context":"http:\u002F\u002Fschema.org","@type":"NewsArticle","image":["https:\u002F\u002Fmiro.medium.com\u002Fv2\u002Fresize:fit:1200\u002F1*bJABur9wzFNACosQkim8kw.png"],"url":"https:\u002F\u002Ftowardsdatascience.com\u002Fhow-to-train-your-siamese-neural-network-4c6da3259463","dateCreated":"2021-02-19T12:33:32.337Z","datePublished":"2021-02-19T12:33:32.337Z","dateModified":"2022-03-30T21:15:38.175Z","headline":"How To Train Your Siamese Neural Network - Towards Data Science","name":"How To Train Your Siamese Neural Network - Towards Data Science","description":"When you first start out with machine learning, it becomes clear from the offset that massive amounts of data are required for accurate and robust model training. And whilst this is true, when…","identifier":"4c6da3259463","author":{"@type":"Person","name":"Cameron Trotter","url":"https:\u002F\u002Ftowardsdatascience.com\u002F@c.trotter2"},"creator":["Cameron Trotter"],"publisher":{"@type":"Organization","name":"Towards Data Science","url":"towardsdatascience.com","logo":{"@type":"ImageObject","width":192,"height":60,"url":"https:\u002F\u002Fmiro.medium.com\u002Fv2\u002Fresize:fit:384\u002F1*cFFKn8rFH4ZndmaYeAs6iQ.png"}},"mainEntityOfPage":"https:\u002F\u002Ftowardsdatascience.com\u002Fhow-to-train-your-siamese-neural-network-4c6da3259463"}</script><style type="text/css" data-fela-rehydration="723" data-fela-type="STATIC">html{box-sizing:border-box;-webkit-text-size-adjust:100%}*, *:before, *:after{box-sizing:inherit}body{margin:0;padding:0;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;color:rgba(0,0,0,0.8);position:relative;min-height:100vh}h1, h2, h3, h4, h5, h6, dl, dd, ol, ul, menu, figure, blockquote, p, pre, form{margin:0}menu, ol, ul{padding:0;list-style:none;list-style-image:none}main{display:block}a{color:inherit;text-decoration:none}a, button, input{-webkit-tap-highlight-color:transparent}img, svg{vertical-align:middle}button{background:transparent;overflow:visible}button, input, optgroup, select, textarea{margin:0}:root{--reach-tabs:1;--reach-menu-button:1}#speechify-root{font-family:Sohne, sans-serif}div[data-popper-reference-hidden="true"]{visibility:hidden;pointer-events:none}
/*XCode style (c) Angel Garcia <angelgarcia.mail@gmail.com>*/.hljs {background: #fff;color: black;
}/* Gray DOCTYPE selectors like WebKit */
.xml .hljs-meta {color: #c0c0c0;
}.hljs-comment,
.hljs-quote {color: #007400;
}.hljs-tag,
.hljs-attribute,
.hljs-keyword,
.hljs-selector-tag,
.hljs-literal,
.hljs-name {color: #aa0d91;
}.hljs-variable,
.hljs-template-variable {color: #3F6E74;
}.hljs-code,
.hljs-string,
.hljs-meta .hljs-string {color: #c41a16;
}.hljs-regexp,
.hljs-link {color: #0E0EFF;
}.hljs-title,
.hljs-symbol,
.hljs-bullet,
.hljs-number {color: #1c00cf;
}.hljs-section,
.hljs-meta {color: #643820;
}.hljs-title.class_,
.hljs-class .hljs-title,
.hljs-type,
.hljs-built_in,
.hljs-params {color: #5c2699;
}.hljs-attr {color: #836C28;
}.hljs-subst {color: #000;
}.hljs-formula {background-color: #eee;font-style: italic;
}.hljs-addition {background-color: #baeeba;
}.hljs-deletion {background-color: #ffc8bd;
}.hljs-selector-id,
.hljs-selector-class {color: #9b703f;
}.hljs-doctag,
.hljs-strong {font-weight: bold;
}.hljs-emphasis {font-style: italic;
}
</style><style type="text/css" data-fela-rehydration="723" data-fela-type="KEYFRAME">@-webkit-keyframes k1{0%{opacity:0.8}50%{opacity:0.5}100%{opacity:0.8}}@-moz-keyframes k1{0%{opacity:0.8}50%{opacity:0.5}100%{opacity:0.8}}@keyframes k1{0%{opacity:0.8}50%{opacity:0.5}100%{opacity:0.8}}@-webkit-keyframes k2{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}@-moz-keyframes k2{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}@keyframes k2{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}</style><style type="text/css" data-fela-rehydration="723" data-fela-type="RULE">.a{font-family:medium-content-sans-serif-font, -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Open Sans", "Helvetica Neue", sans-serif}.b{font-weight:400}.c{background-color:rgba(255, 255, 255, 1)}.l{display:block}.m{position:sticky}.n{top:0}.o{z-index:500}.p{padding:0 24px}.q{align-items:center}.r{border-bottom:solid 1px #F2F2F2}.y{height:41px}.z{line-height:20px}.ab{display:flex}.ac{height:57px}.ae{flex:1 0 auto}.af{color:inherit}.ag{fill:inherit}.ah{font-size:inherit}.ai{border:inherit}.aj{font-family:inherit}.ak{letter-spacing:inherit}.al{font-weight:inherit}.am{padding:0}.an{margin:0}.ao{cursor:pointer}.ap:disabled{cursor:not-allowed}.aq:disabled{color:#6B6B6B}.ar:disabled{fill:#6B6B6B}.au{fill:rgba(0, 0, 0, 1)}.av{height:22px}.aw{margin-left:16px}.ax{border:none}.ay{border-radius:20px}.az{width:240px}.ba{background:#F9F9F9}.bb path{fill:#6B6B6B}.bd{outline:none}.be{font-family:sohne, "Helvetica Neue", Helvetica, Arial, sans-serif}.bf{font-size:14px}.bg{width:100%}.bh{padding:10px 20px 10px 0}.bi{background-color:transparent}.bj{color:#242424}.bk::placeholder{color:#6B6B6B}.bl{display:inline-block}.bm{margin-left:12px}.bn{margin-right:12px}.bo{border-radius:4px}.bp{margin-left:24px}.bq{height:24px}.bw{background-color:#F9F9F9}.bx{border-radius:50%}.by{height:32px}.bz{width:32px}.ca{justify-content:center}.cg{max-width:680px}.ch{min-width:0}.ci{animation:k1 1.2s ease-in-out infinite}.cj{height:100vh}.ck{margin-bottom:16px}.cl{margin-top:48px}.cm{align-items:flex-start}.cn{flex-direction:column}.co{justify-content:space-between}.cp{margin-bottom:24px}.cv{width:80%}.cw{background-color:#F2F2F2}.dc{height:44px}.dd{width:44px}.de{margin:auto 0}.df{margin-bottom:4px}.dg{height:16px}.dh{width:120px}.di{width:80px}.do{margin-bottom:8px}.dp{width:96%}.dq{width:98%}.dr{width:81%}.dx{margin:0 24px}.eb{background:rgba(255, 255, 255, 1)}.ec{box-sizing:border-box}.ed{border:1px solid #F2F2F2}.ee{box-shadow:0 1px 4px #F2F2F2}.ef{max-height:100vh}.eg{overflow-y:auto}.eh{position:absolute}.ei{left:0}.ej{top:calc(100vh + 100px)}.ek{bottom:calc(100vh + 100px)}.el{width:10px}.em{pointer-events:none}.eo{word-break:break-word}.ep{word-wrap:break-word}.eq:after{display:block}.er:after{content:""}.es:after{clear:both}.et{line-height:18px}.eu{letter-spacing:0.077em}.ev{font-style:normal}.ew{font-size:13px}.fc{margin-bottom:-0.31em}.fd{color:#6B6B6B}.fe{text-transform:uppercase}.ff{text-decoration:none}.fg{line-height:1.23}.fh{letter-spacing:0}.fi{font-weight:700}.fy{margin-top:12px}.fz{margin-bottom:-0.27em}.ga{line-height:1.394}.gq{@media all and (max-width: 551.98px):8px}.gr{@media all and (min-width: 552px) and (max-width: 727.98px):8px}.gs{@media all and (min-width: 728px) and (max-width: 903.98px):16px}.gt{@media all and (min-width: 904px) and (max-width: 1079.98px):16px}.gu{@media all and (min-width: 1080px):16px}.ha{align-items:baseline}.hb{width:48px}.hc{height:48px}.hd{border:2px solid rgba(255, 255, 255, 1)}.he{z-index:0}.hf{box-shadow:none}.hg{border:1px solid rgba(0, 0, 0, 0.05)}.hi{position:relative}.hj{margin-left:-12px}.hk{width:28px}.hl{height:28px}.hm{z-index:1}.hn{width:24px}.ho{margin-bottom:2px}.hp{flex-wrap:nowrap}.hq{font-size:16px}.hr{line-height:24px}.ht{margin:0 8px}.hu{display:inline}.hv{color:rgba(102, 138, 170, 1)}.hw{fill:rgba(102, 138, 170, 1)}.hx:disabled{opacity:0.3}.ia{flex:0 0 auto}.id{flex-wrap:wrap}.ig{white-space:pre-wrap}.ih{margin-right:4px}.ii{overflow:hidden}.ij{max-height:20px}.ik{text-overflow:ellipsis}.il{display:-webkit-box}.im{-webkit-line-clamp:1}.in{-webkit-box-orient:vertical}.io{word-break:break-all}.iq{padding-left:8px}.ir{padding-right:8px}.js> *{flex-shrink:0}.jt{overflow-x:scroll}.ju::-webkit-scrollbar{display:none}.jv{scrollbar-width:none}.jw{-ms-overflow-style:none}.jz{width:74px}.ka{flex-direction:row}.kd{-webkit-user-select:none}.ke{border:0}.kf{cursor:progress}.kg{fill:rgba(117, 117, 117, 1)}.kj{opacity:0.25}.kk{outline:0}.kl{user-select:none}.km> svg{pointer-events:none}.kv{opacity:1}.kw{padding:4px 0}.kx{fill:#6B6B6B}.la{margin-top:0px}.lb{width:16px}.lc{padding:8px 2px}.ld svg{color:#6B6B6B}.lu{clear:both}.md{margin-left:auto}.me{margin-right:auto}.mf{max-width:4000px}.ml{padding-top:5px}.mm{padding-bottom:5px}.mo{cursor:zoom-in}.mp{z-index:auto}.mr{max-width:100%}.ms{height:auto}.mt{margin-top:10px}.mu{text-align:center}.mv{max-width:728px}.my{text-decoration:underline}.mz{line-height:1.58}.na{letter-spacing:-0.004em}.nb{font-family:source-serif-pro, Georgia, Cambria, "Times New Roman", Times, serif}.nu{margin-bottom:-0.46em}.nv{line-height:1.12}.nw{letter-spacing:-0.022em}.nx{font-weight:600}.oq{margin-bottom:-0.28em}.ow{background:none}.ox{font-style:italic}.oy{max-width:720px}.oz{font-style:inherit}.pa{max-width:400px}.pe{list-style-type:decimal}.pf{margin-left:30px}.pg{padding-left:0px}.pm{line-height:1.18}.qc{max-width:903px}.qd{list-style-type:disc}.qe{max-width:714px}.qf{margin:auto}.qg{padding-bottom:100%}.qh{height:0}.qi{padding:2px 4px}.qj{font-size:75%}.qk> strong{font-family:inherit}.ql{font-family:source-code-pro, Menlo, Monaco, "Courier New", Courier, monospace}.qm{max-width:964px}.qn{max-width:379px}.qo{max-width:969px}.qp{max-width:790px}.qq{margin-top:32px}.qr{margin-bottom:14px}.qs{padding-top:24px}.qt{padding-bottom:10px}.qu{background-color:#000000}.qv{height:3px}.qw{width:3px}.qx{margin-right:20px}.rd{margin-bottom:26px}.re{margin-top:6px}.rf{margin-top:8px}.rg{margin-right:8px}.rh{padding:8px 16px}.ri{border-radius:100px}.rj{transition:background 300ms ease}.rl{white-space:nowrap}.rm{border-top:none}.rs{height:52px}.rt{max-height:52px}.ru{box-sizing:content-box}.rv{position:static}.rx{max-width:155px}.si{align-items:flex-end}.sj{width:76px}.sk{height:76px}.sl{border:2px solid #F9F9F9}.sm{height:72px}.sn{width:72px}.so{margin-left:-16px}.sp{width:36px}.sq{height:36px}.sr{color:#FFFFFF}.ss{fill:#FFFFFF}.st{background:rgba(102, 138, 170, 1)}.su{border-color:rgba(102, 138, 170, 1)}.sy:disabled{cursor:inherit !important}.sz:disabled:hover{background:rgba(102, 138, 170, 1)}.ta:disabled:hover{border-color:rgba(102, 138, 170, 1)}.tb{border-radius:99em}.tc{width:auto}.td{border-width:1px}.te{border-style:solid}.tf{margin-left:8px}.tg{stroke:#F2F2F2}.th{color:#F2F2F2}.ti{fill:#F2F2F2}.tj{background:#F2F2F2}.tk{border-color:#F2F2F2}.tq{font-weight:500}.tr{font-size:24px}.ts{line-height:30px}.tt{letter-spacing:-0.016em}.tu{margin-top:16px}.tv{height:0px}.tw{border-bottom:solid 1px #E5E5E5}.uc{margin-top:72px}.ud{padding:24px 0}.ue{margin-bottom:0px}.uf{margin-right:16px}.ug{display:inline-flex}.uj{margin-bottom:32px}.uk{margin-top:40px}.ul{align-items:stretch}.vt{flex-grow:0}.vz{height:100%}.wa{display:grid}.wb{grid-template-columns:repeat(12, 1fr)}.wc{grid-template-rows:auto 1fr}.wn{grid-area:image}.wo{grid-area:content}.wu{border-radius:2px}.wv{aspect-ratio:2}.ww{object-fit:cover}.wx{object-position:50% 50%}.xd{box-shadow:inset 0 0 0 1px rgba(0, 0, 0, 0.05)}.xe{height:20px}.xf{width:20px}.xg{padding-right:4px}.yb{padding-top:8px}.yc{max-height:40px}.yd{-webkit-line-clamp:2}.yt{margin-left:20px}.yu{margin-left:4px}.yv{justify-content:flex-end}.yw{flex:0 0 0}.yx{object-position:70% 86%}.ze{fill:#242424}.zf{background:0}.zg{border-color:#242424}.zj:disabled:hover{color:#242424}.zk:disabled:hover{fill:#242424}.zl:disabled:hover{border-color:#242424}.abf{padding-bottom:40px}.abg{padding-top:88px}.abh{margin-bottom:40px}.abi{margin-top:4px}.abj{border-right:3px solid #F9F9F9}.abk{z-index:3}.abl{z-index:2}.abm{margin-left:-24px}.abn{margin-left:-36px}.abo{border-radius:0 3px 3px 0}.abp{width:93px}.as:hover:not(:disabled){color:rgba(25, 25, 25, 1)}.at:hover:not(:disabled){fill:rgba(25, 25, 25, 1)}.hh:hover{background-color:rgba(0, 0, 0, 0.1)}.hs:hover{text-decoration:underline}.hy:hover:not(:disabled){color:rgba(90, 118, 144, 1)}.hz:hover:not(:disabled){fill:rgba(90, 118, 144, 1)}.ki:hover{fill:rgba(117, 117, 117, 1)}.ky:hover{fill:#000000}.kz:hover p{color:#000000}.le:hover svg{color:#000000}.rk:hover{background-color:#F2F2F2}.sv:hover{background:rgba(90, 118, 144, 1)}.sw:hover{border-color:rgba(90, 118, 144, 1)}.sx:hover{cursor:pointer}.tl:hover{background:#F2F2F2}.tm:hover{border-color:#F2F2F2}.tn:hover{cursor:wait}.to:hover{color:#F2F2F2}.tp:hover{fill:#F2F2F2}.zh:hover{color:#000000}.zi:hover{border-color:#242424}.bc:focus-within path{fill:#242424}.kh:focus{fill:rgba(117, 117, 117, 1)}.lf:focus svg{color:#000000}.mq:focus{transform:scale(1.01)}.kn:active{border-style:none}</style><style type="text/css" data-fela-rehydration="723" data-fela-type="RULE" media="all and (min-width: 1080px)">.d{display:none}.bv{width:64px}.cf{margin:0 64px}.cu{height:48px}.db{margin-bottom:52px}.dn{margin-bottom:48px}.dw{margin-bottom:68px}.ea{max-width:680px}.fb{margin-top:3.88em}.fv{font-size:42px}.fw{line-height:52px}.fx{letter-spacing:-0.011em}.gn{font-size:22px}.go{margin-top:0.92em}.gp{line-height:28px}.gz{align-items:center}.je{border-top:solid 1px #F2F2F2}.jf{border-bottom:solid 1px #F2F2F2}.jg{margin:32px 0 0}.jh{padding:3px 8px}.jq> *{margin-right:24px}.jr> :last-child{margin-right:0}.jy{display:flex}.ku{margin-top:0px}.mc{max-width:1192px}.mk{margin-top:56px}.nq{font-size:20px}.nr{margin-top:2em}.ns{line-height:32px}.nt{letter-spacing:-0.003em}.om{font-size:24px}.on{margin-top:1.95em}.oo{line-height:30px}.op{letter-spacing:-0.016em}.ov{margin-top:0.86em}.pd{margin-top:2.14em}.pl{margin-top:1.14em}.pz{margin-top:1.72em}.qa{line-height:24px}.qb{letter-spacing:0}.rc{margin-top:1.25em}.rr{margin-bottom:88px}.sc{display:inline-block}.sh{padding-top:72px}.ub{margin-top:40px}.ui{margin:0}.uy{width:calc(100% + 32px)}.uz{margin-left:-16px}.va{margin-right:-16px}.vp{padding-left:16px}.vq{padding-right:16px}.vr{flex-basis:50%}.vs{max-width:50%}.vy{padding-bottom:56px}.wl{gap:24px 0}.wm{grid-template-areas:"image image image image image image image image image image image image" "content content content content content content content content content content content content"}.wt{display:block}.xc{margin-bottom:16px}.xp{padding-bottom:16px}.xq{flex:1 0 auto}.xz{max-height:48px}.ya{-webkit-line-clamp:2}.yi{padding-top:16px}.yr{max-width:56%}.ys{flex:1 0 0}.za{margin-bottom:24px}.zd{flex-direction:row}.zq{width:min-width}.zz{margin-left:16px}.abe{margin-top:96px}.abu{margin-top:24px}.abv{margin-bottom:56px}</style><style type="text/css" data-fela-rehydration="723" data-fela-type="RULE" media="all and (max-width: 1079.98px)">.e{display:none}.kt{margin-top:0px}.mw{margin-left:auto}.mx{text-align:center}.sb{display:inline-block}</style><style type="text/css" data-fela-rehydration="723" data-fela-type="RULE" media="all and (max-width: 903.98px)">.f{display:none}.ks{margin-top:0px}.sa{display:inline-block}</style><style type="text/css" data-fela-rehydration="723" data-fela-type="RULE" media="all and (max-width: 727.98px)">.g{display:none}.kq{margin-top:0px}.kr{margin-right:0px}.rz{display:inline-block}</style><style type="text/css" data-fela-rehydration="723" data-fela-type="RULE" media="all and (max-width: 551.98px)">.h{display:none}.s{display:flex}.t{justify-content:space-between}.br{width:24px}.cb{margin:0 24px}.cq{height:40px}.cx{margin-bottom:44px}.dj{margin-bottom:32px}.ds{margin-bottom:4px}.ex{margin-top:2.64em}.fj{font-size:32px}.fk{line-height:38px}.fl{letter-spacing:-0.014em}.gb{font-size:18px}.gc{margin-top:0.79em}.gd{line-height:24px}.gv{align-items:flex-start}.ib{flex-direction:column}.ie{margin-bottom:2px}.is{margin:24px -24px 0}.it{padding:0}.ji> *{margin-right:8px}.jj> :last-child{margin-right:24px}.kb{margin-left:0px}.ko{margin-top:0px}.kp{margin-right:0px}.lg{border:1px solid #F2F2F2}.lh{border-radius:99em}.li{padding:0px 16px 0px 12px}.lj{height:38px}.lk{align-items:center}.lm svg{margin-right:8px}.lv{margin:0}.lw{max-width:100%}.mg{margin-top:40px}.nc{margin-top:1.56em}.nd{line-height:28px}.ne{letter-spacing:-0.003em}.ny{font-size:20px}.nz{margin-top:1.2em}.oa{letter-spacing:0}.or{margin-top:0.67em}.ph{margin-top:1.34em}.pn{font-size:16px}.po{margin-top:1.23em}.pp{line-height:20px}.qy{margin-top:0.93em}.rn{margin-bottom:80px}.ry{display:inline-block}.sd{padding-top:48px}.tx{margin-top:32px}.um{width:calc(100% + 24px)}.un{margin-left:-12px}.uo{margin-right:-12px}.vb{padding-left:12px}.vc{padding-right:12px}.vd{flex-basis:100%}.vu{padding-bottom:32px}.wd{gap:24px 0}.we{grid-template-areas:"image image image image image image image image image image image image" "content content content content content content content content content content content content"}.wp{display:block}.wy{margin-bottom:16px}.xh{padding-bottom:16px}.xi{flex:1 0 auto}.xr{max-height:48px}.xs{-webkit-line-clamp:2}.ye{padding-top:16px}.yj{max-width:56%}.yk{flex:1 0 0}.zm{width:100%}.zr{margin-left:0}.zs{margin-top:16px}.aba{margin-top:72px}.ll:hover{border-color:#E5E5E5}</style><style type="text/css" data-fela-rehydration="723" data-fela-type="RULE" media="all and (min-width: 904px) and (max-width: 1079.98px)">.i{display:none}.bu{width:64px}.ce{margin:0 64px}.ct{height:48px}.da{margin-bottom:52px}.dm{margin-bottom:48px}.dv{margin-bottom:68px}.dz{max-width:680px}.fa{margin-top:3.88em}.fs{font-size:42px}.ft{line-height:52px}.fu{letter-spacing:-0.011em}.gk{font-size:22px}.gl{margin-top:0.92em}.gm{line-height:28px}.gy{align-items:center}.ja{border-top:solid 1px #F2F2F2}.jb{border-bottom:solid 1px #F2F2F2}.jc{margin:32px 0 0}.jd{padding:3px 8px}.jo> *{margin-right:24px}.jp> :last-child{margin-right:0}.jx{display:flex}.mb{max-width:1192px}.mj{margin-top:56px}.nm{font-size:20px}.nn{margin-top:2em}.no{line-height:32px}.np{letter-spacing:-0.003em}.oi{font-size:24px}.oj{margin-top:1.95em}.ok{line-height:30px}.ol{letter-spacing:-0.016em}.ou{margin-top:0.86em}.pc{margin-top:2.14em}.pk{margin-top:1.14em}.pw{margin-top:1.72em}.px{line-height:24px}.py{letter-spacing:0}.rb{margin-top:1.25em}.rq{margin-bottom:88px}.sg{padding-top:72px}.ua{margin-top:40px}.uh{margin:0}.uv{width:calc(100% + 32px)}.uw{margin-left:-16px}.ux{margin-right:-16px}.vl{padding-left:16px}.vm{padding-right:16px}.vn{flex-basis:50%}.vo{max-width:50%}.vx{padding-bottom:56px}.wj{gap:24px 0}.wk{grid-template-areas:"image image image image image image image image image image image image" "content content content content content content content content content content content content"}.ws{display:block}.xb{margin-bottom:16px}.xn{padding-bottom:16px}.xo{flex:1 0 auto}.xx{max-height:48px}.xy{-webkit-line-clamp:2}.yh{padding-top:16px}.yp{max-width:56%}.yq{flex:1 0 0}.yz{margin-bottom:24px}.zc{flex-direction:row}.zp{width:min-width}.zx{margin-left:16px}.zy{margin-top:0px}.abd{margin-top:96px}.abs{margin-top:24px}.abt{margin-bottom:56px}</style><style type="text/css" data-fela-rehydration="723" data-fela-type="RULE" media="all and (min-width: 728px) and (max-width: 903.98px)">.j{display:none}.w{display:flex}.x{justify-content:space-between}.bt{width:64px}.cd{margin:0 48px}.cs{height:48px}.cz{margin-bottom:52px}.dl{margin-bottom:48px}.du{margin-bottom:68px}.dy{max-width:680px}.ez{margin-top:3.88em}.fp{font-size:42px}.fq{line-height:52px}.fr{letter-spacing:-0.011em}.gh{font-size:22px}.gi{margin-top:0.92em}.gj{line-height:28px}.gx{align-items:center}.iw{border-top:solid 1px #F2F2F2}.ix{border-bottom:solid 1px #F2F2F2}.iy{margin:32px 0 0}.iz{padding:3px 8px}.jm> *{margin-right:24px}.jn> :last-child{margin-right:0}.lz{margin:0}.ma{max-width:100%}.mi{margin-top:56px}.ni{font-size:20px}.nj{margin-top:2em}.nk{line-height:32px}.nl{letter-spacing:-0.003em}.oe{font-size:24px}.of{margin-top:1.95em}.og{line-height:30px}.oh{letter-spacing:-0.016em}.ot{margin-top:0.86em}.pb{margin-top:2.14em}.pj{margin-top:1.14em}.pt{margin-top:1.72em}.pu{line-height:24px}.pv{letter-spacing:0}.ra{margin-top:1.25em}.rp{margin-bottom:88px}.sf{padding-top:72px}.tz{margin-top:40px}.us{width:calc(100% + 28px)}.ut{margin-left:-14px}.uu{margin-right:-14px}.vh{padding-left:14px}.vi{padding-right:14px}.vj{flex-basis:50%}.vk{max-width:50%}.vw{padding-bottom:56px}.wh{gap:24px 0}.wi{grid-template-areas:"image image image image image image image image image image image image" "content content content content content content content content content content content content"}.wr{display:block}.xa{margin-bottom:16px}.xl{padding-bottom:16px}.xm{flex:1 0 auto}.xv{max-height:48px}.xw{-webkit-line-clamp:2}.yg{padding-top:16px}.yn{max-width:56%}.yo{flex:1 0 0}.yy{margin-bottom:24px}.zb{flex-direction:row}.zo{width:min-width}.zv{margin-left:16px}.zw{margin-top:0px}.abc{margin-top:96px}.abq{margin-top:24px}.abr{margin-bottom:56px}</style><style type="text/css" data-fela-rehydration="723" data-fela-type="RULE" media="all and (min-width: 552px) and (max-width: 727.98px)">.k{display:none}.u{display:flex}.v{justify-content:space-between}.bs{width:24px}.cc{margin:0 24px}.cr{height:40px}.cy{margin-bottom:44px}.dk{margin-bottom:32px}.dt{margin-bottom:4px}.ey{margin-top:2.64em}.fm{font-size:32px}.fn{line-height:38px}.fo{letter-spacing:-0.014em}.ge{font-size:18px}.gf{margin-top:0.79em}.gg{line-height:24px}.gw{align-items:flex-start}.ic{flex-direction:column}.if{margin-bottom:2px}.iu{margin:24px 0 0}.iv{padding:0}.jk> *{margin-right:8px}.jl> :last-child{margin-right:8px}.kc{margin-left:0px}.ln{border:1px solid #F2F2F2}.lo{border-radius:99em}.lp{padding:0px 16px 0px 12px}.lq{height:38px}.lr{align-items:center}.lt svg{margin-right:8px}.lx{margin:0}.ly{max-width:100%}.mh{margin-top:40px}.nf{margin-top:1.56em}.ng{line-height:28px}.nh{letter-spacing:-0.003em}.ob{font-size:20px}.oc{margin-top:1.2em}.od{letter-spacing:0}.os{margin-top:0.67em}.pi{margin-top:1.34em}.pq{font-size:16px}.pr{margin-top:1.23em}.ps{line-height:20px}.qz{margin-top:0.93em}.ro{margin-bottom:80px}.se{padding-top:48px}.ty{margin-top:32px}.up{width:calc(100% + 24px)}.uq{margin-left:-12px}.ur{margin-right:-12px}.ve{padding-left:12px}.vf{padding-right:12px}.vg{flex-basis:100%}.vv{padding-bottom:32px}.wf{gap:24px 0}.wg{grid-template-areas:"image image image image image image image image image image image image" "content content content content content content content content content content content content"}.wq{display:block}.wz{margin-bottom:16px}.xj{padding-bottom:16px}.xk{flex:1 0 auto}.xt{max-height:48px}.xu{-webkit-line-clamp:2}.yf{padding-top:16px}.yl{max-width:56%}.ym{flex:1 0 0}.zn{width:100%}.zt{margin-left:0}.zu{margin-top:16px}.abb{margin-top:72px}.ls:hover{border-color:#E5E5E5}</style><style type="text/css" data-fela-rehydration="723" data-fela-type="RULE" media="print">.rw{display:none}</style><style type="text/css" data-fela-rehydration="723" data-fela-type="RULE" media="(orientation: landscape) and (max-width: 903.98px)">.ip{max-height:none}</style><style type="text/css" data-fela-rehydration="723" data-fela-type="RULE" media="(prefers-reduced-motion: no-preference)">.mn{transition:transform 300ms cubic-bezier(0.2, 0, 0.2, 1)}</style></head><body><div id="root"><div class="a b c"><div class="d e f g h i j k"></div><script>document.domain = document.domain;</script><div class="l c"><div class="l m n o c"><div class="p q r s t u v w x i d y z"></div><div class="p q r ab ac"><div class="ab q ae"><a class="af ag ah ai aj ak al am an ao ap aq ar as at ab" aria-label="Homepage" data-testid="headerMediumLogo" href="https://medium.com/?source=---two_column_layout_nav----------------------------------" rel="noopener follow"><svg viewBox="0 0 3940 610" class="au av"><path d="M594.79 308.2c0 163.76-131.85 296.52-294.5 296.52S5.8 472 5.8 308.2 137.65 11.69 300.29 11.69s294.5 132.75 294.5 296.51M917.86 308.2c0 154.16-65.93 279.12-147.25 279.12s-147.25-125-147.25-279.12S689.29 29.08 770.61 29.08s147.25 125 147.25 279.12M1050 308.2c0 138.12-23.19 250.08-51.79 250.08s-51.79-112-51.79-250.08 23.19-250.08 51.8-250.08S1050 170.09 1050 308.2M1862.77 37.4l.82-.18v-6.35h-167.48l-155.51 365.5-155.51-365.5h-180.48v6.35l.81.18c30.57 6.9 46.09 17.19 46.09 54.3v434.45c0 37.11-15.58 47.4-46.15 54.3l-.81.18V587H1327v-6.35l-.81-.18c-30.57-6.9-46.09-17.19-46.09-54.3V116.9L1479.87 587h11.33l205.59-483.21V536.9c-2.62 29.31-18 38.36-45.68 44.61l-.82.19v6.3h213.3v-6.3l-.82-.19c-27.71-6.25-43.46-15.3-46.08-44.61l-.14-445.2h.14c0-37.11 15.52-47.4 46.08-54.3m97.43 287.8c3.49-78.06 31.52-134.4 78.56-135.37 14.51.24 26.68 5 36.14 14.16 20.1 19.51 29.55 60.28 28.09 121.21zm-2.11 22h250v-1.05c-.71-59.69-18-106.12-51.34-138-28.82-27.55-71.49-42.71-116.31-42.71h-1c-23.26 0-51.79 5.64-72.09 15.86-23.11 10.7-43.49 26.7-60.45 47.7-27.3 33.83-43.84 79.55-47.86 130.93-.13 1.54-.24 3.08-.35 4.62s-.18 2.92-.25 4.39a332.64 332.64 0 0 0-.36 21.69C1860.79 507 1923.65 600 2035.3 600c98 0 155.07-71.64 169.3-167.8l-7.19-2.53c-25 51.68-69.9 83-121 79.18-69.76-5.22-123.2-75.95-118.35-161.63m532.69 157.68c-8.2 19.45-25.31 30.15-48.24 30.15s-43.89-15.74-58.78-44.34c-16-30.7-24.42-74.1-24.42-125.51 0-107 33.28-176.21 84.79-176.21 21.57 0 38.55 10.7 46.65 29.37zm165.84 76.28c-30.57-7.23-46.09-18-46.09-57V5.28L2424.77 60v6.7l1.14-.09c25.62-2.07 43 1.47 53.09 10.79 7.9 7.3 11.75 18.5 11.75 34.26v71.14c-18.31-11.69-40.09-17.38-66.52-17.38-53.6 0-102.59 22.57-137.92 63.56-36.83 42.72-56.3 101.1-56.3 168.81C2230 518.72 2289.53 600 2378.13 600c51.83 0 93.53-28.4 112.62-76.3V588h166.65v-6.66zm159.29-505.33c0-37.76-28.47-66.24-66.24-66.24-37.59 0-67 29.1-67 66.24s29.44 66.24 67 66.24c37.77 0 66.24-28.48 66.24-66.24m43.84 505.33c-30.57-7.23-46.09-18-46.09-57h-.13V166.65l-166.66 47.85v6.5l1 .09c36.06 3.21 45.93 15.63 45.93 57.77V588h166.8v-6.66zm427.05 0c-30.57-7.23-46.09-18-46.09-57V166.65L3082 212.92v6.52l.94.1c29.48 3.1 38 16.23 38 58.56v226c-9.83 19.45-28.27 31-50.61 31.78-36.23 0-56.18-24.47-56.18-68.9V166.66l-166.66 47.85V221l1 .09c36.06 3.2 45.94 15.62 45.94 57.77v191.27a214.48 214.48 0 0 0 3.47 39.82l3 13.05c14.11 50.56 51.08 77 109 77 49.06 0 92.06-30.37 111-77.89v66h166.66v-6.66zM3934.2 588v-6.67l-.81-.19c-33.17-7.65-46.09-22.07-46.09-51.43v-243.2c0-75.83-42.59-121.09-113.93-121.09-52 0-95.85 30.05-112.73 76.86-13.41-49.6-52-76.86-109.06-76.86-50.12 0-89.4 26.45-106.25 71.13v-69.87l-166.66 45.89v6.54l1 .09c35.63 3.16 45.93 15.94 45.93 57V588h155.5v-6.66l-.82-.2c-26.46-6.22-35-17.56-35-46.66V255.72c7-16.35 21.11-35.72 49-35.72 34.64 0 52.2 24 52.2 71.28V588h155.54v-6.66l-.82-.2c-26.46-6.22-35-17.56-35-46.66v-248a160.45 160.45 0 0 0-2.2-27.68c7.42-17.77 22.34-38.8 51.37-38.8 35.13 0 52.2 23.31 52.2 71.28V588z"></path></svg></a><div class="aw h"><div class="ab ax ay az ba q bb bc"><div class="bl" aria-hidden="false" aria-describedby="searchResults" aria-labelledby="searchResults"></div><div class="bm bn ab"><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.1 11.06a6.95 6.95 0 1 1 13.9 0 6.95 6.95 0 0 1-13.9 0zm6.94-8.05a8.05 8.05 0 1 0 5.13 14.26l3.75 3.75a.56.56 0 1 0 .8-.79l-3.74-3.73A8.05 8.05 0 0 0 11.04 3v.01z" fill="currentColor"></path></svg></div><input role="combobox" aria-controls="searchResults" aria-expanded="false" aria-label="search" data-testid="headerSearchInput" tabindex="0" class="ax bd be bf z bg bh bi bj bk" placeholder="Search" value=""/></div></div></div><div class="bo bp bq br bs bt bu bv bw"></div><div class="bo bp bq br h bs k bt bu bv bw"></div><div class="bo bp bq br h bs k bt bu bv bw"></div><div class="bx bp by bz bw"></div></div></div><div class="l"><div class="ds dt du dv dw l"><div class="ab ca"><div class="ch bg dx dy dz ea"></div></div><article><div class="l"><div class="l"><span class="l"></span><section><div><div class="eh ei ej ek el em"></div><div class="eo ep eq er es"><div class="ab ca"><div class="ch bg dx dy dz ea"><h2 id="8e4f" class="et eu ev be b ew ex ey ez fa fb fc fd fe" aria-label="kicker paragraph"><a class="af ff" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">Hands-on Tutorials</a></h2><div><h1 id="a587" class="pw-post-title fg fh ev be fi fj fk fl fm fn fo fp fq fr fs ft fu fv fw fx fy fz bj" data-testid="storyTitle">How To Train Your Siamese Neural Network</h1></div><div><h2 id="dab5" class="pw-subtitle-paragraph ga fh ev be b gb gc gd ge gf gg gh gi gj gk gl gm gn go gp cp fd">The easy way to work with classes not seen at train time</h2><div class="gq gr gs gt gu"><div class="speechify-ignore ab co"><div class="speechify-ignore bg l"><div class="gv gw gx gy gz ab"><div><div class="ab ha"><a href="https://medium.com/@c.trotter2?source=post_page-----4c6da3259463--------------------------------" rel="noopener follow"><div><div class="bl" aria-hidden="false"><div class="l hb hc bx hd he"><div class="l hi"><img alt="Cameron Trotter" class="l ec bx dc dd cw" src="https://miro.medium.com/v2/resize:fill:88:88/1*tA2cVcxhp0PDoY9CDtvL0g.png" width="44" height="44" loading="lazy" data-testid="authorPhoto"/><div class="hf bx l dc dd eh n hg hh"></div></div></div></div></div></a><a href="https://towardsdatascience.com/?source=post_page-----4c6da3259463--------------------------------" rel="noopener follow"><div class="hj ab hi"><div><div class="bl" aria-hidden="false"><div class="l hk hl bx hd hm"><div class="l hi"><img alt="Towards Data Science" class="l ec bx bq hn cw" src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg" width="24" height="24" loading="lazy" data-testid="publicationPhoto"/><div class="hf bx l bq hn eh n hg hh"></div></div></div></div></div></div></a></div></div><div class="bm bg l"><div class="ab"><div style="flex:1"><span class="be b bf z bj"><div class="ho ab q"><div class="ab q hp"><div class="ab q"><div><div class="bl" aria-hidden="false"><p class="be b hq hr bj"><a class="af ag ah ai aj ak al am an ao ap aq ar hs" data-testid="authorName" href="https://medium.com/@c.trotter2?source=post_page-----4c6da3259463--------------------------------" rel="noopener follow">Cameron Trotter</a></p></div></div></div><span class="ht hu" aria-hidden="true"><span class="be b bf z fd">·</span></span><p class="be b hq hr fd"><span><a class="hv hw ah ai aj ak al am an ao ap aq ar hx hy hz" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9eff5ca21d80&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-train-your-siamese-neural-network-4c6da3259463&amp;user=Cameron+Trotter&amp;userId=9eff5ca21d80&amp;source=post_page-9eff5ca21d80----4c6da3259463---------------------post_header-----------" rel="noopener follow">Follow</a></span></p></div></div></span></div></div><div class="l ia"><span class="be b bf z fd"><div class="ab cm ib ic id"><div class="ie if ab"><div class="be b bf z fd ab ig"><span class="ih l ia">Published in</span><div><div class="bl" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hs ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page-----4c6da3259463--------------------------------" rel="noopener follow"><p class="be b bf z ii ij ik il im in io ip bj">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ht hu" aria-hidden="true"><span class="be b bf z fd">·</span></span></div></div><span class="be b bf z fd"><div class="ab ae"><span data-testid="storyReadTime">20 min read</span><div class="iq ir l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="be b bf z fd">·</span></span></div><span data-testid="storyPublishDate">Feb 19, 2021</span></div></span></div></span></div></div></div><div class="ab co is it iu iv iw ix iy iz ja jb jc jd je jf jg jh"><div class="h k w jx jy q"><div class="jz l"><div class="ab q ka"><div class="pw-multi-vote-icon hi ih kb kc kd"><div class=""><div class="ke kf kg kh ki kj kk am kl km kn kd"><svg width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" clip-rule="evenodd" d="M11.37.83L12 3.28l.63-2.45h-1.26zM15.42 1.84l-1.18-.39-.34 2.5 1.52-2.1zM9.76 1.45l-1.19.4 1.53 2.1-.34-2.5zM20.25 11.84l-2.5-4.4a1.42 1.42 0 0 0-.93-.64.96.96 0 0 0-.75.18c-.25.19-.4.42-.45.7l.05.05 2.35 4.13c1.62 2.95 1.1 5.78-1.52 8.4l-.46.41c1-.13 1.93-.6 2.78-1.45 2.7-2.7 2.51-5.59 1.43-7.38zM12.07 9.01c-.13-.69.08-1.3.57-1.77l-2.06-2.07a1.12 1.12 0 0 0-1.56 0c-.15.15-.22.34-.27.53L12.07 9z"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.74 8.3a1.13 1.13 0 0 0-.73-.5.67.67 0 0 0-.53.13c-.15.12-.59.46-.2 1.3l1.18 2.5a.45.45 0 0 1-.23.76.44.44 0 0 1-.48-.25L7.6 6.11a.82.82 0 1 0-1.15 1.15l3.64 3.64a.45.45 0 1 1-.63.63L5.83 7.9 4.8 6.86a.82.82 0 0 0-1.33.9c.04.1.1.18.18.26l1.02 1.03 3.65 3.64a.44.44 0 0 1-.15.73.44.44 0 0 1-.48-.1L4.05 9.68a.82.82 0 0 0-1.4.57.81.81 0 0 0 .24.58l1.53 1.54 2.3 2.28a.45.45 0 0 1-.64.63L3.8 13a.81.81 0 0 0-1.39.57c0 .22.09.43.24.58l4.4 4.4c2.8 2.8 5.5 4.12 8.68.94 2.27-2.28 2.71-4.6 1.34-7.1l-2.32-4.08z"></path></svg></div></div></div><div class="pw-multi-vote-count l ko kp kq kr ks kt ku"><p class="be b ew z fd"><span class="kf">--</span></p></div></div></div><div><div class="bl" aria-hidden="false"><button class="ao ke kv kw ab q kx ky kz" aria-label="responses"><svg width="24" height="24" viewBox="0 0 24 24" class="la"><path d="M18 16.8a7.14 7.14 0 0 0 2.24-5.32c0-4.12-3.53-7.48-8.05-7.48C7.67 4 4 7.36 4 11.48c0 4.13 3.67 7.48 8.2 7.48a8.9 8.9 0 0 0 2.38-.32c.23.2.48.39.75.56 1.06.69 2.2 1.04 3.4 1.04.22 0 .4-.11.48-.29a.5.5 0 0 0-.04-.52 6.4 6.4 0 0 1-1.16-2.65v.02zm-3.12 1.06l-.06-.22-.32.1a8 8 0 0 1-2.3.33c-4.03 0-7.3-2.96-7.3-6.59S8.17 4.9 12.2 4.9c4 0 7.1 2.96 7.1 6.6 0 1.8-.6 3.47-2.02 4.72l-.2.16v.26l.02.3a6.74 6.74 0 0 0 .88 2.4 5.27 5.27 0 0 1-2.17-.86c-.28-.17-.72-.38-.94-.59l.01-.02z"></path></svg></button></div></div></div><div class="ab q ji jj jk jl jm jn jo jp jq jr js jt ju jv jw"><div class="lb k j i d"></div><div class="h k"></div><div class="ec ug cm"><div class="l ae"><div class="ab ca"><div class="lv lx lz uh ui mr ch bg"><div class="ab"><div class="bl bg" aria-hidden="false"><div><div class="bl" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af kx ah ai aj ak al lc an ao ap hx ld le kz lf lg lh li lj s lk ll lm ln lo lp lq u lr ls lt"><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path fill-rule="evenodd" clip-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0zm9-10a10 10 0 1 0 0 20 10 10 0 0 0 0-20zm3.38 10.42l-4.6 3.06a.5.5 0 0 1-.78-.41V8.93c0-.4.45-.63.78-.41l4.6 3.06c.3.2.3.64 0 .84z" fill="currentColor"></path></svg><div class="j i d"><p class="be b bf z fd">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bl" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bl" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af kx ah ai aj ak al lc an ao ap hx ld le kz lf lg lh li lj s lk ll lm ln lo lp lq u lr ls lt"><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path fill-rule="evenodd" clip-rule="evenodd" d="M15.22 4.93a.42.42 0 0 1-.12.13h.01a.45.45 0 0 1-.29.08.52.52 0 0 1-.3-.13L12.5 3v7.07a.5.5 0 0 1-.5.5.5.5 0 0 1-.5-.5V3.02l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.8a.42.42 0 0 1 .07.5zm-.1.14zm.88 2h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11a2 2 0 0 1-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.14c.1.1.15.22.15.35a.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9V8.96c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1z" fill="currentColor"></path></svg><div class="j i d"><p class="be b bf z fd">Share</p></div></button></div></div></div></div></div></div></div></div></div></div></div><div class="lu"><div class="ab ca"><div class="lv lw lx ly lz ma ce mb cf mc ch bg"><figure class="mg mh mi mj mk lu ml mm paragraph-image"><div role="button" tabindex="0" class="mn mo hi mp bg mq"><div class="md me mf"><picture><source srcSet="https://miro.medium.com/v2/resize:fit:640/0*Qu0mmtNKS2OjKTud 640w, https://miro.medium.com/v2/resize:fit:720/0*Qu0mmtNKS2OjKTud 720w, https://miro.medium.com/v2/resize:fit:750/0*Qu0mmtNKS2OjKTud 750w, https://miro.medium.com/v2/resize:fit:786/0*Qu0mmtNKS2OjKTud 786w, https://miro.medium.com/v2/resize:fit:828/0*Qu0mmtNKS2OjKTud 828w, https://miro.medium.com/v2/resize:fit:1100/0*Qu0mmtNKS2OjKTud 1100w, https://miro.medium.com/v2/resize:fit:2000/0*Qu0mmtNKS2OjKTud 2000w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 1000px" type="image/webp"/><source data-testid="og" srcSet="https://miro.medium.com/v2/resize:fit:640/0*Qu0mmtNKS2OjKTud 640w, https://miro.medium.com/v2/resize:fit:720/0*Qu0mmtNKS2OjKTud 720w, https://miro.medium.com/v2/resize:fit:750/0*Qu0mmtNKS2OjKTud 750w, https://miro.medium.com/v2/resize:fit:786/0*Qu0mmtNKS2OjKTud 786w, https://miro.medium.com/v2/resize:fit:828/0*Qu0mmtNKS2OjKTud 828w, https://miro.medium.com/v2/resize:fit:1100/0*Qu0mmtNKS2OjKTud 1100w, https://miro.medium.com/v2/resize:fit:2000/0*Qu0mmtNKS2OjKTud 2000w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 1000px"/><img alt="A very small siamese kitten staring at a laptop" class="bg mr ms c" width="1000" height="667" loading="eager"/></picture></div></div><figcaption class="mt mu mv md me mw mx be b bf z fd">Photo by <a class="af my" href="https://unsplash.com/@serejaris?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Sereja Ris</a> on <a class="af my" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure></div></div></div><div class="ab ca"><div class="ch bg dx dy dz ea"><p id="aff4" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">When you first start out with machine learning, it becomes clear from the offset that massive amounts of data are required for accurate and robust model training. And whilst this is true, when training models for purposes where a custom dataset is required you often need to compromise on the level of data your model sees.</p><p id="218a" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">This was the case for myself; working in conservation tech, any models we deploy to an area are built using data collected from previous years’ surveys, which in some cases may be sparse (certainly nowhere near the levels of benchmarking datasets such as ImageNet [1]). To make matters worse, working in conservation tech means working with open-ended datasets. Because the animals we work with are free roaming, there is no guarantee that a dataset we use for model training will contain examples of everything the model will see in the field.</p><p id="228d" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">This results in somewhat of an uphill battle when trying to deploy models using traditional machine learning approaches. Building a model for conservation is useless if you need thousands of examples for each class and you need to retrain your model each year as the classes change. But this problem isn’t confined to conservation, lots of areas outside of benchmarking have similar issues with amounts of data and rates of change.</p><p id="b82b" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">In this article I will discuss a type of model known as a Siamese Neural Network. Hopefully after reading, you will have a better understanding of how this architecture can help not just in conservation, but in any area where data quantities are limited and rates of class change are fast.</p><h1 id="6326" class="nv nw ev be nx ny nz gd oa ob oc gg od oe of og oh oi oj ok ol om on oo op oq bj">Prerequisites</h1><p id="3d65" class="pw-post-body-paragraph mz na ev nb b gb or nd ne ge os ng nh ni ot nk nl nm ou no np nq ov ns nt nu eo bj">Before getting started you should probably have an understanding of machine learning, specifically Convolutional Neural Networks. If you don’t, I found <a class="ow hv ff" href="https://medium.com/u/631ee5e6343e?source=post_page-----4c6da3259463--------------------------------" rel="noopener" target="_blank">Sumit Saha</a>’s post <a class="af my" rel="noopener" target="_blank" href="/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53"><em class="ox">A Comprehensive Guide to Convolutional Neural Networks — the ELI5 way</em></a><em class="ox"> </em>to be a good starting point in lieu of a formal education in the area. You should probably read that first.</p><p id="d9f5" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">You should also be comfortable with Python, Keras, and TensorFlow. We will be working through examples of code during this article, as I find doing this gives a better understanding than just free form text on its own. All code in this guide was written in TensorFlow 1.14, but there is no reason why the code shouldn’t work in newer versions (possibly with a few modifications), or indeed ported to other deep learning frameworks such as PyTorch.</p><h1 id="bec9" class="nv nw ev be nx ny nz gd oa ob oc gg od oe of og oh oi oj ok ol om on oo op oq bj">What is a Siamese Neural Network?</h1><p id="7aa0" class="pw-post-body-paragraph mz na ev nb b gb or nd ne ge os ng nh ni ot nk nl nm ou no np nq ov ns nt nu eo bj">In short, a Siamese Neural Network is any model architecture which contains at least two parallel, identical, Convolutional Neural Networks. We’ll call these SNNs and CNNs from now on. This parallel CNN architecture allows for the model to learn <em class="ox">similarity</em>, which can be used instead of a direct classification. SNNs have found uptake primarily for image data, such as in facial recognition, although they do have their uses outside of this domain. For example, Selen Uguroglu gave a great talk at NeurIPS 2020 about <a class="af my" href="https://slideslive.com/38943514/similarity-at-netflix" rel="noopener ugc nofollow" target="_blank">how Netflix utilises SNNs to generate user recommendations</a> based on film metadata. For this guide, we will focus on image data.</p><p id="be34" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">Each parallel CNN which forms a part of the SNN is designed to produce an embedding, or a reduced dimensional representation, of the input. For example, if we specify an embedding size of 10 we may input a high dimensional image of size <em class="ox">width</em> * <em class="ox">height</em> * <em class="ox">channels</em> and receive as output a float value vector of size 10 which directly represents the image.</p><p id="c323" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">These embeddings can then be used to optimise a Ranking Loss, and at test time used to generate a similarity score. The parallel CNNs can, in theory, take any form. One important point however is that they must be completely identical; they must share the same architecture, share the same initial and updated weights, and have the same hyperparameters. This consistency allows the model to compare the inputs it receives, usually one per CNN branch. The SigNet paper from Dey <em class="ox">et al. </em>[2] provides an excellent visualisation of this, which can be seen below.</p><figure class="mg mh mi mj mk lu md me paragraph-image"><div role="button" tabindex="0" class="mn mo hi mp bg mq"><div class="md me oy"><picture><source srcSet="https://miro.medium.com/v2/resize:fit:640/format:webp/1*IZbLdsdYRISf_86991WqXQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*IZbLdsdYRISf_86991WqXQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*IZbLdsdYRISf_86991WqXQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*IZbLdsdYRISf_86991WqXQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*IZbLdsdYRISf_86991WqXQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*IZbLdsdYRISf_86991WqXQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IZbLdsdYRISf_86991WqXQ.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"/><source data-testid="og" srcSet="https://miro.medium.com/v2/resize:fit:640/1*IZbLdsdYRISf_86991WqXQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*IZbLdsdYRISf_86991WqXQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*IZbLdsdYRISf_86991WqXQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*IZbLdsdYRISf_86991WqXQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*IZbLdsdYRISf_86991WqXQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*IZbLdsdYRISf_86991WqXQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*IZbLdsdYRISf_86991WqXQ.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"/><img alt="The signet architecture from Dey et al. [1]. Two signatures enter two identical CNNs, which produce an output sent to a loss." class="bg mr ms c" width="700" height="470" loading="lazy"/></picture></div></div><figcaption class="mt mu mv md me mw mx be b bf z fd">The SigNet architecture. Image from Dey <em class="oz">et al. [2].</em></figcaption></figure><p id="297a" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">The goal of SigNet is to determine if a given signature is genuine or a forgery. This can be achieved through the use of two parallel CNNs, trained on genuine and forged signature pairs. Each signature is fed through one branch of the SNN which generates a <em class="ox">d-</em>dimensional embedding for the image. It is these embeddings which are used to optimise a loss function rather than the images themselves. More recent versions of SNNs will most likely utilise triple or even quadruple branching, containing three or four parallel CNNs respectively.</p><h1 id="bdc7" class="nv nw ev be nx ny nz gd oa ob oc gg od oe of og oh oi oj ok ol om on oo op oq bj">What’s the Point of SNNs?</h1><p id="61b6" class="pw-post-body-paragraph mz na ev nb b gb or nd ne ge os ng nh ni ot nk nl nm ou no np nq ov ns nt nu eo bj">Now that we understand the make-up of an SNN, we can highlight their value. Using the generated <em class="ox">d-</em>dimensional embeddings, we can create some <em class="ox">d-</em>dimensional hyperspace that allows the embeddings to be plotted creating clusters. This hyperspace can then be projected down to 2-dimensions for plotting using Principle Component Analysis, or PCA.</p><figure class="mg mh mi mj mk lu md me paragraph-image"><div class="md me pa"><picture><source srcSet="https://miro.medium.com/v2/resize:fit:640/format:webp/1*76eeE-SqxFMP-rtdD76KEw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*76eeE-SqxFMP-rtdD76KEw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*76eeE-SqxFMP-rtdD76KEw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*76eeE-SqxFMP-rtdD76KEw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*76eeE-SqxFMP-rtdD76KEw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*76eeE-SqxFMP-rtdD76KEw.png 1100w, https://miro.medium.com/v2/resize:fit:800/format:webp/1*76eeE-SqxFMP-rtdD76KEw.png 800w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 400px" type="image/webp"/><source data-testid="og" srcSet="https://miro.medium.com/v2/resize:fit:640/1*76eeE-SqxFMP-rtdD76KEw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*76eeE-SqxFMP-rtdD76KEw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*76eeE-SqxFMP-rtdD76KEw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*76eeE-SqxFMP-rtdD76KEw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*76eeE-SqxFMP-rtdD76KEw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*76eeE-SqxFMP-rtdD76KEw.png 1100w, https://miro.medium.com/v2/resize:fit:800/1*76eeE-SqxFMP-rtdD76KEw.png 800w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 400px"/><img alt="A graph showing multiple dots, each coloured 1 of 10 colours. Dots of the same colour are generally clustered together." class="bg mr ms c" width="400" height="405" loading="lazy"/></picture></div><figcaption class="mt mu mv md me mw mx be b bf z fd">A plot of an embedding hyperspace after 100 training epochs, projected down to 2-dimensions using PCA. Image generated using code based on <a class="af my" href="https://github.com/AdrianUng/keras-triplet-loss-mnist/blob/master/Triplet_loss_KERAS_semi_hard_from_TF.ipynb" rel="noopener ugc nofollow" target="_blank">this notebook</a>.</figcaption></figure><p id="de84" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">This plot shows the embedding locations for a subset of the MNIST test data [3]. Here, a model has been trained to generate embeddings of images for the 10 unique classes (images of handwritten digits between 0 and 9). Notice how, even after only 100 training epochs, the model is starting to generate similar embeddings for images of the same class. This can be seen by the clusterings of dots of the same colour in the graph above — some clusters in the plot are visualised on top of each other, this is due to the reduction down to 2-d through PCA. Other visualisations such as t-SNE plots, or reducing to a higher number of dimensions, can help in this situation.</p><p id="d035" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">It’s this embedding clustering that makes SNNs such a powerful tool. If we suddenly decided that we wanted to add another class to the data, then there is no need to retrain the model. The new class embeddings should be generated in such a way that, when plotted into the hyperspace, they are far away from the existing clusters, but cluster together with other examples of the new class as they are added. By using this embedding similarity, we can begin to produce likely classifications for both seen and unseen classes using very little data.</p><h1 id="24fd" class="nv nw ev be nx ny nz gd oa ob oc gg od oe of og oh oi oj ok ol om on oo op oq bj">Model Training</h1><p id="b7f2" class="pw-post-body-paragraph mz na ev nb b gb or nd ne ge os ng nh ni ot nk nl nm ou no np nq ov ns nt nu eo bj">Previously, I mentioned that SNNs consist of <em class="ox">at least two </em>parallel CNN branches, but modern implementations often rely on more. The number of branches in your SNN has a big influence on your model training. Not only do you need to ensure that your data is fed to the SNN in such a way that each branch receives training examples, but your choice of loss function must also take the number of branches into account. Regardless of the number of branches chosen, the <em class="ox">type</em> of loss function will likely stay consistent.</p><p id="68af" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">Ranking Losses, also known as Contrastive Losses, aim to predict relative distances between model inputs when projected onto a hyperspace. This is in comparison to more traditional losses which aim to predict some set of class labels. Ranking Losses play an important role in SNNs, although they are also useful for other tasks such as Natural Language Processing. There are many different types of Ranking Loss, but they all work (generally) in the same way.</p><p id="700d" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">Let’s assume we have two inputs, and we want to know how similar they are. Using a Ranking Loss, we would perform the following steps:</p><ol class=""><li id="6eb1" class="mz na ev nb b gb nc nd ne ge nf ng nh ni pb nk nl nm pc no np nq pd ns nt nu pe pf pg bj">Extract the features from the input.</li><li id="c5ff" class="mz na ev nb b gb ph nd ne ge pi ng nh ni pj nk nl nm pk no np nq pl ns nt nu pe pf pg bj">Embed the extracted features onto a <em class="ox">d-</em>dimensional hyperspace.</li><li id="ceae" class="mz na ev nb b gb ph nd ne ge pi ng nh ni pj nk nl nm pk no np nq pl ns nt nu pe pf pg bj">Calculate the distance between the embeddings (e.g. using Euclidean distance) to be used as a measure of similarity.</li></ol><p id="a138" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">It’s important to note here that we often don’t particularly care about the values of the embeddings, just the distances between them. Taking the plot of the embeddings shown earlier, notice all points lie between about -1.5, 2.0 on the x-axis and about -2.0, 2.0 on the y-axis. There is nothing inherently good or bad about a model that embeds within this range, all that matters is the points are clustering in their respective classes.</p><h2 id="0164" class="pm nw ev be nx pn po pp oa pq pr ps od ni pt pu pv nm pw px py nq pz qa qb fc bj">Triplet Ranking Loss</h2><p id="71f7" class="pw-post-body-paragraph mz na ev nb b gb or nd ne ge os ng nh ni ot nk nl nm ou no np nq ov ns nt nu eo bj">One of the more common types of Ranking Loss used for SNNs is Triplet Ranking Loss. You’ll often see SNNs using this loss function called Triplet Networks as if they are their own thing (and indeed this is how they are defined by Hoffer <em class="ox">e</em>t <em class="ox">al. </em>in the paper that first conceived them [4]) but really they’re just an SNN with three branches. Because Triplet Loss is so commonplace now, and it’s the loss function we’ll be using later in this post, it’s important to understand how it works.</p><figure class="mg mh mi mj mk lu md me paragraph-image"><div role="button" tabindex="0" class="mn mo hi mp bg mq"><div class="md me qc"><picture><source srcSet="https://miro.medium.com/v2/resize:fit:640/format:webp/1*bJABur9wzFNACosQkim8kw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*bJABur9wzFNACosQkim8kw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*bJABur9wzFNACosQkim8kw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*bJABur9wzFNACosQkim8kw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*bJABur9wzFNACosQkim8kw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*bJABur9wzFNACosQkim8kw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bJABur9wzFNACosQkim8kw.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"/><source data-testid="og" srcSet="https://miro.medium.com/v2/resize:fit:640/1*bJABur9wzFNACosQkim8kw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*bJABur9wzFNACosQkim8kw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*bJABur9wzFNACosQkim8kw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*bJABur9wzFNACosQkim8kw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*bJABur9wzFNACosQkim8kw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*bJABur9wzFNACosQkim8kw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*bJABur9wzFNACosQkim8kw.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"/><img alt="Left: 3 input imgs labelled anchor, positive, negative. Centre: An SNN embedding the imgs. Right: The images embedded, arrows showing positive and negative are pushing away, positive and centre are getting closer." class="bg mr ms c" width="700" height="372" loading="lazy"/></picture></div></div><figcaption class="mt mu mv md me mw mx be b bf z fd">An example showing how triplet ranking loss works to pull embedded images of the same class closer together, and different classes further apart. Image by author.</figcaption></figure><p id="b32c" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">Triplet Ranking Loss requires, as the name suggests, three inputs which we call a triplet. Each data-point in the triplet has its own job. The <strong class="nb fi">Anchor </strong>is data of some class C which defines which class the triplet will train the model on. The <strong class="nb fi">Positive</strong> is another example of the class C. The <strong class="nb fi">Negative </strong>is a data-point of some class which is <em class="ox">not</em> C. At train time, each of our triplet components is fed to its own CNN branch to be embedded. These embeddings are passed to the Triplet Loss Function, which is defined as:</p><figure class="mg mh mi mj mk lu md me paragraph-image"><div class="md me pa"><picture><source srcSet="https://miro.medium.com/v2/resize:fit:640/format:webp/1*dPVJTB9t6uRr_-ij_GIFug.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*dPVJTB9t6uRr_-ij_GIFug.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*dPVJTB9t6uRr_-ij_GIFug.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*dPVJTB9t6uRr_-ij_GIFug.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*dPVJTB9t6uRr_-ij_GIFug.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*dPVJTB9t6uRr_-ij_GIFug.png 1100w, https://miro.medium.com/v2/resize:fit:800/format:webp/1*dPVJTB9t6uRr_-ij_GIFug.png 800w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 400px" type="image/webp"/><source data-testid="og" srcSet="https://miro.medium.com/v2/resize:fit:640/1*dPVJTB9t6uRr_-ij_GIFug.png 640w, https://miro.medium.com/v2/resize:fit:720/1*dPVJTB9t6uRr_-ij_GIFug.png 720w, https://miro.medium.com/v2/resize:fit:750/1*dPVJTB9t6uRr_-ij_GIFug.png 750w, https://miro.medium.com/v2/resize:fit:786/1*dPVJTB9t6uRr_-ij_GIFug.png 786w, https://miro.medium.com/v2/resize:fit:828/1*dPVJTB9t6uRr_-ij_GIFug.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*dPVJTB9t6uRr_-ij_GIFug.png 1100w, https://miro.medium.com/v2/resize:fit:800/1*dPVJTB9t6uRr_-ij_GIFug.png 800w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 400px"/><img alt="L equals the max of 0 or D(A,P) — D(A,N) + margin" class="bg mr ms c" width="400" height="24" loading="lazy"/></picture></div></figure><p id="8302" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">Where <em class="ox">D(A,P) </em>is the embedding distance between the Anchor and the Positive, and <em class="ox">D(A,N) </em>is the embedding distance between the Anchor and the Negative. We also define some margin - an often used initial value for this is 0.2, the margin used in FaceNet [5].</p><p id="9852" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">The purpose of this function is to minimise the distance between the Anchor and the Positive, whilst maximising the distance between the Anchor and the Negative. For a more in-depth look at Triplet Ranking Loss, I’d suggest <a class="af my" href="https://gombru.github.io/2019/04/03/ranking_loss/" rel="noopener ugc nofollow" target="_blank">this excellent post</a> from Raúl Gómez.</p><h2 id="6daa" class="pm nw ev be nx pn po pp oa pq pr ps od ni pt pu pv nm pw px py nq pz qa qb fc bj">Semi-Hard Triplet Mining</h2><p id="4078" class="pw-post-body-paragraph mz na ev nb b gb or nd ne ge os ng nh ni ot nk nl nm ou no np nq ov ns nt nu eo bj">Because of the importance of the triplet components, it is imperative that our SNN is provided only with triplets which will enable it to learn. More specifically, we want to provide Negatives such that our triplets allow the model to learn, but not be so difficult that learning takes too long.</p><p id="a8d7" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">An easy way to do this is through a process known as Semi-Hard Triplet Mining. To perform this, we first define three categories of triplet:</p><ul class=""><li id="9e7e" class="mz na ev nb b gb nc nd ne ge nf ng nh ni pb nk nl nm pc no np nq pd ns nt nu qd pf pg bj"><strong class="nb fi">Easy Triplets</strong> are those where <em class="ox">D(A,P) + margin &lt; D(A,N)</em>, thus <em class="ox">L = 0.</em></li><li id="d611" class="mz na ev nb b gb ph nd ne ge pi ng nh ni pj nk nl nm pk no np nq pl ns nt nu qd pf pg bj"><strong class="nb fi">Hard Triplets</strong> are those where <em class="ox">D(A,N) &lt; D(A,P).</em></li><li id="6647" class="mz na ev nb b gb ph nd ne ge pi ng nh ni pj nk nl nm pk no np nq pl ns nt nu qd pf pg bj"><strong class="nb fi">Semi-Hard Triplets </strong>are those where <em class="ox">D(A,P) &lt; D(A,N) &lt; D(A,P) + margin.</em></li></ul><p id="a135" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">The goal is to find as many Semi-Hard Triplets as possible. These triplets have a positive loss, but the Positive embedding distance is closer to the Anchor embedding than the Negative. This allows for fast training, but is still difficult enough for the model to actually learn something during training.</p><p id="6c27" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">Finding these Semi-Hard triplets can be performed in one of two ways. In <strong class="nb fi">Offline </strong>mining, the entire dataset is converted into triplets before training. In <strong class="nb fi">Online</strong> mining, batches of data are fed in, with random triplets generated on the fly.</p><p id="b9bd" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">As a general rule of thumb, Online mining should be performed wherever possible as it allows for much faster training due to the ability to constantly update our threshold definition of a Semi-Hard Triplet as training progresses. This can be supplemented with data augmentation, which can also be performed in an Online fashion.</p><h1 id="3ccd" class="nv nw ev be nx ny nz gd oa ob oc gg od oe of og oh oi oj ok ol om on oo op oq bj">Using SNNs at Inference Time</h1><p id="e2b5" class="pw-post-body-paragraph mz na ev nb b gb or nd ne ge os ng nh ni ot nk nl nm ou no np nq ov ns nt nu eo bj">Now that we understand how SNNs are trained, we next need to understand how they can be used at inference time. During training we used all of the branches of the SNN, whereas inference can be performed using a single CNN branch.</p><p id="85a9" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">At inference time, the input image of an unknown class is processed by the CNN branch and has its features embedded. This embedding is then plotted onto the hyperspace and compared with the other clusters. This provides us with a list of similarity scores, or relative distances between the image of unknown class and all of the existing clusters. The clusters we compare our input image against are known as the <strong class="nb fi">support set</strong>. Let’s take a look at an example to help understand this.</p><figure class="mg mh mi mj mk lu md me paragraph-image"><div role="button" tabindex="0" class="mn mo hi mp bg mq"><div class="md me qe"><picture><source srcSet="https://miro.medium.com/v2/resize:fit:640/format:webp/1*_Ue4uUtayX2vW600HjGbyg.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*_Ue4uUtayX2vW600HjGbyg.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*_Ue4uUtayX2vW600HjGbyg.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*_Ue4uUtayX2vW600HjGbyg.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*_Ue4uUtayX2vW600HjGbyg.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*_Ue4uUtayX2vW600HjGbyg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_Ue4uUtayX2vW600HjGbyg.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"/><source data-testid="og" srcSet="https://miro.medium.com/v2/resize:fit:640/1*_Ue4uUtayX2vW600HjGbyg.png 640w, https://miro.medium.com/v2/resize:fit:720/1*_Ue4uUtayX2vW600HjGbyg.png 720w, https://miro.medium.com/v2/resize:fit:750/1*_Ue4uUtayX2vW600HjGbyg.png 750w, https://miro.medium.com/v2/resize:fit:786/1*_Ue4uUtayX2vW600HjGbyg.png 786w, https://miro.medium.com/v2/resize:fit:828/1*_Ue4uUtayX2vW600HjGbyg.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*_Ue4uUtayX2vW600HjGbyg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*_Ue4uUtayX2vW600HjGbyg.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"/><img alt="L: image of a moth labelled ‘Test Image’. M: Multiple moths labelled ‘Support Set’. R: The first Support Set image on its own" class="bg mr ms c" width="700" height="810" loading="lazy"/></picture></div></div><figcaption class="mt mu mv md me mw mx be b bf z fd">Finding the most likely family class for the test image moth, based on data from Vetrova <em class="oz">et al. [6]. </em>Image generated using code based on <a class="af my" href="https://github.com/asagar60/One-Shot-Learning/blob/master/Omniglot_data/One_shot_implementation.ipynb" rel="noopener ugc nofollow" target="_blank">this notebook</a>.</figcaption></figure><p id="ab0b" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">The plot above is the output of an SNN I created to determine the scientific family of moths. Each image in the dataset, adapted from Vetrova <em class="ox">et al. </em>[6], was labelled with one of four scientific family names or labelled as ‘larvae’, giving a total of five classes. For ease of visualisation (although in hindsight not necessarily ease of understanding) each of the known labelled classes is displayed in the support set, shown in the middle of the plot above, using a random example image from each class. On the left of the plot is a test image; this is a moth image unseen by the SNN, which is now tasked with determining the scientific family.</p><p id="4d14" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">First, the SNN embeds the test image using the embedding function learned during training. Next, it compares this embedding with the support set embeddings, which provides a most likely moth family for the test image. To the right of the plot, we can see the first image in the support set has been printed again.</p><p id="838f" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">The code used to generate the plot above was told to show the corresponding example of the test image’s family first (the plotting code knows the correct class, the SNN does not). Because the first support set image is shown again on the right of the plot, this tells us that the SNN was correct in determining the scientific family of the test image moth! If this plot is a bit confusing to you, don’t worry as we’ll be working through creating the same plot on different, simpler, data later.</p><p id="1dda" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">This code could be extended further to alert users if an embedding is placed in a new area of the hyperspace if if exceeds some predefined class distance threshold. This could be an indication that a new moth family has been seen by the SNN for the first time.</p><h1 id="ccb6" class="nv nw ev be nx ny nz gd oa ob oc gg od oe of og oh oi oj ok ol om on oo op oq bj">Where Do We Measure From?</h1><p id="d6c1" class="pw-post-body-paragraph mz na ev nb b gb or nd ne ge os ng nh ni ot nk nl nm ou no np nq ov ns nt nu eo bj">In order to determine the distance between the test image and the classes in the support set, we need a location for each class to measure from. At first glance, it might seem okay to use a randomly selected embedding from each support set class; after all, if all embeddings are perfectly clustered surely it doesn’t matter which one we use?</p><p id="91d6" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">Whilst this assumption certainly holds <em class="ox">if </em>our class embeddings are perfectly clustered, in a real world system this won’t be the case. Let’s examine the toy example below.</p><figure class="mg mh mi mj mk lu md me paragraph-image"><div role="button" tabindex="0" class="mn mo hi mp bg mq"><div class="md me oy"><picture><source srcSet="https://miro.medium.com/v2/resize:fit:640/format:webp/1*fruv2ApgYYJy_0c27wHiaQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*fruv2ApgYYJy_0c27wHiaQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*fruv2ApgYYJy_0c27wHiaQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*fruv2ApgYYJy_0c27wHiaQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*fruv2ApgYYJy_0c27wHiaQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*fruv2ApgYYJy_0c27wHiaQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fruv2ApgYYJy_0c27wHiaQ.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"/><source data-testid="og" srcSet="https://miro.medium.com/v2/resize:fit:640/1*fruv2ApgYYJy_0c27wHiaQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*fruv2ApgYYJy_0c27wHiaQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*fruv2ApgYYJy_0c27wHiaQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*fruv2ApgYYJy_0c27wHiaQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*fruv2ApgYYJy_0c27wHiaQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*fruv2ApgYYJy_0c27wHiaQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*fruv2ApgYYJy_0c27wHiaQ.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"/><img alt="Group of crosses top left, group of squares bottom right, 1 circled. 1 cross in square group, circled. Triangle above squares" class="bg mr ms c" width="700" height="394" loading="lazy"/></picture></div></div><figcaption class="mt mu mv md me mw mx be b bf z fd">An example embedding space with two classes, crosses and squares, and a yet undetermined class embedding represented by a triangle. Image by author.</figcaption></figure><p id="16f8" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">In this example we have a two class embedding space, one for crosses and one for squares. All of the square class embeddings are clustered to the right of the plot, however the class of crosses has one embedding which has not been clustered with the others in the top left. This erroneous cross has been embedded into the space where the squares usually cluster. There is also a triangle plotted in the top-right, this is the current test image, embedded into the space but not yet assigned to a class based on its distance to the other clusters.</p><p id="28ad" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">In order to determine if the triangle should actually be a cross or a square, we randomly select an embedding to measure from for each class; the erroneous cross and the bottom-left square are chosen (both circled). If we compare the distances from these embeddings, the chosen cross is closest, so the triangle would be labelled a cross.</p><p id="6fe2" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">However looking at the plot as a whole, it’s clear that the triangle should probably be labelled as a square, and the cross is an outlier. By selecting random embeddings to measure from we run the risk of having outliers skew the distance measurement, and thus the final outcome.</p><p id="29b5" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">This can be solved using <strong class="nb fi">prototypes</strong>, an elegant and easy to understand solution to our problem. Prototypes are essentially generalised embeddings for each class, reducing the effect of outliers on the distance measurements. These can be calculated in a variety of ways, but simple techniques such as taking the median work well. Let’s update the toy example…</p><figure class="mg mh mi mj mk lu md me paragraph-image"><div role="button" tabindex="0" class="mn mo hi mp bg mq"><div class="md me oy"><picture><source srcSet="https://miro.medium.com/v2/resize:fit:640/format:webp/1*Dfi3oss8UuI627yajuXw6Q.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*Dfi3oss8UuI627yajuXw6Q.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*Dfi3oss8UuI627yajuXw6Q.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*Dfi3oss8UuI627yajuXw6Q.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*Dfi3oss8UuI627yajuXw6Q.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*Dfi3oss8UuI627yajuXw6Q.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dfi3oss8UuI627yajuXw6Q.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"/><source data-testid="og" srcSet="https://miro.medium.com/v2/resize:fit:640/1*Dfi3oss8UuI627yajuXw6Q.png 640w, https://miro.medium.com/v2/resize:fit:720/1*Dfi3oss8UuI627yajuXw6Q.png 720w, https://miro.medium.com/v2/resize:fit:750/1*Dfi3oss8UuI627yajuXw6Q.png 750w, https://miro.medium.com/v2/resize:fit:786/1*Dfi3oss8UuI627yajuXw6Q.png 786w, https://miro.medium.com/v2/resize:fit:828/1*Dfi3oss8UuI627yajuXw6Q.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*Dfi3oss8UuI627yajuXw6Q.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*Dfi3oss8UuI627yajuXw6Q.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"/><img alt="Same plot as above, but now there are two ‘P’s in the middle of each cluster, which have been circled." class="bg mr ms c" width="700" height="394" loading="lazy"/></picture></div></div><figcaption class="mt mu mv md me mw mx be b bf z fd">The same embedding space as previous, but with the inclusion of prototypes. Image by author.</figcaption></figure><p id="6877" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">Now, each class has been given a prototype near the centre of its cluster (e.g. Pₓ is the prototype for the cross class). If we select the prototypes when measuring similarity, our triangle is correctly labelled as a square. This simple solution can greatly reduce the effect outliers have when calculating similarities.</p><p id="ad4c" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">Determining how prototypes should be calculated is difficult, and solutions such as using the median may break down with certain datasets. For example, if all of our cross class examples formed a circle of radius 1 around the origin and the square class examples formed a circle of radius 2, the prototypes would both now be formed at the origin, resulting in equal distance measurements. We’d need to find another way to calculate the prototypes for that dataset.</p><h1 id="2370" class="nv nw ev be nx ny nz gd oa ob oc gg od oe of og oh oi oj ok ol om on oo op oq bj">Building a Siamese Neural Network</h1><p id="0368" class="pw-post-body-paragraph mz na ev nb b gb or nd ne ge os ng nh ni ot nk nl nm ou no np nq ov ns nt nu eo bj">Now that we have a grasp of the underlying theory of SNNs and why they are an important tool, let’s take a look at how we build one. As mentioned previously, we’ll be using Python, Keras, and TensorFlow 1.14 for this although there’s really nothing preventing this code being converted for use in another framework like PyTorch; I use TensorFlow out of personal preference rather than because it’s better for making SNNs. We’re also going to stick with using MNIST as our dataset, both for consistency and for ease of training.</p><p id="6391" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">The code here is based on a variety of sources, which I will link as we go, but the underlying construction is based on the approach described in Amit Yadav’s <a class="af my" href="https://www.coursera.org/projects/siamese-network-triplet-loss-keras" rel="noopener ugc nofollow" target="_blank">Coursera,</a> which is itself based on FaceNet [5].</p><p id="6bc6" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">If you prefer to have full code rather than snippets, this is available from my <a class="af my" href="https://github.com/Trotts/Siamese-Neural-Network-MNIST-Triplet-Loss" rel="noopener ugc nofollow" target="_blank">Github</a>.</p><h2 id="d712" class="pm nw ev be nx pn po pp oa pq pr ps od ni pt pu pv nm pw px py nq pz qa qb fc bj">Step 1: Importing packages</h2><p id="c87b" class="pw-post-body-paragraph mz na ev nb b gb or nd ne ge os ng nh ni ot nk nl nm ou no np nq ov ns nt nu eo bj">First, we’re going to need to import the required packages. For a complete list of package versions used on the virtual machine to run this code, see <a class="af my" href="https://github.com/Trotts/Siamese-Neural-Network-MNIST-Triplet-Loss/blob/main/requirements.txt" rel="noopener ugc nofollow" target="_blank">here</a>. I tested this code with Python 3.6.7.</p><figure class="mg mh mi mj mk lu"><div class="qf ii l hi"><div class="qg qh l"></div></div></figure><h2 id="6ff1" class="pm nw ev be nx pn po pp oa pq pr ps od ni pt pu pv nm pw px py nq pz qa qb fc bj">Step 2: Importing data</h2><p id="41b6" class="pw-post-body-paragraph mz na ev nb b gb or nd ne ge os ng nh ni ot nk nl nm ou no np nq ov ns nt nu eo bj">Next, we need to import a dataset for our SNN to work with. As previously mentioned we’ll be using MNIST, which can be loaded using TensorFlow’s <code class="cw qi qj qk ql b">mnist.load_data()</code>.</p><p id="f7d8" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">After the data is loaded in, it is reshaped and flattened. This allows the data to be read into the SNN more easily.</p><figure class="mg mh mi mj mk lu"><div class="qf ii l hi"><div class="qg qh l"></div></div></figure><p id="42e7" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">Note that we only have a height and width here as MNIST is greyscale, therefore only has 1 colour channel. If we had a dataset with multiple colour channels we would need to adapt our code for this, for example using <code class="cw qi qj qk ql b">x_train_w_h_c</code> instead.</p><h2 id="9ff2" class="pm nw ev be nx pn po pp oa pq pr ps od ni pt pu pv nm pw px py nq pz qa qb fc bj">Step 3: Create the triplets</h2><p id="0bed" class="pw-post-body-paragraph mz na ev nb b gb or nd ne ge os ng nh ni ot nk nl nm ou no np nq ov ns nt nu eo bj">Now we need to create our MNIST triplets. Two methods are required for this.</p><p id="b6b8" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">The first, <code class="cw qi qj qk ql b">create_batch()</code>, generates triplets by randomly selecting two class labels, one for the Anchor/Positive and one for the Negative, before randomly selecting a class example for each.</p><figure class="mg mh mi mj mk lu"><div class="qf ii l hi"><div class="qg qh l"></div></div></figure><p id="dc73" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">The second, <code class="cw qi qj qk ql b">create_hard_batch()</code>, creates a batch of random triplets using <code class="cw qi qj qk ql b">create_batch()</code>, and embeds them using the current SNN. This allows us to determine which triplets in the batch are Semi-Hard; if they are we keep <code class="cw qi qj qk ql b">num_hard</code> of them, populating the rest of the batch with other random triplets. By padding with random triplets, we allow for training to begin as well as ensure our batches are of a consistent size.</p><figure class="mg mh mi mj mk lu"><div class="qf ii l hi"><div class="qg qh l"></div></div></figure><h2 id="4f7a" class="pm nw ev be nx pn po pp oa pq pr ps od ni pt pu pv nm pw px py nq pz qa qb fc bj">Step 4: Defining the SNN</h2><p id="ef5a" class="pw-post-body-paragraph mz na ev nb b gb or nd ne ge os ng nh ni ot nk nl nm ou no np nq ov ns nt nu eo bj">The SNN is defined in two parts. First, we must create the embedding model. This model receives an input image and generates a <em class="ox">d</em>-dimensional embedding. We create a very shallow embedding model here, but more complex models can be created.</p><figure class="mg mh mi mj mk lu"><div class="qf ii l hi"><div class="qg qh l"></div></div></figure><p id="ca48" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">Next, we create a model which receives a triplet, passes it to the embedding model sequentially for embedding, then passes the resultant embeddings to the triplet loss function.</p><figure class="mg mh mi mj mk lu"><div class="qf ii l hi"><div class="qg qh l"></div></div></figure><h2 id="7b92" class="pm nw ev be nx pn po pp oa pq pr ps od ni pt pu pv nm pw px py nq pz qa qb fc bj">Step 5: Defining the triplet loss function</h2><p id="40de" class="pw-post-body-paragraph mz na ev nb b gb or nd ne ge os ng nh ni ot nk nl nm ou no np nq ov ns nt nu eo bj">In order for the SNN to train using the triplets, we need to define the triplet loss function. This mirrors the triplet loss function equation shown previously.</p><figure class="mg mh mi mj mk lu"><div class="qf ii l hi"><div class="qg qh l"></div></div></figure><h2 id="9b9e" class="pm nw ev be nx pn po pp oa pq pr ps od ni pt pu pv nm pw px py nq pz qa qb fc bj">Step 6: Defining the data generator</h2><p id="e750" class="pw-post-body-paragraph mz na ev nb b gb or nd ne ge os ng nh ni ot nk nl nm ou no np nq ov ns nt nu eo bj">In order to pass our triplets to the network, we need to create a data generator function. Both an <code class="cw qi qj qk ql b">x</code> and <code class="cw qi qj qk ql b">y</code> is required here by TensorFlow, but we don’t need a <code class="cw qi qj qk ql b">y</code> value, so we pass a filler.</p><figure class="mg mh mi mj mk lu"><div class="qf ii l hi"><div class="qg qh l"></div></div></figure><h2 id="7cbf" class="pm nw ev be nx pn po pp oa pq pr ps od ni pt pu pv nm pw px py nq pz qa qb fc bj">Step 7: Setting up for training and evaluation</h2><p id="c2e6" class="pw-post-body-paragraph mz na ev nb b gb or nd ne ge os ng nh ni ot nk nl nm ou no np nq ov ns nt nu eo bj">Now that we have defined the basics of the SNN, we can set up the model for training. First, we define our hyperparameters. Next, we create and compile the models. I specify that this is performed using the CPU, but this may not be required depending on your setup.</p><p id="db63" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">Once the models are compiled, we store a subset of the test image embeddings. The model hasn’t been trained yet, so this gives us a good baseline to show how the embeddings have changed through the training process. Embedding visualisations via PCA are based on <a class="af my" href="https://github.com/AdrianUng/keras-triplet-loss-mnist/blob/master/Triplet_loss_KERAS_semi_hard_from_TF.ipynb" rel="noopener ugc nofollow" target="_blank">this notebook</a> by AdrianUng.</p><figure class="mg mh mi mj mk lu"><div class="qf ii l hi"><div class="qg qh l"></div></div></figure><p id="f687" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">Further evaluation can be performed on our SNN. Code used in this step is heavily influenced by <a class="ow hv ff" href="https://medium.com/u/d0c371fcaf92?source=post_page-----4c6da3259463--------------------------------" rel="noopener" target="_blank">Eric Craeymeersch</a>’s <a class="af my" href="https://medium.com/@crimy/one-shot-learning-siamese-networks-and-triplet-loss-with-keras-2885ed022352" rel="noopener">One Shot Learning, Siamese Networks and Triplet Loss with Keras</a> and <a class="af my" href="https://github.com/asagar60/One-Shot-Learning/blob/master/Omniglot_data/One_shot_implementation.ipynb" rel="noopener ugc nofollow" target="_blank">this notebook</a> from asagar60.</p><figure class="mg mh mi mj mk lu"><div class="qf ii l hi"><div class="qg qh l"></div></div></figure><p id="a2e0" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">Let’s take a look at the evaluation of the untrained model. From the plots, we can see our model is unable to distinguish between similar and dissimilar images. This is most pronounced in the third plot, highlighting the test images and their most likely classes, with very little difference between their scores.</p><figure class="mg mh mi mj mk lu md me paragraph-image"><div role="button" tabindex="0" class="mn mo hi mp bg mq"><div class="md me qm"><picture><source srcSet="https://miro.medium.com/v2/resize:fit:640/format:webp/1*-WzkLsvSdq19xw-PItdlqA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*-WzkLsvSdq19xw-PItdlqA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*-WzkLsvSdq19xw-PItdlqA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*-WzkLsvSdq19xw-PItdlqA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*-WzkLsvSdq19xw-PItdlqA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*-WzkLsvSdq19xw-PItdlqA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-WzkLsvSdq19xw-PItdlqA.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"/><source data-testid="og" srcSet="https://miro.medium.com/v2/resize:fit:640/1*-WzkLsvSdq19xw-PItdlqA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*-WzkLsvSdq19xw-PItdlqA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*-WzkLsvSdq19xw-PItdlqA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*-WzkLsvSdq19xw-PItdlqA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*-WzkLsvSdq19xw-PItdlqA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*-WzkLsvSdq19xw-PItdlqA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*-WzkLsvSdq19xw-PItdlqA.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"/><img alt="1: an AUC plot showing an AUC @ 0.663, 2: boxplot showing embedding distances between classes, 3: Images and likely classes" class="bg mr ms c" width="700" height="772" loading="lazy"/></picture></div></div><figcaption class="mt mu mv md me mw mx be b bf z fd">Image generated using code based on <a class="af my" href="https://medium.com/@crimy/one-shot-learning-siamese-networks-and-triplet-loss-with-keras-2885ed022352" rel="noopener">this notebook</a>.</figcaption></figure><p id="5476" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">Now we have compiled the models, we can also generate example random and Semi-Hard triplets. This code is based on a <a class="af my" href="https://zhangruochi.com/Create-a-Siamese-Network-with-Triplet-Loss-in-Keras/2020/08/11/" rel="noopener ugc nofollow" target="_blank">blog post</a> by Ruochi Zang.</p><figure class="mg mh mi mj mk lu"><div class="qf ii l hi"><div class="qg qh l"></div></div></figure><p id="8bf9" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">This produces the following:</p><figure class="mg mh mi mj mk lu md me paragraph-image"><div class="md me qn"><picture><source srcSet="https://miro.medium.com/v2/resize:fit:640/format:webp/1*oDuvzAgmWtgaF3sBVyaPtQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*oDuvzAgmWtgaF3sBVyaPtQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*oDuvzAgmWtgaF3sBVyaPtQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*oDuvzAgmWtgaF3sBVyaPtQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*oDuvzAgmWtgaF3sBVyaPtQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*oDuvzAgmWtgaF3sBVyaPtQ.png 1100w, https://miro.medium.com/v2/resize:fit:758/format:webp/1*oDuvzAgmWtgaF3sBVyaPtQ.png 758w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 379px" type="image/webp"/><source data-testid="og" srcSet="https://miro.medium.com/v2/resize:fit:640/1*oDuvzAgmWtgaF3sBVyaPtQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*oDuvzAgmWtgaF3sBVyaPtQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*oDuvzAgmWtgaF3sBVyaPtQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*oDuvzAgmWtgaF3sBVyaPtQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*oDuvzAgmWtgaF3sBVyaPtQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*oDuvzAgmWtgaF3sBVyaPtQ.png 1100w, https://miro.medium.com/v2/resize:fit:758/1*oDuvzAgmWtgaF3sBVyaPtQ.png 758w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 379px"/><img alt="output of the above code showing an example random triplet (4, 4, 6) and a semi-hard triplet (8,8,6) all look similar." class="bg mr ms c" width="379" height="324" loading="lazy"/></picture></div><figcaption class="mt mu mv md me mw mx be b bf z fd">Image generated using code based on <a class="af my" href="https://zhangruochi.com/Create-a-Siamese-Network-with-Triplet-Loss-in-Keras/2020/08/11/" rel="noopener ugc nofollow" target="_blank">this blog post</a>.</figcaption></figure><p id="a16b" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">Our example random triplet contains an Anchor and Positive of class 4, and a Negative of class 6. Our Semi-Hard triplet contains an Anchor and Positive of class 8, and a Negative of class 6, but note how similar they are in composition.</p><h2 id="2239" class="pm nw ev be nx pn po pp oa pq pr ps od ni pt pu pv nm pw px py nq pz qa qb fc bj">Step 8: Logging output from our model training</h2><p id="c625" class="pw-post-body-paragraph mz na ev nb b gb or nd ne ge os ng nh ni ot nk nl nm ou no np nq ov ns nt nu eo bj">Let’s set up some logging and custom callbacks before we train our model, to help us if we need to come back at a later date. The Tensorboard logging callback is adapted from erenon’s <a class="af my" href="https://stackoverflow.com/questions/44861149/keras-use-tensorboard-with-train-on-batch/52581175#52581175" rel="noopener ugc nofollow" target="_blank">helpful Stack Overflow answer,</a> whilst the saving of the best model based on the validation loss is adapted from <a class="af my" href="https://stackoverflow.com/questions/58103035/how-can-we-perform-early-stopping-with-train-on-batch/58103272#58103272" rel="noopener ugc nofollow" target="_blank">another Stack Overflow answer</a> from OverLordGoldDragon.</p><figure class="mg mh mi mj mk lu"><div class="qf ii l hi"><div class="qg qh l"></div></div></figure><h2 id="a155" class="pm nw ev be nx pn po pp oa pq pr ps od ni pt pu pv nm pw px py nq pz qa qb fc bj">Step 9: Training the SNN</h2><p id="6ec3" class="pw-post-body-paragraph mz na ev nb b gb or nd ne ge os ng nh ni ot nk nl nm ou no np nq ov ns nt nu eo bj">Now that all of our setup has been completed, it is time to start training! I first begin by selecting the total number of GPUs available, and parallelising the model training over them. You may need to amend this should you not have access to multiple GPUs.</p><figure class="mg mh mi mj mk lu"><div class="qf ii l hi"><div class="qg qh l"></div></div></figure><p id="1bc3" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">Note that when running <code class="cw qi qj qk ql b">model.fit()</code> we provide train and test data generators rather than the train and test data directly. This allows for online triplet mining to occur.</p><h2 id="9563" class="pm nw ev be nx pn po pp oa pq pr ps od ni pt pu pv nm pw px py nq pz qa qb fc bj">Step 10: Evaluating the trained model</h2><p id="df57" class="pw-post-body-paragraph mz na ev nb b gb or nd ne ge os ng nh ni ot nk nl nm ou no np nq ov ns nt nu eo bj">Once the model has trained, we can then evaluate it and compare its embeddings. First, we load in the trained model. I do this by reloading the saved logging files, but if you’re just running this all in one notebook as a closed system, there isn’t really a need to reload once the model is trained.</p><p id="cff2" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">Once the models are loaded, we perform the same PCA decomposition we did on the untrained model to visualise how the embeddings have changed.</p><figure class="mg mh mi mj mk lu"><div class="qf ii l hi"><div class="qg qh l"></div></div></figure><p id="6744" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">At the end of the above code-block, we run <code class="cw qi qj qk ql b">evaluate()</code> again, which produces the below graphs:</p><figure class="mg mh mi mj mk lu md me paragraph-image"><div role="button" tabindex="0" class="mn mo hi mp bg mq"><div class="md me qm"><picture><source srcSet="https://miro.medium.com/v2/resize:fit:640/format:webp/1*PbgRcxPHbYyqxaNGTg845Q.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*PbgRcxPHbYyqxaNGTg845Q.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*PbgRcxPHbYyqxaNGTg845Q.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*PbgRcxPHbYyqxaNGTg845Q.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*PbgRcxPHbYyqxaNGTg845Q.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*PbgRcxPHbYyqxaNGTg845Q.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PbgRcxPHbYyqxaNGTg845Q.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"/><source data-testid="og" srcSet="https://miro.medium.com/v2/resize:fit:640/1*PbgRcxPHbYyqxaNGTg845Q.png 640w, https://miro.medium.com/v2/resize:fit:720/1*PbgRcxPHbYyqxaNGTg845Q.png 720w, https://miro.medium.com/v2/resize:fit:750/1*PbgRcxPHbYyqxaNGTg845Q.png 750w, https://miro.medium.com/v2/resize:fit:786/1*PbgRcxPHbYyqxaNGTg845Q.png 786w, https://miro.medium.com/v2/resize:fit:828/1*PbgRcxPHbYyqxaNGTg845Q.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*PbgRcxPHbYyqxaNGTg845Q.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*PbgRcxPHbYyqxaNGTg845Q.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"/><img alt="1: an AUC plot showing an AUC @ 0.985, 2: boxplot of embedding distances between classes, 3: Images and likely classes" class="bg mr ms c" width="700" height="733" loading="lazy"/></picture></div></div><figcaption class="mt mu mv md me mw mx be b bf z fd">Image generated using code based on <a class="af my" href="https://medium.com/@crimy/one-shot-learning-siamese-networks-and-triplet-loss-with-keras-2885ed022352" rel="noopener">this notebook</a>.</figcaption></figure><p id="3a6f" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">Note how the first plot now shows an AUC of 0.985 and increased distance between our classes. Interestingly, when looking at the test images and their most likely classes, we can see for the 2nd and 3rd test images the corresponding class has been correctly achieved (for example, taking the 2nd test image, of class 0, we can see the lowest score for all the support set classes is also at class 0), however looking at the 1st test image, all of the scores for the support set classes are very close, indicating the trained model has had difficulties classifying this image.</p><p id="e788" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">To confirm our model has trained correctly and class clusters are now forming, lets plot the PCA decomposed embeddings we have stored previously.</p><figure class="mg mh mi mj mk lu"><div class="qf ii l hi"><div class="qg qh l"></div></div></figure><p id="6662" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">This code produces the following output:</p><figure class="mg mh mi mj mk lu md me paragraph-image"><div role="button" tabindex="0" class="mn mo hi mp bg mq"><div class="md me qo"><picture><source srcSet="https://miro.medium.com/v2/resize:fit:640/format:webp/1*RAc1pg2Z03wMfzQf7tXK0Q.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*RAc1pg2Z03wMfzQf7tXK0Q.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*RAc1pg2Z03wMfzQf7tXK0Q.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*RAc1pg2Z03wMfzQf7tXK0Q.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*RAc1pg2Z03wMfzQf7tXK0Q.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*RAc1pg2Z03wMfzQf7tXK0Q.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RAc1pg2Z03wMfzQf7tXK0Q.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"/><source data-testid="og" srcSet="https://miro.medium.com/v2/resize:fit:640/1*RAc1pg2Z03wMfzQf7tXK0Q.png 640w, https://miro.medium.com/v2/resize:fit:720/1*RAc1pg2Z03wMfzQf7tXK0Q.png 720w, https://miro.medium.com/v2/resize:fit:750/1*RAc1pg2Z03wMfzQf7tXK0Q.png 750w, https://miro.medium.com/v2/resize:fit:786/1*RAc1pg2Z03wMfzQf7tXK0Q.png 786w, https://miro.medium.com/v2/resize:fit:828/1*RAc1pg2Z03wMfzQf7tXK0Q.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*RAc1pg2Z03wMfzQf7tXK0Q.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*RAc1pg2Z03wMfzQf7tXK0Q.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"/><img alt="Left: data points before training, class examples are all mixed. Right: same data but after training, mostly clustered." class="bg mr ms c" width="700" height="356" loading="lazy"/></picture></div></div><figcaption class="mt mu mv md me mw mx be b bf z fd">Image generated using code based on <a class="af my" href="https://github.com/AdrianUng/keras-triplet-loss-mnist/blob/master/Triplet_loss_KERAS_semi_hard_from_TF.ipynb" rel="noopener ugc nofollow" target="_blank">this notebook</a>.</figcaption></figure><p id="317c" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">The left plot shows the embedding locations before training, decomposed into 2-dimensions using PCA for visualisation, and with each colour representing a distinct class as shown by the legend. Note how the embeddings are all jumbled up, there is no clear clustering structure, which makes sense as the model has not learned to separate the classes out. This is in contrast to the right plot, which shows the same data points embedded by a trained SNN. We can see clear clustering on the outskirts of the plot, but the middle is still looking a bit messy. Our plot indicates the model has learned very well to cluster embedded images of class 1 for example (the cluster in the bottom left), but still struggles with embedded images of class 5, which are still mostly in the centre. This is backed up by our previous plots, which shows the model struggling to determine a most likely match for the class 5 test image.</p><p id="b5f2" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">It would be good to quantify how well our model is performing. This can be achieved using an n-way accuracy score, utilising the prototypes we discussed before. In n-way accuracy, <code class="cw qi qj qk ql b">val_steps</code> number of randomly selected test images are compared to a support set of size <em class="ox">n. </em>This provides an indication of model accuracy when <em class="ox">n</em> is the same as the total number of classes, <code class="cw qi qj qk ql b">num_classes</code> in the code below. MNIST has 10 classes, giving a 10-way accuracy.</p><figure class="mg mh mi mj mk lu"><div class="qf ii l hi"><div class="qg qh l"></div></div></figure><p id="faa5" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">When we run the above code, <strong class="nb fi">the SNN achieves a 10-way accuracy of 97.4%</strong>, a commendable score. Due to the random nature with which the test images are chosen, you could perform cross validation here if you wish.</p><p id="93a1" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">Finally, lets take a look at producing a support set image, similar to that shown in the previous moths example, only this time generated using MNIST. Again, we will use a 10 class support set.</p><figure class="mg mh mi mj mk lu"><div class="qf ii l hi"><div class="qg qh l"></div></div></figure><p id="4ee8" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">This produces the following plot:</p><figure class="mg mh mi mj mk lu md me paragraph-image"><div role="button" tabindex="0" class="mn mo hi mp bg mq"><div class="md me qp"><picture><source srcSet="https://miro.medium.com/v2/resize:fit:640/format:webp/1*WbvyHcsIsql3NaOJUDZe3w.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*WbvyHcsIsql3NaOJUDZe3w.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*WbvyHcsIsql3NaOJUDZe3w.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*WbvyHcsIsql3NaOJUDZe3w.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*WbvyHcsIsql3NaOJUDZe3w.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*WbvyHcsIsql3NaOJUDZe3w.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WbvyHcsIsql3NaOJUDZe3w.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"/><source data-testid="og" srcSet="https://miro.medium.com/v2/resize:fit:640/1*WbvyHcsIsql3NaOJUDZe3w.png 640w, https://miro.medium.com/v2/resize:fit:720/1*WbvyHcsIsql3NaOJUDZe3w.png 720w, https://miro.medium.com/v2/resize:fit:750/1*WbvyHcsIsql3NaOJUDZe3w.png 750w, https://miro.medium.com/v2/resize:fit:786/1*WbvyHcsIsql3NaOJUDZe3w.png 786w, https://miro.medium.com/v2/resize:fit:828/1*WbvyHcsIsql3NaOJUDZe3w.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*WbvyHcsIsql3NaOJUDZe3w.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*WbvyHcsIsql3NaOJUDZe3w.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"/><img alt="L: image of a 2 labelled ‘Test Image’. M: Support set with numbers 0–9, 2 is first. R: The support set 2 image on its own." class="bg mr ms c" width="700" height="780" loading="lazy"/></picture></div></div><figcaption class="mt mu mv md me mw mx be b bf z fd">Image generated using code based on <a class="af my" href="https://github.com/asagar60/One-Shot-Learning/blob/master/Omniglot_data/One_shot_implementation.ipynb" rel="noopener ugc nofollow" target="_blank">this notebook</a>.</figcaption></figure><p id="063c" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">If the moth example discussed earlier in this article was confusing, hopefully the same plot using MNIST is clearer. The code has randomly selected a class 2 test image to classify, which is compared to the prototypes of all other classes in the support set. Again, the plotting code knows the test image is of class 2, and so the support set 2 is shown first. On the right, the same support set 2 is shown again, indicating the SNN has correctly determined a most likely class of 2 for the test image, which it should do for other test images approximately 97.4% of the time!</p><h1 id="cdec" class="nv nw ev be nx ny nz gd oa ob oc gg od oe of og oh oi oj ok ol om on oo op oq bj">Conclusion</h1><p id="8876" class="pw-post-body-paragraph mz na ev nb b gb or nd ne ge os ng nh ni ot nk nl nm ou no np nq ov ns nt nu eo bj">In this article, we have learned what a Siamese Neural Network is, how to train them, and how to utilise them at inference time. Even though we have utilised a toy example through the use of MNIST, I hope that it is clear how powerful SNNs can be when working with open-ended datasets where you may not have all classes available to you at the time of dataset creation, and how new unseen-at-train-time classes would be handled by the model.</p><p id="e9fa" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">I hope that I have provided you with a good balance of theoretical knowledge and practical application, and I’d like to thank everyone whom I have mentioned throughout for providing open-source code and Stack Overflow answers. This article, and indeed my own work in conservation tech, would not have been possible without it.</p><p id="a5e1" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">When I first started writing, I was worried there wouldn’t be enough content for it to be worthwhile. Now that it is finished, I realise how wrong I was! Hopefully if you have made it this far (and haven’t just skipped to the end) this article has taught you something, and if this is the case please do let me know on <a class="af my" href="https://twitter.com/camtrotts" rel="noopener ugc nofollow" target="_blank">Twitter</a> or <a class="af my" href="https://www.linkedin.com/in/cameron-trotter-0b6594109/" rel="noopener ugc nofollow" target="_blank">LinkedIn</a>.</p></div></div></div><div class="ab ca qq qr qs qt" role="separator"><span class="qu bx bl qv qw qx"></span><span class="qu bx bl qv qw qx"></span><span class="qu bx bl qv qw"></span></div><div class="eo ep eq er es"><div class="ab ca"><div class="ch bg dx dy dz ea"><h1 id="a69e" class="nv nw ev be nx ny qy gd oa ob qz gg od oe ra og oh oi rb ok ol om rc oo op oq bj">References</h1><p id="01ad" class="pw-post-body-paragraph mz na ev nb b gb or nd ne ge os ng nh ni ot nk nl nm ou no np nq ov ns nt nu eo bj">[1] Deng, J., Dong, W., Socher, R., Li, L.J., Li, K. and Fei-Fei, L., 2009, June. Imagenet: A large-scale hierarchical image database. In <em class="ox">2009 IEEE conference on computer vision and pattern recognition</em> (pp. 248–255). IEEE.</p><p id="07a4" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">[2] Dey, S., Dutta, A., Toledo, J.I., Ghosh, S.K., Lladós, J. and Pal, U., 2017. Signet: Convolutional siamese network for writer independent offline signature verification. <em class="ox">arXiv preprint arXiv:1707.02131</em>.</p><p id="0bab" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">[3] LeCun, Y., Bottou, L., Bengio, Y. and Haffner, P., 1998. Gradient-based learning applied to document recognition. <em class="ox">Proceedings of the IEEE</em>, <em class="ox">86</em>(11), pp.2278–2324.</p><p id="a8cb" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">[4] Hoffer, E. and Ailon, N., 2015, October. Deep metric learning using triplet network. In <em class="ox">International Workshop on Similarity-Based Pattern Recognition</em> (pp. 84–92). Springer, Cham.</p><p id="4f77" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">[5] Schroff, Florian, Dmitry Kalenichenko, and James Philbin. Facenet: A unified embedding for face recognition and clustering. In <em class="ox">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pp. 815–823. 2015.</p><p id="8cc9" class="pw-post-body-paragraph mz na ev nb b gb nc nd ne ge nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu eo bj">[6] Vetrova, V., Coup, S., Frank, E. and Cree, M.J., 2018, November. Hidden features: Experiments with feature transfer for fine-grained multi-class and one-class image categorization. In <em class="ox">2018 International Conference on Image and Vision Computing New Zealand (IVCNZ)</em> (pp. 1–6). IEEE.</p></div></div></div></div></section></div></div></article><div class="ab ca"><div class="ch bg dx dy dz ea"></div></div></div><div class="ab ca"><div class="ch bg dx dy dz ea"><div class="rd re ab id"><div class="rf ab"><a class="rg ax am ao" href="https://medium.com/tag/machine-learning?source=post_page-----4c6da3259463---------------machine_learning-----------------" rel="noopener follow"><div class="rh hi cw ri ed rj rk be b bf z bj rl">Machine Learning</div></a></div><div class="rf ab"><a class="rg ax am ao" href="https://medium.com/tag/siamese-networks?source=post_page-----4c6da3259463---------------siamese_networks-----------------" rel="noopener follow"><div class="rh hi cw ri ed rj rk be b bf z bj rl">Siamese Networks</div></a></div><div class="rf ab"><a class="rg ax am ao" href="https://medium.com/tag/convolutional-network?source=post_page-----4c6da3259463---------------convolutional_network-----------------" rel="noopener follow"><div class="rh hi cw ri ed rj rk be b bf z bj rl">Convolutional Network</div></a></div><div class="rf ab"><a class="rg ax am ao" href="https://medium.com/tag/editors-pick?source=post_page-----4c6da3259463---------------editors_pick-----------------" rel="noopener follow"><div class="rh hi cw ri ed rj rk be b bf z bj rl">Editors Pick</div></a></div><div class="rf ab"><a class="rg ax am ao" href="https://medium.com/tag/hands-on-tutorials?source=post_page-----4c6da3259463---------------hands_on_tutorials-----------------" rel="noopener follow"><div class="rh hi cw ri ed rj rk be b bf z bj rl">Hands On Tutorials</div></a></div></div></div></div><div class="l"></div><footer class="rm rn ro rp rq rr rs rt ru ab q rv hm c"><div class="l ae"><div class="ab ca"><div class="ch bg dx dy dz ea"><div class="ab co rw"><div class="ab q ka"><div class="rx l"><span class="l ry rz sa e d"><div class="ab q ka"><div class="pw-multi-vote-icon hi ih kb kc kd"><div class=""><div class="ke kf kg kh ki kj kk am kl km kn kd"><svg width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" clip-rule="evenodd" d="M11.37.83L12 3.28l.63-2.45h-1.26zM15.42 1.84l-1.18-.39-.34 2.5 1.52-2.1zM9.76 1.45l-1.19.4 1.53 2.1-.34-2.5zM20.25 11.84l-2.5-4.4a1.42 1.42 0 0 0-.93-.64.96.96 0 0 0-.75.18c-.25.19-.4.42-.45.7l.05.05 2.35 4.13c1.62 2.95 1.1 5.78-1.52 8.4l-.46.41c1-.13 1.93-.6 2.78-1.45 2.7-2.7 2.51-5.59 1.43-7.38zM12.07 9.01c-.13-.69.08-1.3.57-1.77l-2.06-2.07a1.12 1.12 0 0 0-1.56 0c-.15.15-.22.34-.27.53L12.07 9z"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.74 8.3a1.13 1.13 0 0 0-.73-.5.67.67 0 0 0-.53.13c-.15.12-.59.46-.2 1.3l1.18 2.5a.45.45 0 0 1-.23.76.44.44 0 0 1-.48-.25L7.6 6.11a.82.82 0 1 0-1.15 1.15l3.64 3.64a.45.45 0 1 1-.63.63L5.83 7.9 4.8 6.86a.82.82 0 0 0-1.33.9c.04.1.1.18.18.26l1.02 1.03 3.65 3.64a.44.44 0 0 1-.15.73.44.44 0 0 1-.48-.1L4.05 9.68a.82.82 0 0 0-1.4.57.81.81 0 0 0 .24.58l1.53 1.54 2.3 2.28a.45.45 0 0 1-.64.63L3.8 13a.81.81 0 0 0-1.39.57c0 .22.09.43.24.58l4.4 4.4c2.8 2.8 5.5 4.12 8.68.94 2.27-2.28 2.71-4.6 1.34-7.1l-2.32-4.08z"></path></svg></div></div></div><div class="pw-multi-vote-count l ko kp kq kr ks kt ku"><p class="be b ew z fd"><span class="kf">--</span></p></div></div></span><span class="l h g f sb sc"><div class="ab q ka"><div class="pw-multi-vote-icon hi ih kb kc kd"><div class=""><div class="ke kf kg kh ki kj kk am kl km kn kd"><svg width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" clip-rule="evenodd" d="M11.37.83L12 3.28l.63-2.45h-1.26zM15.42 1.84l-1.18-.39-.34 2.5 1.52-2.1zM9.76 1.45l-1.19.4 1.53 2.1-.34-2.5zM20.25 11.84l-2.5-4.4a1.42 1.42 0 0 0-.93-.64.96.96 0 0 0-.75.18c-.25.19-.4.42-.45.7l.05.05 2.35 4.13c1.62 2.95 1.1 5.78-1.52 8.4l-.46.41c1-.13 1.93-.6 2.78-1.45 2.7-2.7 2.51-5.59 1.43-7.38zM12.07 9.01c-.13-.69.08-1.3.57-1.77l-2.06-2.07a1.12 1.12 0 0 0-1.56 0c-.15.15-.22.34-.27.53L12.07 9z"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.74 8.3a1.13 1.13 0 0 0-.73-.5.67.67 0 0 0-.53.13c-.15.12-.59.46-.2 1.3l1.18 2.5a.45.45 0 0 1-.23.76.44.44 0 0 1-.48-.25L7.6 6.11a.82.82 0 1 0-1.15 1.15l3.64 3.64a.45.45 0 1 1-.63.63L5.83 7.9 4.8 6.86a.82.82 0 0 0-1.33.9c.04.1.1.18.18.26l1.02 1.03 3.65 3.64a.44.44 0 0 1-.15.73.44.44 0 0 1-.48-.1L4.05 9.68a.82.82 0 0 0-1.4.57.81.81 0 0 0 .24.58l1.53 1.54 2.3 2.28a.45.45 0 0 1-.64.63L3.8 13a.81.81 0 0 0-1.39.57c0 .22.09.43.24.58l4.4 4.4c2.8 2.8 5.5 4.12 8.68.94 2.27-2.28 2.71-4.6 1.34-7.1l-2.32-4.08z"></path></svg></div></div></div><div class="pw-multi-vote-count l ko kp kq kr ks kt ku"><p class="be b ew z fd"><span class="kf">--</span></p></div></div></span></div><div class="bp ab"><div><div class="bl" aria-hidden="false"><button class="ao ke kv kw ab q kx ky kz" aria-label="responses"><svg width="24" height="24" viewBox="0 0 24 24" class="la"><path d="M18 16.8a7.14 7.14 0 0 0 2.24-5.32c0-4.12-3.53-7.48-8.05-7.48C7.67 4 4 7.36 4 11.48c0 4.13 3.67 7.48 8.2 7.48a8.9 8.9 0 0 0 2.38-.32c.23.2.48.39.75.56 1.06.69 2.2 1.04 3.4 1.04.22 0 .4-.11.48-.29a.5.5 0 0 0-.04-.52 6.4 6.4 0 0 1-1.16-2.65v.02zm-3.12 1.06l-.06-.22-.32.1a8 8 0 0 1-2.3.33c-4.03 0-7.3-2.96-7.3-6.59S8.17 4.9 12.2 4.9c4 0 7.1 2.96 7.1 6.6 0 1.8-.6 3.47-2.02 4.72l-.2.16v.26l.02.3a6.74 6.74 0 0 0 .88 2.4 5.27 5.27 0 0 1-2.17-.86c-.28-.17-.72-.38-.94-.59l.01-.02z"></path></svg></button></div></div></div></div><div class="ab q"><div class="qx l ia"></div><div class="qx l ia"><div class="bl" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bl" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="footerSocialShareButton" class="af kx ah ai aj ak al lc an ao ap hx ld le kz lf"><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path fill-rule="evenodd" clip-rule="evenodd" d="M15.22 4.93a.42.42 0 0 1-.12.13h.01a.45.45 0 0 1-.29.08.52.52 0 0 1-.3-.13L12.5 3v7.07a.5.5 0 0 1-.5.5.5.5 0 0 1-.5-.5V3.02l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.8a.42.42 0 0 1 .07.5zm-.1.14zm.88 2h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11a2 2 0 0 1-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.14c.1.1.15.22.15.35a.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9V8.96c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1z" fill="currentColor"></path></svg></button></div></div></div></div></div></div></div></div></div></footer><div class="sd se sf sg sh l bw"><div class="ab ca"><div class="ch bg dx dy dz ea"><div class="ck ab si co"><div class="ab ha"><a href="https://medium.com/@c.trotter2?source=post_page-----4c6da3259463--------------------------------" rel="noopener follow"><div class="l sj sk bx sl he"><div class="l hi"><img alt="Cameron Trotter" class="l ec bx sm sn cw" src="https://miro.medium.com/v2/resize:fill:144:144/1*tA2cVcxhp0PDoY9CDtvL0g.png" width="72" height="72" loading="lazy"/><div class="hf bx l sm sn eh n hg hh"></div></div></div></a><a href="https://towardsdatascience.com/?source=post_page-----4c6da3259463--------------------------------" rel="noopener follow"><div class="so ab hi"><div><div class="bl" aria-hidden="false"><div class="l sp sq bx sl hm"><div class="l hi"><img alt="Towards Data Science" class="l ec bx by bz cw" src="https://miro.medium.com/v2/resize:fill:64:64/1*CJe3891yB1A1mzMdqemkdg.jpeg" width="32" height="32" loading="lazy"/><div class="hf bx l by bz eh n hg hh"></div></div></div></div></div></div></a></div><div class="j i d"><div class="ab"><span><a class="be b bf z sr rh ss st su sv sw sx sy hx sz ta tb tc td te ec bl ff mu" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9eff5ca21d80&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-train-your-siamese-neural-network-4c6da3259463&amp;user=Cameron+Trotter&amp;userId=9eff5ca21d80&amp;source=post_page-9eff5ca21d80----4c6da3259463---------------------follow_profile-----------" rel="noopener follow">Follow</a></span><div class="tf l"><div><div><div class="bl" aria-hidden="false"><div class="l"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ff1843dddb687&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-train-your-siamese-neural-network-4c6da3259463&amp;newsletterV3=9eff5ca21d80&amp;newsletterV3Id=f1843dddb687&amp;user=Cameron+Trotter&amp;userId=9eff5ca21d80&amp;source=-----4c6da3259463---------------------subscribe_user-----------" rel="noopener follow"><button class="be b bf z th am ti tj tk tl tm tn to tp sy hx sz ta tb td te ec bl ff mu" aria-label="Subscribe"><svg width="38" height="38" viewBox="0 0 38 38" fill="none" class="tg sq sp"><rect x="26.25" y="9.25" width="0.5" height="6.5" rx="0.25"></rect><rect x="29.75" y="12.25" width="0.5" height="6.5" rx="0.25" transform="rotate(90 29.75 12.25)"></rect><path d="M19.5 12.5h-7a1 1 0 0 0-1 1v11a1 1 0 0 0 1 1h13a1 1 0 0 0 1-1v-5"></path><path d="M11.5 14.5L19 20l4-3"></path></svg></button></a></span></div></div></div></div></div></div></div></div><div class="ab cm co"><div class="l"><div class="ab q"><a class="af ag ah ai aj ak al am an ao ap aq ar as at ab q" href="https://medium.com/@c.trotter2?source=post_page-----4c6da3259463--------------------------------" rel="noopener follow"><h2 class="pw-author-name be tq tr ts tt bj"><span class="eo">Written by <!-- -->Cameron Trotter</span></h2></a></div><div class="rf ab"><div class="l ia"><span class="pw-follower-count be b bf z bj"><a class="af ag ah ai aj ak al am an ao ap aq ar hs" href="https://medium.com/@c.trotter2/followers?source=post_page-----4c6da3259463--------------------------------" rel="noopener follow">28 Followers</a></span></div><div class="be b bf z ii ij ik ab im in io ip fd ig"><span class="ht l" aria-hidden="true"><span class="be b bf z fd">·</span></span><span class="l ia">Writer for </span><div><div class="bl" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hs ab q" href="https://towardsdatascience.com/?source=post_page-----4c6da3259463--------------------------------" rel="noopener follow"><p class="be b bf z ii ij ik il im in io ip bj">Towards Data Science</p></a></div></div></div></div><div class="tu l"><p class="be b bf z bj"><span class="eo">Computer Vision PhD Student @ Newcastle University (UK)</span></p></div></div><div class="h k"><div class="ab"><span><a class="be b bf z sr rh ss st su sv sw sx sy hx sz ta tb tc td te ec bl ff mu" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9eff5ca21d80&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-train-your-siamese-neural-network-4c6da3259463&amp;user=Cameron+Trotter&amp;userId=9eff5ca21d80&amp;source=post_page-9eff5ca21d80----4c6da3259463---------------------follow_profile-----------" rel="noopener follow">Follow</a></span><div class="tf l"><div><div><div class="bl" aria-hidden="false"><div class="l"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ff1843dddb687&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-train-your-siamese-neural-network-4c6da3259463&amp;newsletterV3=9eff5ca21d80&amp;newsletterV3Id=f1843dddb687&amp;user=Cameron+Trotter&amp;userId=9eff5ca21d80&amp;source=-----4c6da3259463---------------------subscribe_user-----------" rel="noopener follow"><button class="be b bf z th am ti tj tk tl tm tn to tp sy hx sz ta tb td te ec bl ff mu" aria-label="Subscribe"><svg width="38" height="38" viewBox="0 0 38 38" fill="none" class="tg sq sp"><rect x="26.25" y="9.25" width="0.5" height="6.5" rx="0.25"></rect><rect x="29.75" y="12.25" width="0.5" height="6.5" rx="0.25" transform="rotate(90 29.75 12.25)"></rect><path d="M19.5 12.5h-7a1 1 0 0 0-1 1v11a1 1 0 0 0 1 1h13a1 1 0 0 0 1-1v-5"></path><path d="M11.5 14.5L19 20l4-3"></path></svg></button></a></span></div></div></div></div></div></div></div></div><div class="tv bg tw tx ty tz ua ub"></div></div></div><div class="ab ca"><div class="ch bg dx dy dz ea"><div class="uj uk l"><h2 class="be tq hq z fh bj">More from <!-- -->Cameron Trotter<!-- --> and Towards Data Science</h2></div><div class="ul ab ka id um un uo up uq ur us ut uu uv uw ux uy uz va"><div class="vb vc vd lw ve vf vg ly vh vi vj vk vl vm vn vo vp vq vr vs vt"><div class="vu vv vw vx vy vz l"><article class="vz"><div class="vz ru l"><div class="bg vz"><div class="vz l"><div class="vz wa wb wc wd we wf wg wh wi wj wk wl wm"><div class="wn"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" aria-label="Mastering Customer Segmentation with LLM" rel="noopener follow" href="/mastering-customer-segmentation-with-llm-3d9008235f41?source=author_recirc-----4c6da3259463----0---------------------3a083127_c893_4424_b19c_c1eccf1006a9-------"><div class="wp wq wr ws wt"><img alt="Mastering Customer Segmentation with LLM" class="bg wu wv ww wx bw" src="https://miro.medium.com/v2/resize:fit:1358/1*Smqluk9f53V0NlpnknnHvw.png" loading="lazy"/></div></a></div><div class="wo ab ca cn"><div class="wy wz xa xb xc ab"><div class="rg l"><div><div class="bl" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@damiangilgonzalez?source=author_recirc-----4c6da3259463----0---------------------3a083127_c893_4424_b19c_c1eccf1006a9-------" rel="noopener follow"><div class="l hi"><img alt="Damian Gil" class="l ec bx xe xf cw" src="https://miro.medium.com/v2/resize:fill:40:40/1*g7rD02yuhg7PQyT7BIs0Ag.jpeg" width="20" height="20" loading="lazy"/><div class="xd bx l xe xf eh n ax hh"></div></div></a></div></div></div><div class="xg l"><div><div class="bl" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hs ab q" href="https://medium.com/@damiangilgonzalez?source=author_recirc-----4c6da3259463----0---------------------3a083127_c893_4424_b19c_c1eccf1006a9-------" rel="noopener follow"><p class="be b ew z ii ij ik il im in io ip bj">Damian Gil</p></a></div></div></div><div class="xg l"><p class="be b ew z fd">in</p></div><div class="l"><div><div class="bl" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hs ab q" href="https://towardsdatascience.com/?source=author_recirc-----4c6da3259463----0---------------------3a083127_c893_4424_b19c_c1eccf1006a9-------" rel="noopener follow"><p class="be b ew z ii ij ik il im in io ip bj">Towards Data Science</p></a></div></div></div></div><div class="xh xi xj xk xl xm xn xo xp xq l eo"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="/mastering-customer-segmentation-with-llm-3d9008235f41?source=author_recirc-----4c6da3259463----0---------------------3a083127_c893_4424_b19c_c1eccf1006a9-------"><div title=""><h2 class="be fi ny gd xr xs oa ob gg xt xu od ni pu xv xw pv nm px xx xy py nq qa xz ya qb ii ik il in ip bj">Mastering Customer Segmentation with LLM</h2></div><div class="yb l"><h3 class="be b hq z ii yc ik il yd in ip fd">Unlock advanced customer segmentation techniques using LLMs, and improve your clustering models with advanced techniques</h3></div></a></div><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="/mastering-customer-segmentation-with-llm-3d9008235f41?source=author_recirc-----4c6da3259463----0---------------------3a083127_c893_4424_b19c_c1eccf1006a9-------"><span class="be b ew z fd"><div class="ab q"><span>24 min read</span><span class="ht l" aria-hidden="true"><span class="be b bf z fd">·</span></span><span>Sep 26</span></div></span></a><div class="ye yf yg yh yi l"><div class="ab co"><div class="am yj yk yl ym yn yo yp yq yr ys ab q"><div class="ab q ka"><div class="pw-multi-vote-icon hi ih kb kc kd"><div class=""><div class="ke kf kg kh ki kj kk am kl km kn kd"><svg width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" clip-rule="evenodd" d="M11.37.83L12 3.28l.63-2.45h-1.26zM15.42 1.84l-1.18-.39-.34 2.5 1.52-2.1zM9.76 1.45l-1.19.4 1.53 2.1-.34-2.5zM20.25 11.84l-2.5-4.4a1.42 1.42 0 0 0-.93-.64.96.96 0 0 0-.75.18c-.25.19-.4.42-.45.7l.05.05 2.35 4.13c1.62 2.95 1.1 5.78-1.52 8.4l-.46.41c1-.13 1.93-.6 2.78-1.45 2.7-2.7 2.51-5.59 1.43-7.38zM12.07 9.01c-.13-.69.08-1.3.57-1.77l-2.06-2.07a1.12 1.12 0 0 0-1.56 0c-.15.15-.22.34-.27.53L12.07 9z"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.74 8.3a1.13 1.13 0 0 0-.73-.5.67.67 0 0 0-.53.13c-.15.12-.59.46-.2 1.3l1.18 2.5a.45.45 0 0 1-.23.76.44.44 0 0 1-.48-.25L7.6 6.11a.82.82 0 1 0-1.15 1.15l3.64 3.64a.45.45 0 1 1-.63.63L5.83 7.9 4.8 6.86a.82.82 0 0 0-1.33.9c.04.1.1.18.18.26l1.02 1.03 3.65 3.64a.44.44 0 0 1-.15.73.44.44 0 0 1-.48-.1L4.05 9.68a.82.82 0 0 0-1.4.57.81.81 0 0 0 .24.58l1.53 1.54 2.3 2.28a.45.45 0 0 1-.64.63L3.8 13a.81.81 0 0 0-1.39.57c0 .22.09.43.24.58l4.4 4.4c2.8 2.8 5.5 4.12 8.68.94 2.27-2.28 2.71-4.6 1.34-7.1l-2.32-4.08z"></path></svg></div></div></div><div class="pw-multi-vote-count l ko kp kq kr ks kt ku"><p class="be b ew z fd"><span class="kf">--</span></p></div></div><div class="yt l"><div><div class="bl" aria-hidden="false"><a class="af kx ah ke aj ak al kw an ao ap aq ar as at kv ab q ky kz" aria-label="responses" rel="noopener follow" href="/mastering-customer-segmentation-with-llm-3d9008235f41?responsesOpen=true&amp;sortBy=REVERSE_CHRON&amp;source=author_recirc-----4c6da3259463----0---------------------3a083127_c893_4424_b19c_c1eccf1006a9-------"><svg width="24" height="24" viewBox="0 0 24 24" class=""><path d="M18 16.8a7.14 7.14 0 0 0 2.24-5.32c0-4.12-3.53-7.48-8.05-7.48C7.67 4 4 7.36 4 11.48c0 4.13 3.67 7.48 8.2 7.48a8.9 8.9 0 0 0 2.38-.32c.23.2.48.39.75.56 1.06.69 2.2 1.04 3.4 1.04.22 0 .4-.11.48-.29a.5.5 0 0 0-.04-.52 6.4 6.4 0 0 1-1.16-2.65v.02zm-3.12 1.06l-.06-.22-.32.1a8 8 0 0 1-2.3.33c-4.03 0-7.3-2.96-7.3-6.59S8.17 4.9 12.2 4.9c4 0 7.1 2.96 7.1 6.6 0 1.8-.6 3.47-2.02 4.72l-.2.16v.26l.02.3a6.74 6.74 0 0 0 .88 2.4 5.27 5.27 0 0 1-2.17-.86c-.28-.17-.72-.38-.94-.59l.01-.02z"></path></svg><p class="be b ew z fd"><span class="pw-responses-count yu la">24</span></p></a></div></div></div></div><div class="ab q yv yw"></div></div></div><div class="j i d"><div class="tv bg tw qq"></div></div></div></div></div></div></div></article></div></div><div class="vb vc vd lw ve vf vg ly vh vi vj vk vl vm vn vo vp vq vr vs vt"><div class="vu vv vw vx vy vz l"><article class="vz"><div class="vz ru l"><div class="bg vz"><div class="vz l"><div class="vz wa wb wc wd we wf wg wh wi wj wk wl wm"><div class="wn"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" aria-label="Don’t Start Your Data Science Journey Without These 5 Must-Do Steps From a Spotify Data Scientist" rel="noopener follow" href="/dont-start-your-data-science-journey-without-these-5-must-do-steps-from-a-spotify-data-scientist-c9cec11fd1b?source=author_recirc-----4c6da3259463----1---------------------3a083127_c893_4424_b19c_c1eccf1006a9-------"><div class="wp wq wr ws wt"><img alt="Don’t Start Your Data Science Journey Without These 5 Must-Do Steps From a Spotify Data Scientist" class="bg wu wv ww yx bw" src="https://miro.medium.com/v2/resize:fit:1358/1*xx1TdhbY_vupGMqhN0Z15A.png" loading="lazy"/></div></a></div><div class="wo ab ca cn"><div class="wy wz xa xb xc ab"><div class="rg l"><div><div class="bl" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@elalamik?source=author_recirc-----4c6da3259463----1---------------------3a083127_c893_4424_b19c_c1eccf1006a9-------" rel="noopener follow"><div class="l hi"><img alt="Khouloud El Alami" class="l ec bx xe xf cw" src="https://miro.medium.com/v2/resize:fill:40:40/1*f7K34KvvGl7J3iY5ts1dHQ@2x.jpeg" width="20" height="20" loading="lazy"/><div class="xd bx l xe xf eh n ax hh"></div></div></a></div></div></div><div class="xg l"><div><div class="bl" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hs ab q" href="https://medium.com/@elalamik?source=author_recirc-----4c6da3259463----1---------------------3a083127_c893_4424_b19c_c1eccf1006a9-------" rel="noopener follow"><p class="be b ew z ii ij ik il im in io ip bj">Khouloud El Alami</p></a></div></div></div><div class="xg l"><p class="be b ew z fd">in</p></div><div class="l"><div><div class="bl" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hs ab q" href="https://towardsdatascience.com/?source=author_recirc-----4c6da3259463----1---------------------3a083127_c893_4424_b19c_c1eccf1006a9-------" rel="noopener follow"><p class="be b ew z ii ij ik il im in io ip bj">Towards Data Science</p></a></div></div></div></div><div class="xh xi xj xk xl xm xn xo xp xq l eo"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="/dont-start-your-data-science-journey-without-these-5-must-do-steps-from-a-spotify-data-scientist-c9cec11fd1b?source=author_recirc-----4c6da3259463----1---------------------3a083127_c893_4424_b19c_c1eccf1006a9-------"><div title="Don’t Start Your Data Science Journey Without These 5 Must-Do Steps From a Spotify Data Scientist"><h2 class="be fi ny gd xr xs oa ob gg xt xu od ni pu xv xw pv nm px xx xy py nq qa xz ya qb ii ik il in ip bj">Don’t Start Your Data Science Journey Without These 5 Must-Do Steps From a Spotify Data Scientist</h2></div><div class="yb l"><h3 class="be b hq z ii yc ik il yd in ip fd">A complete guide to everything I wish I’d done before starting my Data Science journey, here’s to acing your first year with data</h3></div></a></div><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="/dont-start-your-data-science-journey-without-these-5-must-do-steps-from-a-spotify-data-scientist-c9cec11fd1b?source=author_recirc-----4c6da3259463----1---------------------3a083127_c893_4424_b19c_c1eccf1006a9-------"><span class="be b ew z fd"><div class="ab q"><div class="ru ab"><div class="bl" aria-hidden="false"><button class="l ax ao am" aria-label="Member-only story"><div class=""><div><div class="bl" aria-hidden="false"><svg width="16" height="16" viewBox="0 0 20 20" fill="none"><path d="M12.4 12.77l-1.81 4.99a.63.63 0 0 1-1.18 0l-1.8-4.99a.63.63 0 0 0-.38-.37l-4.99-1.81a.62.62 0 0 1 0-1.18l4.99-1.8a.63.63 0 0 0 .37-.38l1.81-4.99a.63.63 0 0 1 1.18 0l1.8 4.99a.63.63 0 0 0 .38.37l4.99 1.81a.63.63 0 0 1 0 1.18l-4.99 1.8a.63.63 0 0 0-.37.38z" fill="#FFC017"></path></svg></div></div></div></button></div></div><span class="ht l" aria-hidden="true"><span class="be b bf z fd">·</span></span><span>18 min read</span><span class="ht l" aria-hidden="true"><span class="be b bf z fd">·</span></span><span>Sep 24</span></div></span></a><div class="ye yf yg yh yi l"><div class="ab co"><div class="am yj yk yl ym yn yo yp yq yr ys ab q"><div class="ab q ka"><div class="pw-multi-vote-icon hi ih kb kc kd"><div class=""><div class="ke kf kg kh ki kj kk am kl km kn kd"><svg width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" clip-rule="evenodd" d="M11.37.83L12 3.28l.63-2.45h-1.26zM15.42 1.84l-1.18-.39-.34 2.5 1.52-2.1zM9.76 1.45l-1.19.4 1.53 2.1-.34-2.5zM20.25 11.84l-2.5-4.4a1.42 1.42 0 0 0-.93-.64.96.96 0 0 0-.75.18c-.25.19-.4.42-.45.7l.05.05 2.35 4.13c1.62 2.95 1.1 5.78-1.52 8.4l-.46.41c1-.13 1.93-.6 2.78-1.45 2.7-2.7 2.51-5.59 1.43-7.38zM12.07 9.01c-.13-.69.08-1.3.57-1.77l-2.06-2.07a1.12 1.12 0 0 0-1.56 0c-.15.15-.22.34-.27.53L12.07 9z"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.74 8.3a1.13 1.13 0 0 0-.73-.5.67.67 0 0 0-.53.13c-.15.12-.59.46-.2 1.3l1.18 2.5a.45.45 0 0 1-.23.76.44.44 0 0 1-.48-.25L7.6 6.11a.82.82 0 1 0-1.15 1.15l3.64 3.64a.45.45 0 1 1-.63.63L5.83 7.9 4.8 6.86a.82.82 0 0 0-1.33.9c.04.1.1.18.18.26l1.02 1.03 3.65 3.64a.44.44 0 0 1-.15.73.44.44 0 0 1-.48-.1L4.05 9.68a.82.82 0 0 0-1.4.57.81.81 0 0 0 .24.58l1.53 1.54 2.3 2.28a.45.45 0 0 1-.64.63L3.8 13a.81.81 0 0 0-1.39.57c0 .22.09.43.24.58l4.4 4.4c2.8 2.8 5.5 4.12 8.68.94 2.27-2.28 2.71-4.6 1.34-7.1l-2.32-4.08z"></path></svg></div></div></div><div class="pw-multi-vote-count l ko kp kq kr ks kt ku"><p class="be b ew z fd"><span class="kf">--</span></p></div></div><div class="yt l"><div><div class="bl" aria-hidden="false"><a class="af kx ah ke aj ak al kw an ao ap aq ar as at kv ab q ky kz" aria-label="responses" rel="noopener follow" href="/dont-start-your-data-science-journey-without-these-5-must-do-steps-from-a-spotify-data-scientist-c9cec11fd1b?responsesOpen=true&amp;sortBy=REVERSE_CHRON&amp;source=author_recirc-----4c6da3259463----1---------------------3a083127_c893_4424_b19c_c1eccf1006a9-------"><svg width="24" height="24" viewBox="0 0 24 24" class=""><path d="M18 16.8a7.14 7.14 0 0 0 2.24-5.32c0-4.12-3.53-7.48-8.05-7.48C7.67 4 4 7.36 4 11.48c0 4.13 3.67 7.48 8.2 7.48a8.9 8.9 0 0 0 2.38-.32c.23.2.48.39.75.56 1.06.69 2.2 1.04 3.4 1.04.22 0 .4-.11.48-.29a.5.5 0 0 0-.04-.52 6.4 6.4 0 0 1-1.16-2.65v.02zm-3.12 1.06l-.06-.22-.32.1a8 8 0 0 1-2.3.33c-4.03 0-7.3-2.96-7.3-6.59S8.17 4.9 12.2 4.9c4 0 7.1 2.96 7.1 6.6 0 1.8-.6 3.47-2.02 4.72l-.2.16v.26l.02.3a6.74 6.74 0 0 0 .88 2.4 5.27 5.27 0 0 1-2.17-.86c-.28-.17-.72-.38-.94-.59l.01-.02z"></path></svg><p class="be b ew z fd"><span class="pw-responses-count yu la">20</span></p></a></div></div></div></div><div class="ab q yv yw"></div></div></div><div class="j i d"><div class="tv bg tw qq"></div></div></div></div></div></div></div></article></div></div><div class="vb vc vd lw ve vf vg ly vh vi vj vk vl vm vn vo vp vq vr vs vt"><div class="vu vv vw vx vy vz l"><article class="vz"><div class="vz ru l"><div class="bg vz"><div class="vz l"><div class="vz wa wb wc wd we wf wg wh wi wj wk wl wm"><div class="wn"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" aria-label="Cat holding a atom where the nucleus are documents." rel="noopener follow" href="/forget-rag-the-future-is-rag-fusion-1147298d8ad1?source=author_recirc-----4c6da3259463----2---------------------3a083127_c893_4424_b19c_c1eccf1006a9-------"><div class="wp wq wr ws wt"><img alt="Cat holding a atom where the nucleus are documents." class="bg wu wv ww wx bw" src="https://miro.medium.com/v2/resize:fit:1358/1*Fks46Zel9FUan3oxhgGz_Q.jpeg" loading="lazy"/></div></a></div><div class="wo ab ca cn"><div class="wy wz xa xb xc ab"><div class="rg l"><div><div class="bl" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://araudaschl.medium.com/?source=author_recirc-----4c6da3259463----2---------------------3a083127_c893_4424_b19c_c1eccf1006a9-------" rel="noopener follow"><div class="l hi"><img alt="Adrian H. Raudaschl" class="l ec bx xe xf cw" src="https://miro.medium.com/v2/resize:fill:40:40/2*a4-0O9zzHroih3uHMDmVEA.jpeg" width="20" height="20" loading="lazy"/><div class="xd bx l xe xf eh n ax hh"></div></div></a></div></div></div><div class="xg l"><div><div class="bl" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hs ab q" href="https://araudaschl.medium.com/?source=author_recirc-----4c6da3259463----2---------------------3a083127_c893_4424_b19c_c1eccf1006a9-------" rel="noopener follow"><p class="be b ew z ii ij ik il im in io ip bj">Adrian H. Raudaschl</p></a></div></div></div><div class="xg l"><p class="be b ew z fd">in</p></div><div class="l"><div><div class="bl" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hs ab q" href="https://towardsdatascience.com/?source=author_recirc-----4c6da3259463----2---------------------3a083127_c893_4424_b19c_c1eccf1006a9-------" rel="noopener follow"><p class="be b ew z ii ij ik il im in io ip bj">Towards Data Science</p></a></div></div></div></div><div class="xh xi xj xk xl xm xn xo xp xq l eo"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="/forget-rag-the-future-is-rag-fusion-1147298d8ad1?source=author_recirc-----4c6da3259463----2---------------------3a083127_c893_4424_b19c_c1eccf1006a9-------"><div title=""><h2 class="be fi ny gd xr xs oa ob gg xt xu od ni pu xv xw pv nm px xx xy py nq qa xz ya qb ii ik il in ip bj">Forget RAG, the Future is RAG-Fusion</h2></div><div class="yb l"><h3 class="be b hq z ii yc ik il yd in ip fd">The Next Frontier of Search: Retrieval Augmented Generation meets Reciprocal Rank Fusion and Generated Queries</h3></div></a></div><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="/forget-rag-the-future-is-rag-fusion-1147298d8ad1?source=author_recirc-----4c6da3259463----2---------------------3a083127_c893_4424_b19c_c1eccf1006a9-------"><span class="be b ew z fd"><div class="ab q"><div class="ru ab"><div class="bl" aria-hidden="false"><button class="l ax ao am" aria-label="Member-only story"><div class=""><div><div class="bl" aria-hidden="false"><svg width="16" height="16" viewBox="0 0 20 20" fill="none"><path d="M12.4 12.77l-1.81 4.99a.63.63 0 0 1-1.18 0l-1.8-4.99a.63.63 0 0 0-.38-.37l-4.99-1.81a.62.62 0 0 1 0-1.18l4.99-1.8a.63.63 0 0 0 .37-.38l1.81-4.99a.63.63 0 0 1 1.18 0l1.8 4.99a.63.63 0 0 0 .38.37l4.99 1.81a.63.63 0 0 1 0 1.18l-4.99 1.8a.63.63 0 0 0-.37.38z" fill="#FFC017"></path></svg></div></div></div></button></div></div><span class="ht l" aria-hidden="true"><span class="be b bf z fd">·</span></span><span>10 min read</span><span class="ht l" aria-hidden="true"><span class="be b bf z fd">·</span></span><span>Oct 6</span></div></span></a><div class="ye yf yg yh yi l"><div class="ab co"><div class="am yj yk yl ym yn yo yp yq yr ys ab q"><div class="ab q ka"><div class="pw-multi-vote-icon hi ih kb kc kd"><div class=""><div class="ke kf kg kh ki kj kk am kl km kn kd"><svg width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" clip-rule="evenodd" d="M11.37.83L12 3.28l.63-2.45h-1.26zM15.42 1.84l-1.18-.39-.34 2.5 1.52-2.1zM9.76 1.45l-1.19.4 1.53 2.1-.34-2.5zM20.25 11.84l-2.5-4.4a1.42 1.42 0 0 0-.93-.64.96.96 0 0 0-.75.18c-.25.19-.4.42-.45.7l.05.05 2.35 4.13c1.62 2.95 1.1 5.78-1.52 8.4l-.46.41c1-.13 1.93-.6 2.78-1.45 2.7-2.7 2.51-5.59 1.43-7.38zM12.07 9.01c-.13-.69.08-1.3.57-1.77l-2.06-2.07a1.12 1.12 0 0 0-1.56 0c-.15.15-.22.34-.27.53L12.07 9z"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.74 8.3a1.13 1.13 0 0 0-.73-.5.67.67 0 0 0-.53.13c-.15.12-.59.46-.2 1.3l1.18 2.5a.45.45 0 0 1-.23.76.44.44 0 0 1-.48-.25L7.6 6.11a.82.82 0 1 0-1.15 1.15l3.64 3.64a.45.45 0 1 1-.63.63L5.83 7.9 4.8 6.86a.82.82 0 0 0-1.33.9c.04.1.1.18.18.26l1.02 1.03 3.65 3.64a.44.44 0 0 1-.15.73.44.44 0 0 1-.48-.1L4.05 9.68a.82.82 0 0 0-1.4.57.81.81 0 0 0 .24.58l1.53 1.54 2.3 2.28a.45.45 0 0 1-.64.63L3.8 13a.81.81 0 0 0-1.39.57c0 .22.09.43.24.58l4.4 4.4c2.8 2.8 5.5 4.12 8.68.94 2.27-2.28 2.71-4.6 1.34-7.1l-2.32-4.08z"></path></svg></div></div></div><div class="pw-multi-vote-count l ko kp kq kr ks kt ku"><p class="be b ew z fd"><span class="kf">--</span></p></div></div><div class="yt l"><div><div class="bl" aria-hidden="false"><a class="af kx ah ke aj ak al kw an ao ap aq ar as at kv ab q ky kz" aria-label="responses" rel="noopener follow" href="/forget-rag-the-future-is-rag-fusion-1147298d8ad1?responsesOpen=true&amp;sortBy=REVERSE_CHRON&amp;source=author_recirc-----4c6da3259463----2---------------------3a083127_c893_4424_b19c_c1eccf1006a9-------"><svg width="24" height="24" viewBox="0 0 24 24" class=""><path d="M18 16.8a7.14 7.14 0 0 0 2.24-5.32c0-4.12-3.53-7.48-8.05-7.48C7.67 4 4 7.36 4 11.48c0 4.13 3.67 7.48 8.2 7.48a8.9 8.9 0 0 0 2.38-.32c.23.2.48.39.75.56 1.06.69 2.2 1.04 3.4 1.04.22 0 .4-.11.48-.29a.5.5 0 0 0-.04-.52 6.4 6.4 0 0 1-1.16-2.65v.02zm-3.12 1.06l-.06-.22-.32.1a8 8 0 0 1-2.3.33c-4.03 0-7.3-2.96-7.3-6.59S8.17 4.9 12.2 4.9c4 0 7.1 2.96 7.1 6.6 0 1.8-.6 3.47-2.02 4.72l-.2.16v.26l.02.3a6.74 6.74 0 0 0 .88 2.4 5.27 5.27 0 0 1-2.17-.86c-.28-.17-.72-.38-.94-.59l.01-.02z"></path></svg><p class="be b ew z fd"><span class="pw-responses-count yu la">19</span></p></a></div></div></div></div><div class="ab q yv yw"></div></div></div><div class="j i d"><div class="tv bg tw qq"></div></div></div></div></div></div></div></article></div></div><div class="vb vc vd lw ve vf vg ly vh vi vj vk vl vm vn vo vp vq vr vs vt"><div class="vu vv vw vx vy vz l"><article class="vz"><div class="vz ru l"><div class="bg vz"><div class="vz l"><div class="vz wa wb wc wd we wf wg wh wi wj wk wl wm"><div class="wn"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" aria-label="Coding was Hard Until I Learned These 2 Things!" rel="noopener follow" href="/coding-was-hard-until-i-learned-these-2-things-1219840d0a0a?source=author_recirc-----4c6da3259463----3---------------------3a083127_c893_4424_b19c_c1eccf1006a9-------"><div class="wp wq wr ws wt"><img alt="Coding was Hard Until I Learned These 2 Things!" class="bg wu wv ww wx bw" src="https://miro.medium.com/v2/resize:fit:1358/1*KnV1cBSw-kWyh7Y6XEEzrA.jpeg" loading="lazy"/></div></a></div><div class="wo ab ca cn"><div class="wy wz xa xb xc ab"><div class="rg l"><div><div class="bl" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://natassha6789.medium.com/?source=author_recirc-----4c6da3259463----3---------------------3a083127_c893_4424_b19c_c1eccf1006a9-------" rel="noopener follow"><div class="l hi"><img alt="Natassha Selvaraj" class="l ec bx xe xf cw" src="https://miro.medium.com/v2/resize:fill:40:40/1*WHVVSRCz66KUX57Bo8oAWQ.jpeg" width="20" height="20" loading="lazy"/><div class="xd bx l xe xf eh n ax hh"></div></div></a></div></div></div><div class="xg l"><div><div class="bl" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hs ab q" href="https://natassha6789.medium.com/?source=author_recirc-----4c6da3259463----3---------------------3a083127_c893_4424_b19c_c1eccf1006a9-------" rel="noopener follow"><p class="be b ew z ii ij ik il im in io ip bj">Natassha Selvaraj</p></a></div></div></div><div class="xg l"><p class="be b ew z fd">in</p></div><div class="l"><div><div class="bl" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hs ab q" href="https://towardsdatascience.com/?source=author_recirc-----4c6da3259463----3---------------------3a083127_c893_4424_b19c_c1eccf1006a9-------" rel="noopener follow"><p class="be b ew z ii ij ik il im in io ip bj">Towards Data Science</p></a></div></div></div></div><div class="xh xi xj xk xl xm xn xo xp xq l eo"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="/coding-was-hard-until-i-learned-these-2-things-1219840d0a0a?source=author_recirc-----4c6da3259463----3---------------------3a083127_c893_4424_b19c_c1eccf1006a9-------"><div title=""><h2 class="be fi ny gd xr xs oa ob gg xt xu od ni pu xv xw pv nm px xx xy py nq qa xz ya qb ii ik il in ip bj">Coding was Hard Until I Learned These 2 Things!</h2></div><div class="yb l"><h3 class="be b hq z ii yc ik il yd in ip fd">Here’s what helped me go from “aspiring programmer” to actually landing a job in the field.</h3></div></a></div><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="/coding-was-hard-until-i-learned-these-2-things-1219840d0a0a?source=author_recirc-----4c6da3259463----3---------------------3a083127_c893_4424_b19c_c1eccf1006a9-------"><span class="be b ew z fd"><div class="ab q"><div class="ru ab"><div class="bl" aria-hidden="false"><button class="l ax ao am" aria-label="Member-only story"><div class=""><div><div class="bl" aria-hidden="false"><svg width="16" height="16" viewBox="0 0 20 20" fill="none"><path d="M12.4 12.77l-1.81 4.99a.63.63 0 0 1-1.18 0l-1.8-4.99a.63.63 0 0 0-.38-.37l-4.99-1.81a.62.62 0 0 1 0-1.18l4.99-1.8a.63.63 0 0 0 .37-.38l1.81-4.99a.63.63 0 0 1 1.18 0l1.8 4.99a.63.63 0 0 0 .38.37l4.99 1.81a.63.63 0 0 1 0 1.18l-4.99 1.8a.63.63 0 0 0-.37.38z" fill="#FFC017"></path></svg></div></div></div></button></div></div><span class="ht l" aria-hidden="true"><span class="be b bf z fd">·</span></span><span>7 min read</span><span class="ht l" aria-hidden="true"><span class="be b bf z fd">·</span></span><span>Oct 2</span></div></span></a><div class="ye yf yg yh yi l"><div class="ab co"><div class="am yj yk yl ym yn yo yp yq yr ys ab q"><div class="ab q ka"><div class="pw-multi-vote-icon hi ih kb kc kd"><div class=""><div class="ke kf kg kh ki kj kk am kl km kn kd"><svg width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" clip-rule="evenodd" d="M11.37.83L12 3.28l.63-2.45h-1.26zM15.42 1.84l-1.18-.39-.34 2.5 1.52-2.1zM9.76 1.45l-1.19.4 1.53 2.1-.34-2.5zM20.25 11.84l-2.5-4.4a1.42 1.42 0 0 0-.93-.64.96.96 0 0 0-.75.18c-.25.19-.4.42-.45.7l.05.05 2.35 4.13c1.62 2.95 1.1 5.78-1.52 8.4l-.46.41c1-.13 1.93-.6 2.78-1.45 2.7-2.7 2.51-5.59 1.43-7.38zM12.07 9.01c-.13-.69.08-1.3.57-1.77l-2.06-2.07a1.12 1.12 0 0 0-1.56 0c-.15.15-.22.34-.27.53L12.07 9z"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.74 8.3a1.13 1.13 0 0 0-.73-.5.67.67 0 0 0-.53.13c-.15.12-.59.46-.2 1.3l1.18 2.5a.45.45 0 0 1-.23.76.44.44 0 0 1-.48-.25L7.6 6.11a.82.82 0 1 0-1.15 1.15l3.64 3.64a.45.45 0 1 1-.63.63L5.83 7.9 4.8 6.86a.82.82 0 0 0-1.33.9c.04.1.1.18.18.26l1.02 1.03 3.65 3.64a.44.44 0 0 1-.15.73.44.44 0 0 1-.48-.1L4.05 9.68a.82.82 0 0 0-1.4.57.81.81 0 0 0 .24.58l1.53 1.54 2.3 2.28a.45.45 0 0 1-.64.63L3.8 13a.81.81 0 0 0-1.39.57c0 .22.09.43.24.58l4.4 4.4c2.8 2.8 5.5 4.12 8.68.94 2.27-2.28 2.71-4.6 1.34-7.1l-2.32-4.08z"></path></svg></div></div></div><div class="pw-multi-vote-count l ko kp kq kr ks kt ku"><p class="be b ew z fd"><span class="kf">--</span></p></div></div><div class="yt l"><div><div class="bl" aria-hidden="false"><a class="af kx ah ke aj ak al kw an ao ap aq ar as at kv ab q ky kz" aria-label="responses" rel="noopener follow" href="/coding-was-hard-until-i-learned-these-2-things-1219840d0a0a?responsesOpen=true&amp;sortBy=REVERSE_CHRON&amp;source=author_recirc-----4c6da3259463----3---------------------3a083127_c893_4424_b19c_c1eccf1006a9-------"><svg width="24" height="24" viewBox="0 0 24 24" class=""><path d="M18 16.8a7.14 7.14 0 0 0 2.24-5.32c0-4.12-3.53-7.48-8.05-7.48C7.67 4 4 7.36 4 11.48c0 4.13 3.67 7.48 8.2 7.48a8.9 8.9 0 0 0 2.38-.32c.23.2.48.39.75.56 1.06.69 2.2 1.04 3.4 1.04.22 0 .4-.11.48-.29a.5.5 0 0 0-.04-.52 6.4 6.4 0 0 1-1.16-2.65v.02zm-3.12 1.06l-.06-.22-.32.1a8 8 0 0 1-2.3.33c-4.03 0-7.3-2.96-7.3-6.59S8.17 4.9 12.2 4.9c4 0 7.1 2.96 7.1 6.6 0 1.8-.6 3.47-2.02 4.72l-.2.16v.26l.02.3a6.74 6.74 0 0 0 .88 2.4 5.27 5.27 0 0 1-2.17-.86c-.28-.17-.72-.38-.94-.59l.01-.02z"></path></svg><p class="be b ew z fd"><span class="pw-responses-count yu la">22</span></p></a></div></div></div></div><div class="ab q yv yw"></div></div></div></div></div></div></div></div></article></div></div></div><div class="tv bg tw dj dk yy yz za"></div><div class="ab ib ic zb zc zd"><a class="be b bf z bj rh ze zf zg zh ky zi sx sy hx zj zk zl tb zm zn zo zp zq td te ec bl ff mu" href="https://medium.com/@c.trotter2?source=post_page-----4c6da3259463--------------------------------" rel="noopener follow"><div class="l mu">See all from <!-- -->Cameron Trotter</div></a><div class="zr zs zt zu zv zw zx zy zz ku l"><a class="be b bf z bj rh ze zf zg zh ky zi sx sy hx zj zk zl tb zm zn zo zp zq td te ec bl ff mu" href="https://towardsdatascience.com/?source=post_page-----4c6da3259463--------------------------------" rel="noopener follow"><div class="l mu">See all from <!-- -->Towards Data Science</div></a></div></div></div></div><div class="tv bg tw aba abb abc abd abe"></div><div class="ab ca"><div class="ch bg dx dy dz ea"><div class="abf abg l"><h2 class="be tq ny gd oa ob gg od oe og oh oi ok ol om oo op bj">Recommended from Medium</h2><div class="mg mh mi mj mk l"><div class="ul ab ka id um un uo up uq ur us ut uu uv uw ux uy uz va"><div class="vb vc vd lw ve vf vg ly vh vi vj vk vl vm vn vo vp vq vr vs vt"><div class="vu vv vw vx vy vz l"><article class="vz"><div class="vz ru l"><div class="bg vz"><div class="vz l"><div class="vz wa wb wc wd we wf wg wh wi wj wk wl wm"><div class="wn"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" aria-label="Siamese Networks Introduction and Implementation" href="https://medium.com/@prabhattgs12345789/siamese-neural-network-enhancing-ai-capabilities-with-pairwise-comparisons-4f00e2dd8256?source=read_next_recirc-----4c6da3259463----0---------------------131088ad_b7ec_4260_ba56_4e3eb3e6766d-------" rel="noopener follow"><div class="wp wq wr ws wt"><img alt="Siamese Networks Introduction and Implementation" class="bg wu wv ww wx bw" src="https://miro.medium.com/v2/resize:fit:1358/1*B6QOjW7zTpEg5NwhBGeJFg.png" loading="lazy"/></div></a></div><div class="wo ab ca cn"><div class="wy wz xa xb xc ab"><div class="rg l"><div><div class="bl" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@prabhattgs12345789?source=read_next_recirc-----4c6da3259463----0---------------------131088ad_b7ec_4260_ba56_4e3eb3e6766d-------" rel="noopener follow"><div class="l hi"><img alt="Prabhat Kumar" class="l ec bx xe xf cw" src="https://miro.medium.com/v2/resize:fill:40:40/0*5QFAm42C4ompIATz" width="20" height="20" loading="lazy"/><div class="xd bx l xe xf eh n ax hh"></div></div></a></div></div></div><div class="xg l"><div><div class="bl" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hs ab q" href="https://medium.com/@prabhattgs12345789?source=read_next_recirc-----4c6da3259463----0---------------------131088ad_b7ec_4260_ba56_4e3eb3e6766d-------" rel="noopener follow"><p class="be b ew z ii ij ik il im in io ip bj">Prabhat Kumar</p></a></div></div></div></div><div class="xh xi xj xk xl xm xn xo xp xq l eo"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@prabhattgs12345789/siamese-neural-network-enhancing-ai-capabilities-with-pairwise-comparisons-4f00e2dd8256?source=read_next_recirc-----4c6da3259463----0---------------------131088ad_b7ec_4260_ba56_4e3eb3e6766d-------" rel="noopener follow"><div title=""><h2 class="be fi ny gd xr xs oa ob gg xt xu od ni pu xv xw pv nm px xx xy py nq qa xz ya qb ii ik il in ip bj">Siamese Networks Introduction and Implementation</h2></div><div class="yb l"><h3 class="be b hq z ii yc ik il yd in ip fd">Artificial Intelligence (AI) has revolutionized numerous industries, and its applications continue to expand. One of the key areas where AI…</h3></div></a></div><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@prabhattgs12345789/siamese-neural-network-enhancing-ai-capabilities-with-pairwise-comparisons-4f00e2dd8256?source=read_next_recirc-----4c6da3259463----0---------------------131088ad_b7ec_4260_ba56_4e3eb3e6766d-------" rel="noopener follow"><span class="be b ew z fd"><div class="ab q"><span>5 min read</span><span class="ht l" aria-hidden="true"><span class="be b bf z fd">·</span></span><span>Jun 25</span></div></span></a><div class="ye yf yg yh yi l"><div class="ab co"><div class="am yj yk yl ym yn yo yp yq yr ys ab q"><div class="ab q ka"><div class="pw-multi-vote-icon hi ih kb kc kd"><div class=""><div class="ke kf kg kh ki kj kk am kl km kn kd"><svg width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" clip-rule="evenodd" d="M11.37.83L12 3.28l.63-2.45h-1.26zM15.42 1.84l-1.18-.39-.34 2.5 1.52-2.1zM9.76 1.45l-1.19.4 1.53 2.1-.34-2.5zM20.25 11.84l-2.5-4.4a1.42 1.42 0 0 0-.93-.64.96.96 0 0 0-.75.18c-.25.19-.4.42-.45.7l.05.05 2.35 4.13c1.62 2.95 1.1 5.78-1.52 8.4l-.46.41c1-.13 1.93-.6 2.78-1.45 2.7-2.7 2.51-5.59 1.43-7.38zM12.07 9.01c-.13-.69.08-1.3.57-1.77l-2.06-2.07a1.12 1.12 0 0 0-1.56 0c-.15.15-.22.34-.27.53L12.07 9z"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.74 8.3a1.13 1.13 0 0 0-.73-.5.67.67 0 0 0-.53.13c-.15.12-.59.46-.2 1.3l1.18 2.5a.45.45 0 0 1-.23.76.44.44 0 0 1-.48-.25L7.6 6.11a.82.82 0 1 0-1.15 1.15l3.64 3.64a.45.45 0 1 1-.63.63L5.83 7.9 4.8 6.86a.82.82 0 0 0-1.33.9c.04.1.1.18.18.26l1.02 1.03 3.65 3.64a.44.44 0 0 1-.15.73.44.44 0 0 1-.48-.1L4.05 9.68a.82.82 0 0 0-1.4.57.81.81 0 0 0 .24.58l1.53 1.54 2.3 2.28a.45.45 0 0 1-.64.63L3.8 13a.81.81 0 0 0-1.39.57c0 .22.09.43.24.58l4.4 4.4c2.8 2.8 5.5 4.12 8.68.94 2.27-2.28 2.71-4.6 1.34-7.1l-2.32-4.08z"></path></svg></div></div></div><div class="pw-multi-vote-count l ko kp kq kr ks kt ku"><p class="be b ew z fd"><span class="kf">--</span></p></div></div><div class="yt l"><div><div class="bl" aria-hidden="false"><a class="af kx ah ke aj ak al kw an ao ap aq ar as at kv ab q ky kz" aria-label="responses" href="https://medium.com/@prabhattgs12345789/siamese-neural-network-enhancing-ai-capabilities-with-pairwise-comparisons-4f00e2dd8256?responsesOpen=true&amp;sortBy=REVERSE_CHRON&amp;source=read_next_recirc-----4c6da3259463----0---------------------131088ad_b7ec_4260_ba56_4e3eb3e6766d-------" rel="noopener follow"><svg width="24" height="24" viewBox="0 0 24 24" class=""><path d="M18 16.8a7.14 7.14 0 0 0 2.24-5.32c0-4.12-3.53-7.48-8.05-7.48C7.67 4 4 7.36 4 11.48c0 4.13 3.67 7.48 8.2 7.48a8.9 8.9 0 0 0 2.38-.32c.23.2.48.39.75.56 1.06.69 2.2 1.04 3.4 1.04.22 0 .4-.11.48-.29a.5.5 0 0 0-.04-.52 6.4 6.4 0 0 1-1.16-2.65v.02zm-3.12 1.06l-.06-.22-.32.1a8 8 0 0 1-2.3.33c-4.03 0-7.3-2.96-7.3-6.59S8.17 4.9 12.2 4.9c4 0 7.1 2.96 7.1 6.6 0 1.8-.6 3.47-2.02 4.72l-.2.16v.26l.02.3a6.74 6.74 0 0 0 .88 2.4 5.27 5.27 0 0 1-2.17-.86c-.28-.17-.72-.38-.94-.59l.01-.02z"></path></svg><p class="be b ew z fd"><span class="pw-responses-count yu la">1</span></p></a></div></div></div></div><div class="ab q yv yw"></div></div></div><div class="j i d"><div class="tv bg tw qq"></div></div></div></div></div></div></div></article></div></div><div class="vb vc vd lw ve vf vg ly vh vi vj vk vl vm vn vo vp vq vr vs vt"><div class="vu vv vw vx vy vz l"><article class="vz"><div class="vz ru l"><div class="bg vz"><div class="vz l"><div class="vz wa wb wc wd we wf wg wh wi wj wk wl wm"><div class="wn"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" aria-label="Power of Siamese Networks and Triplet Loss: Tackling Unbalanced Datasets" href="https://medium.com/@mandalsouvik/power-of-siamese-networks-and-triplet-loss-tackling-unbalanced-datasets-ebb2bb6efdb1?source=read_next_recirc-----4c6da3259463----1---------------------131088ad_b7ec_4260_ba56_4e3eb3e6766d-------" rel="noopener follow"><div class="wp wq wr ws wt"><img alt="Power of Siamese Networks and Triplet Loss: Tackling Unbalanced Datasets" class="bg wu wv ww wx bw" src="https://miro.medium.com/v2/resize:fit:1358/1*4C4f3HYbqjP5N2BNDWqzjw.png" loading="lazy"/></div></a></div><div class="wo ab ca cn"><div class="wy wz xa xb xc ab"><div class="rg l"><div><div class="bl" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@mandalsouvik?source=read_next_recirc-----4c6da3259463----1---------------------131088ad_b7ec_4260_ba56_4e3eb3e6766d-------" rel="noopener follow"><div class="l hi"><img alt="Souvik Mandal" class="l ec bx xe xf cw" src="https://miro.medium.com/v2/resize:fill:40:40/1*O-8qMqfsL76mwwps2Kq2jA.jpeg" width="20" height="20" loading="lazy"/><div class="xd bx l xe xf eh n ax hh"></div></div></a></div></div></div><div class="xg l"><div><div class="bl" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hs ab q" href="https://medium.com/@mandalsouvik?source=read_next_recirc-----4c6da3259463----1---------------------131088ad_b7ec_4260_ba56_4e3eb3e6766d-------" rel="noopener follow"><p class="be b ew z ii ij ik il im in io ip bj">Souvik Mandal</p></a></div></div></div></div><div class="xh xi xj xk xl xm xn xo xp xq l eo"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@mandalsouvik/power-of-siamese-networks-and-triplet-loss-tackling-unbalanced-datasets-ebb2bb6efdb1?source=read_next_recirc-----4c6da3259463----1---------------------131088ad_b7ec_4260_ba56_4e3eb3e6766d-------" rel="noopener follow"><div title="Power of Siamese Networks and Triplet Loss: Tackling Unbalanced Datasets"><h2 class="be fi ny gd xr xs oa ob gg xt xu od ni pu xv xw pv nm px xx xy py nq qa xz ya qb ii ik il in ip bj">Power of Siamese Networks and Triplet Loss: Tackling Unbalanced Datasets</h2></div><div class="yb l"><h3 class="be b hq z ii yc ik il yd in ip fd">Solve unbalanced Datasets and Image Recognition Tasks: Unveiling the Potential of Siamese Networks, Triplet Loss, and Contrastive Loss</h3></div></a></div><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@mandalsouvik/power-of-siamese-networks-and-triplet-loss-tackling-unbalanced-datasets-ebb2bb6efdb1?source=read_next_recirc-----4c6da3259463----1---------------------131088ad_b7ec_4260_ba56_4e3eb3e6766d-------" rel="noopener follow"><span class="be b ew z fd"><div class="ab q"><span>8 min read</span><span class="ht l" aria-hidden="true"><span class="be b bf z fd">·</span></span><span>May 17</span></div></span></a><div class="ye yf yg yh yi l"><div class="ab co"><div class="am yj yk yl ym yn yo yp yq yr ys ab q"><div class="ab q ka"><div class="pw-multi-vote-icon hi ih kb kc kd"><div class=""><div class="ke kf kg kh ki kj kk am kl km kn kd"><svg width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" clip-rule="evenodd" d="M11.37.83L12 3.28l.63-2.45h-1.26zM15.42 1.84l-1.18-.39-.34 2.5 1.52-2.1zM9.76 1.45l-1.19.4 1.53 2.1-.34-2.5zM20.25 11.84l-2.5-4.4a1.42 1.42 0 0 0-.93-.64.96.96 0 0 0-.75.18c-.25.19-.4.42-.45.7l.05.05 2.35 4.13c1.62 2.95 1.1 5.78-1.52 8.4l-.46.41c1-.13 1.93-.6 2.78-1.45 2.7-2.7 2.51-5.59 1.43-7.38zM12.07 9.01c-.13-.69.08-1.3.57-1.77l-2.06-2.07a1.12 1.12 0 0 0-1.56 0c-.15.15-.22.34-.27.53L12.07 9z"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.74 8.3a1.13 1.13 0 0 0-.73-.5.67.67 0 0 0-.53.13c-.15.12-.59.46-.2 1.3l1.18 2.5a.45.45 0 0 1-.23.76.44.44 0 0 1-.48-.25L7.6 6.11a.82.82 0 1 0-1.15 1.15l3.64 3.64a.45.45 0 1 1-.63.63L5.83 7.9 4.8 6.86a.82.82 0 0 0-1.33.9c.04.1.1.18.18.26l1.02 1.03 3.65 3.64a.44.44 0 0 1-.15.73.44.44 0 0 1-.48-.1L4.05 9.68a.82.82 0 0 0-1.4.57.81.81 0 0 0 .24.58l1.53 1.54 2.3 2.28a.45.45 0 0 1-.64.63L3.8 13a.81.81 0 0 0-1.39.57c0 .22.09.43.24.58l4.4 4.4c2.8 2.8 5.5 4.12 8.68.94 2.27-2.28 2.71-4.6 1.34-7.1l-2.32-4.08z"></path></svg></div></div></div><div class="pw-multi-vote-count l ko kp kq kr ks kt ku"><p class="be b ew z fd"><span class="kf">--</span></p></div></div><div class="yt l"><div><div class="bl" aria-hidden="false"><a class="af kx ah ke aj ak al kw an ao ap aq ar as at kv ab q ky kz" aria-label="responses" href="https://medium.com/@mandalsouvik/power-of-siamese-networks-and-triplet-loss-tackling-unbalanced-datasets-ebb2bb6efdb1?responsesOpen=true&amp;sortBy=REVERSE_CHRON&amp;source=read_next_recirc-----4c6da3259463----1---------------------131088ad_b7ec_4260_ba56_4e3eb3e6766d-------" rel="noopener follow"><svg width="24" height="24" viewBox="0 0 24 24" class=""><path d="M18 16.8a7.14 7.14 0 0 0 2.24-5.32c0-4.12-3.53-7.48-8.05-7.48C7.67 4 4 7.36 4 11.48c0 4.13 3.67 7.48 8.2 7.48a8.9 8.9 0 0 0 2.38-.32c.23.2.48.39.75.56 1.06.69 2.2 1.04 3.4 1.04.22 0 .4-.11.48-.29a.5.5 0 0 0-.04-.52 6.4 6.4 0 0 1-1.16-2.65v.02zm-3.12 1.06l-.06-.22-.32.1a8 8 0 0 1-2.3.33c-4.03 0-7.3-2.96-7.3-6.59S8.17 4.9 12.2 4.9c4 0 7.1 2.96 7.1 6.6 0 1.8-.6 3.47-2.02 4.72l-.2.16v.26l.02.3a6.74 6.74 0 0 0 .88 2.4 5.27 5.27 0 0 1-2.17-.86c-.28-.17-.72-.38-.94-.59l.01-.02z"></path></svg></a></div></div></div></div><div class="ab q yv yw"></div></div></div></div></div></div></div></div></article></div></div></div></div><div class="tv bg tw abh"></div><h2 class="be tq hq z fh bj">Lists</h2><div class="qq l"><div class="cm ab ka id um un uo up uq ur us ut uu uv uw ux uy uz va"><div class="vb vc vd lw ve vf vg ly vh vi vj vk vl vm vn vo vp vq vr vs vt"><a class="af ag ah ai aj ak al am an ao ap aq ar as at ab ck cm" href="https://medium.com/@ben.putney/list/predictive-modeling-w-python-e3668ea008e1?source=read_next_recirc-----4c6da3259463--------------------------------" rel="noopener follow"><div class="abo abp ii ab ia hi"><div class="hi wu abj bw abk"><div class="wu hc ii l"><img alt="" class="" src="https://miro.medium.com/v2/resize:fill:96:96/0*r4yjMpEmqzHCUvWC.jpg" width="48" height="48" loading="lazy" role="presentation"/></div></div><div class="hi wu abj bw abl abm"><div class="wu hc ii l"><img alt="" class="" src="https://miro.medium.com/v2/resize:fill:96:96/1*bv2KUVNLi2sFNjBTdoBmWw.png" width="48" height="48" loading="lazy" role="presentation"/></div></div><div class="hi wu bw hm abn"><div class="wu hc ii l"><img alt="" class="" src="https://miro.medium.com/v2/resize:fill:96:96/0*zsngbTOmFCy6sUCx.jpeg" width="48" height="48" loading="lazy" role="presentation"/></div></div></div><div class="aw l"><h2 class="be tq hq z ii yc ik il yd in ip fh bj">Predictive Modeling w/ Python</h2><div class="be b ew z fd ab abi">20 stories<span class="ht l" aria-hidden="true"><span class="be b bf z fd">·</span></span>503<!-- --> <!-- -->saves</div></div></a></div><div class="vb vc vd lw ve vf vg ly vh vi vj vk vl vm vn vo vp vq vr vs vt"><a class="af ag ah ai aj ak al am an ao ap aq ar as at ab ck cm" href="https://destingong.medium.com/list/practical-guides-to-machine-learning-a877c2a39884?source=read_next_recirc-----4c6da3259463--------------------------------" rel="noopener follow"><div class="abo abp ii ab ia hi"><div class="hi wu abj bw abk"><div class="wu hc ii l"><img alt="Principal Component Analysis for ML" class="" src="https://miro.medium.com/v2/resize:fill:96:96/1*swd_PY6vTCyPnsgBYoFZfA.png" width="48" height="48" loading="lazy"/></div></div><div class="hi wu abj bw abl abm"><div class="wu hc ii l"><img alt="Time Series Analysis" class="" src="https://miro.medium.com/v2/resize:fill:96:96/1*8sSAHftNwd_RNJ3k4VA0pA.png" width="48" height="48" loading="lazy"/></div></div><div class="hi wu bw hm abn"><div class="wu hc ii l"><img alt="deep learning cheatsheet for beginner" class="" src="https://miro.medium.com/v2/resize:fill:96:96/1*uNyD4yNMH-DnOel1wzxOOA.png" width="48" height="48" loading="lazy"/></div></div></div><div class="aw l"><h2 class="be tq hq z ii yc ik il yd in ip fh bj">Practical Guides to Machine Learning</h2><div class="be b ew z fd ab abi">10 stories<span class="ht l" aria-hidden="true"><span class="be b bf z fd">·</span></span>575<!-- --> <!-- -->saves</div></div></a></div><div class="vb vc vd lw ve vf vg ly vh vi vj vk vl vm vn vo vp vq vr vs vt"><a class="af ag ah ai aj ak al am an ao ap aq ar as at ab ck cm" href="https://medium.com/@AMGAS14/list/natural-language-processing-0a856388a93a?source=read_next_recirc-----4c6da3259463--------------------------------" rel="noopener follow"><div class="abo abp ii ab ia hi"><div class="hi wu abj bw abk"><div class="wu hc ii l"><img alt="" class="" src="https://miro.medium.com/v2/resize:fill:96:96/1*8cAKzo9aJEtQJ6JNO99Q_g.png" width="48" height="48" loading="lazy" role="presentation"/></div></div><div class="hi wu abj bw abl abm"><div class="wu hc ii l"><img alt="" class="" src="https://miro.medium.com/v2/resize:fill:96:96/1*YZLpgfgla1EEdXmmg6ebsA.png" width="48" height="48" loading="lazy" role="presentation"/></div></div><div class="hi wu bw hm abn"><div class="wu hc ii l"><img alt="" class="" src="https://miro.medium.com/v2/resize:fill:96:96/1*oVpdNnl1VqexrfeXKOQa5A.png" width="48" height="48" loading="lazy" role="presentation"/></div></div></div><div class="aw l"><h2 class="be tq hq z ii yc ik il yd in ip fh bj">Natural Language Processing</h2><div class="be b ew z fd ab abi">723 stories<span class="ht l" aria-hidden="true"><span class="be b bf z fd">·</span></span>322<!-- --> <!-- -->saves</div></div></a></div><div class="vb vc vd lw ve vf vg ly vh vi vj vk vl vm vn vo vp vq vr vs vt"><a class="af ag ah ai aj ak al am an ao ap aq ar as at ab ck cm" href="https://medium.com/@MediumStaff/list/the-new-chatbots-chatgpt-bard-and-beyond-5969c7449b7f?source=read_next_recirc-----4c6da3259463--------------------------------" rel="noopener follow"><div class="abo abp ii ab ia hi"><div class="hi wu abj bw abk"><div class="wu hc ii l"><img alt="Image by vectorjuice on FreePik" class="" src="https://miro.medium.com/v2/resize:fill:96:96/0*3OsUtsnlTx9Svm4c.jpg" width="48" height="48" loading="lazy"/></div></div><div class="hi wu abj bw abl abm"><div class="wu hc ii l"><img alt="" class="" src="https://miro.medium.com/v2/resize:fill:96:96/1*IPZF1hcDWwpPqOz2vL7NxQ.png" width="48" height="48" loading="lazy" role="presentation"/></div></div><div class="hi wu bw hm abn"><div class="wu hc ii l"><img alt="" class="" src="https://miro.medium.com/v2/resize:fill:96:96/1*0fHUKyg3xtpNWpop35PR4g.png" width="48" height="48" loading="lazy" role="presentation"/></div></div></div><div class="aw l"><h2 class="be tq hq z ii yc ik il yd in ip fh bj">The New Chatbots: ChatGPT, Bard, and Beyond</h2><div class="be b ew z fd ab abi">12 stories<span class="ht l" aria-hidden="true"><span class="be b bf z fd">·</span></span>152<!-- --> <!-- -->saves</div></div></a></div></div></div><div class="tv bg tw zs dj zu dk abq abr abs abt abu abv"></div><div class="ul ab ka id um un uo up uq ur us ut uu uv uw ux uy uz va"><div class="vb vc vd lw ve vf vg ly vh vi vj vk vl vm vn vo vp vq vr vs vt"><div class="vu vv vw vx vy vz l"><article class="vz"><div class="vz ru l"><div class="bg vz"><div class="vz l"><div class="vz wa wb wc wd we wf wg wh wi wj wk wl wm"><div class="wn"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" aria-label="Training a Siamese model with a triplet loss function on MNIST dataset using PyTorch" href="https://medium.com/@mehranziadloo/training-a-siamese-model-with-a-triplet-loss-function-on-mnist-dataset-using-pytorch-225908e59bda?source=read_next_recirc-----4c6da3259463----0---------------------131088ad_b7ec_4260_ba56_4e3eb3e6766d-------" rel="noopener follow"><div class="wp wq wr ws wt"><img alt="Training a Siamese model with a triplet loss function on MNIST dataset using PyTorch" class="bg wu wv ww wx bw" src="https://miro.medium.com/v2/resize:fit:1358/1*ELi9QOrBPIixuSwBVctoMQ.jpeg" loading="lazy"/></div></a></div><div class="wo ab ca cn"><div class="wy wz xa xb xc ab"><div class="rg l"><div><div class="bl" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@mehranziadloo?source=read_next_recirc-----4c6da3259463----0---------------------131088ad_b7ec_4260_ba56_4e3eb3e6766d-------" rel="noopener follow"><div class="l hi"><img alt="Mehran Ziadloo" class="l ec bx xe xf cw" src="https://miro.medium.com/v2/resize:fill:40:40/0*ez2-73Y93GYNDgg0.jpg" width="20" height="20" loading="lazy"/><div class="xd bx l xe xf eh n ax hh"></div></div></a></div></div></div><div class="xg l"><div><div class="bl" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hs ab q" href="https://medium.com/@mehranziadloo?source=read_next_recirc-----4c6da3259463----0---------------------131088ad_b7ec_4260_ba56_4e3eb3e6766d-------" rel="noopener follow"><p class="be b ew z ii ij ik il im in io ip bj">Mehran Ziadloo</p></a></div></div></div></div><div class="xh xi xj xk xl xm xn xo xp xq l eo"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@mehranziadloo/training-a-siamese-model-with-a-triplet-loss-function-on-mnist-dataset-using-pytorch-225908e59bda?source=read_next_recirc-----4c6da3259463----0---------------------131088ad_b7ec_4260_ba56_4e3eb3e6766d-------" rel="noopener follow"><div title="Training a Siamese model with a triplet loss function on MNIST dataset using PyTorch"><h2 class="be fi ny gd xr xs oa ob gg xt xu od ni pu xv xw pv nm px xx xy py nq qa xz ya qb ii ik il in ip bj">Training a Siamese model with a triplet loss function on MNIST dataset using PyTorch</h2></div><div class="yb l"><h3 class="be b hq z ii yc ik il yd in ip fd">Let’s do an exercise and see how a simple Siamese model does on MNIST dataset when accompanied by a triplet loss function. I promise you…</h3></div></a></div><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@mehranziadloo/training-a-siamese-model-with-a-triplet-loss-function-on-mnist-dataset-using-pytorch-225908e59bda?source=read_next_recirc-----4c6da3259463----0---------------------131088ad_b7ec_4260_ba56_4e3eb3e6766d-------" rel="noopener follow"><span class="be b ew z fd"><div class="ab q"><span>11 min read</span><span class="ht l" aria-hidden="true"><span class="be b bf z fd">·</span></span><span>Aug 28</span></div></span></a><div class="ye yf yg yh yi l"><div class="ab co"><div class="am yj yk yl ym yn yo yp yq yr ys ab q"><div class="ab q ka"><div class="pw-multi-vote-icon hi ih kb kc kd"><div class=""><div class="ke kf kg kh ki kj kk am kl km kn kd"><svg width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" clip-rule="evenodd" d="M11.37.83L12 3.28l.63-2.45h-1.26zM15.42 1.84l-1.18-.39-.34 2.5 1.52-2.1zM9.76 1.45l-1.19.4 1.53 2.1-.34-2.5zM20.25 11.84l-2.5-4.4a1.42 1.42 0 0 0-.93-.64.96.96 0 0 0-.75.18c-.25.19-.4.42-.45.7l.05.05 2.35 4.13c1.62 2.95 1.1 5.78-1.52 8.4l-.46.41c1-.13 1.93-.6 2.78-1.45 2.7-2.7 2.51-5.59 1.43-7.38zM12.07 9.01c-.13-.69.08-1.3.57-1.77l-2.06-2.07a1.12 1.12 0 0 0-1.56 0c-.15.15-.22.34-.27.53L12.07 9z"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.74 8.3a1.13 1.13 0 0 0-.73-.5.67.67 0 0 0-.53.13c-.15.12-.59.46-.2 1.3l1.18 2.5a.45.45 0 0 1-.23.76.44.44 0 0 1-.48-.25L7.6 6.11a.82.82 0 1 0-1.15 1.15l3.64 3.64a.45.45 0 1 1-.63.63L5.83 7.9 4.8 6.86a.82.82 0 0 0-1.33.9c.04.1.1.18.18.26l1.02 1.03 3.65 3.64a.44.44 0 0 1-.15.73.44.44 0 0 1-.48-.1L4.05 9.68a.82.82 0 0 0-1.4.57.81.81 0 0 0 .24.58l1.53 1.54 2.3 2.28a.45.45 0 0 1-.64.63L3.8 13a.81.81 0 0 0-1.39.57c0 .22.09.43.24.58l4.4 4.4c2.8 2.8 5.5 4.12 8.68.94 2.27-2.28 2.71-4.6 1.34-7.1l-2.32-4.08z"></path></svg></div></div></div><div class="pw-multi-vote-count l ko kp kq kr ks kt ku"><p class="be b ew z fd"><span class="kf">--</span></p></div></div><div class="yt l"><div><div class="bl" aria-hidden="false"><a class="af kx ah ke aj ak al kw an ao ap aq ar as at kv ab q ky kz" aria-label="responses" href="https://medium.com/@mehranziadloo/training-a-siamese-model-with-a-triplet-loss-function-on-mnist-dataset-using-pytorch-225908e59bda?responsesOpen=true&amp;sortBy=REVERSE_CHRON&amp;source=read_next_recirc-----4c6da3259463----0---------------------131088ad_b7ec_4260_ba56_4e3eb3e6766d-------" rel="noopener follow"><svg width="24" height="24" viewBox="0 0 24 24" class=""><path d="M18 16.8a7.14 7.14 0 0 0 2.24-5.32c0-4.12-3.53-7.48-8.05-7.48C7.67 4 4 7.36 4 11.48c0 4.13 3.67 7.48 8.2 7.48a8.9 8.9 0 0 0 2.38-.32c.23.2.48.39.75.56 1.06.69 2.2 1.04 3.4 1.04.22 0 .4-.11.48-.29a.5.5 0 0 0-.04-.52 6.4 6.4 0 0 1-1.16-2.65v.02zm-3.12 1.06l-.06-.22-.32.1a8 8 0 0 1-2.3.33c-4.03 0-7.3-2.96-7.3-6.59S8.17 4.9 12.2 4.9c4 0 7.1 2.96 7.1 6.6 0 1.8-.6 3.47-2.02 4.72l-.2.16v.26l.02.3a6.74 6.74 0 0 0 .88 2.4 5.27 5.27 0 0 1-2.17-.86c-.28-.17-.72-.38-.94-.59l.01-.02z"></path></svg><p class="be b ew z fd"><span class="pw-responses-count yu la">1</span></p></a></div></div></div></div><div class="ab q yv yw"></div></div></div><div class="j i d"><div class="tv bg tw qq"></div></div></div></div></div></div></div></article></div></div><div class="vb vc vd lw ve vf vg ly vh vi vj vk vl vm vn vo vp vq vr vs vt"><div class="vu vv vw vx vy vz l"><article class="vz"><div class="vz ru l"><div class="bg vz"><div class="vz l"><div class="vz wa wb wc wd we wf wg wh wi wj wk wl wm"><div class="wn"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" aria-label="Audio Enhancement and Denoising Methods" href="https://ankurdhuriya.medium.com/audio-enhancement-and-denoising-methods-3644f0cad85b?source=read_next_recirc-----4c6da3259463----1---------------------131088ad_b7ec_4260_ba56_4e3eb3e6766d-------" rel="noopener follow"><div class="wp wq wr ws wt"><img alt="Audio Enhancement and Denoising Methods" class="bg wu wv ww wx bw" src="https://miro.medium.com/v2/resize:fit:1358/0*wA8EZHsEvgldNxbB" loading="lazy"/></div></a></div><div class="wo ab ca cn"><div class="wy wz xa xb xc ab"><div class="rg l"><div><div class="bl" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://ankurdhuriya.medium.com/?source=read_next_recirc-----4c6da3259463----1---------------------131088ad_b7ec_4260_ba56_4e3eb3e6766d-------" rel="noopener follow"><div class="l hi"><img alt="Ankur Dhuriya" class="l ec bx xe xf cw" src="https://miro.medium.com/v2/resize:fill:40:40/1*rLZWKn5flcb2AJZ2NpXv2g.jpeg" width="20" height="20" loading="lazy"/><div class="xd bx l xe xf eh n ax hh"></div></div></a></div></div></div><div class="xg l"><div><div class="bl" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hs ab q" href="https://ankurdhuriya.medium.com/?source=read_next_recirc-----4c6da3259463----1---------------------131088ad_b7ec_4260_ba56_4e3eb3e6766d-------" rel="noopener follow"><p class="be b ew z ii ij ik il im in io ip bj">Ankur Dhuriya</p></a></div></div></div></div><div class="xh xi xj xk xl xm xn xo xp xq l eo"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://ankurdhuriya.medium.com/audio-enhancement-and-denoising-methods-3644f0cad85b?source=read_next_recirc-----4c6da3259463----1---------------------131088ad_b7ec_4260_ba56_4e3eb3e6766d-------" rel="noopener follow"><div title=""><h2 class="be fi ny gd xr xs oa ob gg xt xu od ni pu xv xw pv nm px xx xy py nq qa xz ya qb ii ik il in ip bj">Audio Enhancement and Denoising Methods</h2></div><div class="yb l"><h3 class="be b hq z ii yc ik il yd in ip fd">Explore a range of powerful methods and techniques for audio enhancement and denoising. From spectral subtraction to deep learning…</h3></div></a></div><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://ankurdhuriya.medium.com/audio-enhancement-and-denoising-methods-3644f0cad85b?source=read_next_recirc-----4c6da3259463----1---------------------131088ad_b7ec_4260_ba56_4e3eb3e6766d-------" rel="noopener follow"><span class="be b ew z fd"><div class="ab q"><span>6 min read</span><span class="ht l" aria-hidden="true"><span class="be b bf z fd">·</span></span><span>Jul 11</span></div></span></a><div class="ye yf yg yh yi l"><div class="ab co"><div class="am yj yk yl ym yn yo yp yq yr ys ab q"><div class="ab q ka"><div class="pw-multi-vote-icon hi ih kb kc kd"><div class=""><div class="ke kf kg kh ki kj kk am kl km kn kd"><svg width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" clip-rule="evenodd" d="M11.37.83L12 3.28l.63-2.45h-1.26zM15.42 1.84l-1.18-.39-.34 2.5 1.52-2.1zM9.76 1.45l-1.19.4 1.53 2.1-.34-2.5zM20.25 11.84l-2.5-4.4a1.42 1.42 0 0 0-.93-.64.96.96 0 0 0-.75.18c-.25.19-.4.42-.45.7l.05.05 2.35 4.13c1.62 2.95 1.1 5.78-1.52 8.4l-.46.41c1-.13 1.93-.6 2.78-1.45 2.7-2.7 2.51-5.59 1.43-7.38zM12.07 9.01c-.13-.69.08-1.3.57-1.77l-2.06-2.07a1.12 1.12 0 0 0-1.56 0c-.15.15-.22.34-.27.53L12.07 9z"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.74 8.3a1.13 1.13 0 0 0-.73-.5.67.67 0 0 0-.53.13c-.15.12-.59.46-.2 1.3l1.18 2.5a.45.45 0 0 1-.23.76.44.44 0 0 1-.48-.25L7.6 6.11a.82.82 0 1 0-1.15 1.15l3.64 3.64a.45.45 0 1 1-.63.63L5.83 7.9 4.8 6.86a.82.82 0 0 0-1.33.9c.04.1.1.18.18.26l1.02 1.03 3.65 3.64a.44.44 0 0 1-.15.73.44.44 0 0 1-.48-.1L4.05 9.68a.82.82 0 0 0-1.4.57.81.81 0 0 0 .24.58l1.53 1.54 2.3 2.28a.45.45 0 0 1-.64.63L3.8 13a.81.81 0 0 0-1.39.57c0 .22.09.43.24.58l4.4 4.4c2.8 2.8 5.5 4.12 8.68.94 2.27-2.28 2.71-4.6 1.34-7.1l-2.32-4.08z"></path></svg></div></div></div><div class="pw-multi-vote-count l ko kp kq kr ks kt ku"><p class="be b ew z fd"><span class="kf">--</span></p></div></div><div class="yt l"><div><div class="bl" aria-hidden="false"><a class="af kx ah ke aj ak al kw an ao ap aq ar as at kv ab q ky kz" aria-label="responses" href="https://ankurdhuriya.medium.com/audio-enhancement-and-denoising-methods-3644f0cad85b?responsesOpen=true&amp;sortBy=REVERSE_CHRON&amp;source=read_next_recirc-----4c6da3259463----1---------------------131088ad_b7ec_4260_ba56_4e3eb3e6766d-------" rel="noopener follow"><svg width="24" height="24" viewBox="0 0 24 24" class=""><path d="M18 16.8a7.14 7.14 0 0 0 2.24-5.32c0-4.12-3.53-7.48-8.05-7.48C7.67 4 4 7.36 4 11.48c0 4.13 3.67 7.48 8.2 7.48a8.9 8.9 0 0 0 2.38-.32c.23.2.48.39.75.56 1.06.69 2.2 1.04 3.4 1.04.22 0 .4-.11.48-.29a.5.5 0 0 0-.04-.52 6.4 6.4 0 0 1-1.16-2.65v.02zm-3.12 1.06l-.06-.22-.32.1a8 8 0 0 1-2.3.33c-4.03 0-7.3-2.96-7.3-6.59S8.17 4.9 12.2 4.9c4 0 7.1 2.96 7.1 6.6 0 1.8-.6 3.47-2.02 4.72l-.2.16v.26l.02.3a6.74 6.74 0 0 0 .88 2.4 5.27 5.27 0 0 1-2.17-.86c-.28-.17-.72-.38-.94-.59l.01-.02z"></path></svg></a></div></div></div></div><div class="ab q yv yw"></div></div></div><div class="j i d"><div class="tv bg tw qq"></div></div></div></div></div></div></div></article></div></div><div class="vb vc vd lw ve vf vg ly vh vi vj vk vl vm vn vo vp vq vr vs vt"><div class="vu vv vw vx vy vz l"><article class="vz"><div class="vz ru l"><div class="bg vz"><div class="vz l"><div class="vz wa wb wc wd we wf wg wh wi wj wk wl wm"><div class="wn"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" aria-label="All Pairs Cosine Similarity in PyTorch" href="https://medium.com/@dhruvbird/all-pairs-cosine-similarity-in-pytorch-867e722c8572?source=read_next_recirc-----4c6da3259463----2---------------------131088ad_b7ec_4260_ba56_4e3eb3e6766d-------" rel="noopener follow"><div class="wp wq wr ws wt"><img alt="All Pairs Cosine Similarity in PyTorch" class="bg wu wv ww wx bw" src="https://miro.medium.com/v2/resize:fit:1358/1*NJbNjabBD8qOMqU1gMzpEQ.png" loading="lazy"/></div></a></div><div class="wo ab ca cn"><div class="wy wz xa xb xc ab"><div class="rg l"><div><div class="bl" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@dhruvbird?source=read_next_recirc-----4c6da3259463----2---------------------131088ad_b7ec_4260_ba56_4e3eb3e6766d-------" rel="noopener follow"><div class="l hi"><img alt="Dhruv Matani" class="l ec bx xe xf cw" src="https://miro.medium.com/v2/resize:fill:40:40/1*FYD0sd3nwpCGl88e7tNYUQ@2x.jpeg" width="20" height="20" loading="lazy"/><div class="xd bx l xe xf eh n ax hh"></div></div></a></div></div></div><div class="xg l"><div><div class="bl" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hs ab q" href="https://medium.com/@dhruvbird?source=read_next_recirc-----4c6da3259463----2---------------------131088ad_b7ec_4260_ba56_4e3eb3e6766d-------" rel="noopener follow"><p class="be b ew z ii ij ik il im in io ip bj">Dhruv Matani</p></a></div></div></div></div><div class="xh xi xj xk xl xm xn xo xp xq l eo"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@dhruvbird/all-pairs-cosine-similarity-in-pytorch-867e722c8572?source=read_next_recirc-----4c6da3259463----2---------------------131088ad_b7ec_4260_ba56_4e3eb3e6766d-------" rel="noopener follow"><div title=""><h2 class="be fi ny gd xr xs oa ob gg xt xu od ni pu xv xw pv nm px xx xy py nq qa xz ya qb ii ik il in ip bj">All Pairs Cosine Similarity in PyTorch</h2></div><div class="yb l"><h3 class="be b hq z ii yc ik il yd in ip fd">PyTorch defines a cosine_similarity function to compute pairwise cosine similarity between pairs of vectors. However, there’s no method to…</h3></div></a></div><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@dhruvbird/all-pairs-cosine-similarity-in-pytorch-867e722c8572?source=read_next_recirc-----4c6da3259463----2---------------------131088ad_b7ec_4260_ba56_4e3eb3e6766d-------" rel="noopener follow"><span class="be b ew z fd"><div class="ab q"><span>7 min read</span><span class="ht l" aria-hidden="true"><span class="be b bf z fd">·</span></span><span>Jun 8</span></div></span></a><div class="ye yf yg yh yi l"><div class="ab co"><div class="am yj yk yl ym yn yo yp yq yr ys ab q"><div class="ab q ka"><div class="pw-multi-vote-icon hi ih kb kc kd"><div class=""><div class="ke kf kg kh ki kj kk am kl km kn kd"><svg width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" clip-rule="evenodd" d="M11.37.83L12 3.28l.63-2.45h-1.26zM15.42 1.84l-1.18-.39-.34 2.5 1.52-2.1zM9.76 1.45l-1.19.4 1.53 2.1-.34-2.5zM20.25 11.84l-2.5-4.4a1.42 1.42 0 0 0-.93-.64.96.96 0 0 0-.75.18c-.25.19-.4.42-.45.7l.05.05 2.35 4.13c1.62 2.95 1.1 5.78-1.52 8.4l-.46.41c1-.13 1.93-.6 2.78-1.45 2.7-2.7 2.51-5.59 1.43-7.38zM12.07 9.01c-.13-.69.08-1.3.57-1.77l-2.06-2.07a1.12 1.12 0 0 0-1.56 0c-.15.15-.22.34-.27.53L12.07 9z"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.74 8.3a1.13 1.13 0 0 0-.73-.5.67.67 0 0 0-.53.13c-.15.12-.59.46-.2 1.3l1.18 2.5a.45.45 0 0 1-.23.76.44.44 0 0 1-.48-.25L7.6 6.11a.82.82 0 1 0-1.15 1.15l3.64 3.64a.45.45 0 1 1-.63.63L5.83 7.9 4.8 6.86a.82.82 0 0 0-1.33.9c.04.1.1.18.18.26l1.02 1.03 3.65 3.64a.44.44 0 0 1-.15.73.44.44 0 0 1-.48-.1L4.05 9.68a.82.82 0 0 0-1.4.57.81.81 0 0 0 .24.58l1.53 1.54 2.3 2.28a.45.45 0 0 1-.64.63L3.8 13a.81.81 0 0 0-1.39.57c0 .22.09.43.24.58l4.4 4.4c2.8 2.8 5.5 4.12 8.68.94 2.27-2.28 2.71-4.6 1.34-7.1l-2.32-4.08z"></path></svg></div></div></div><div class="pw-multi-vote-count l ko kp kq kr ks kt ku"><p class="be b ew z fd"><span class="kf">--</span></p></div></div><div class="yt l"><div><div class="bl" aria-hidden="false"><a class="af kx ah ke aj ak al kw an ao ap aq ar as at kv ab q ky kz" aria-label="responses" href="https://medium.com/@dhruvbird/all-pairs-cosine-similarity-in-pytorch-867e722c8572?responsesOpen=true&amp;sortBy=REVERSE_CHRON&amp;source=read_next_recirc-----4c6da3259463----2---------------------131088ad_b7ec_4260_ba56_4e3eb3e6766d-------" rel="noopener follow"><svg width="24" height="24" viewBox="0 0 24 24" class=""><path d="M18 16.8a7.14 7.14 0 0 0 2.24-5.32c0-4.12-3.53-7.48-8.05-7.48C7.67 4 4 7.36 4 11.48c0 4.13 3.67 7.48 8.2 7.48a8.9 8.9 0 0 0 2.38-.32c.23.2.48.39.75.56 1.06.69 2.2 1.04 3.4 1.04.22 0 .4-.11.48-.29a.5.5 0 0 0-.04-.52 6.4 6.4 0 0 1-1.16-2.65v.02zm-3.12 1.06l-.06-.22-.32.1a8 8 0 0 1-2.3.33c-4.03 0-7.3-2.96-7.3-6.59S8.17 4.9 12.2 4.9c4 0 7.1 2.96 7.1 6.6 0 1.8-.6 3.47-2.02 4.72l-.2.16v.26l.02.3a6.74 6.74 0 0 0 .88 2.4 5.27 5.27 0 0 1-2.17-.86c-.28-.17-.72-.38-.94-.59l.01-.02z"></path></svg><p class="be b ew z fd"><span class="pw-responses-count yu la">1</span></p></a></div></div></div></div><div class="ab q yv yw"></div></div></div><div class="j i d"><div class="tv bg tw qq"></div></div></div></div></div></div></div></article></div></div><div class="vb vc vd lw ve vf vg ly vh vi vj vk vl vm vn vo vp vq vr vs vt"><div class="vu vv vw vx vy vz l"><article class="vz"><div class="vz ru l"><div class="bg vz"><div class="vz l"><div class="vz wa wb wc wd we wf wg wh wi wj wk wl wm"><div class="wn"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" aria-label="Multi-Label Classification in Python: Empowering Machine Learning with Versatility" href="https://medium.com/@evertongomede/multi-label-classification-in-python-empowering-machine-learning-with-versatility-9dbae34aacdb?source=read_next_recirc-----4c6da3259463----3---------------------131088ad_b7ec_4260_ba56_4e3eb3e6766d-------" rel="noopener follow"><div class="wp wq wr ws wt"><img alt="Multi-Label Classification in Python: Empowering Machine Learning with Versatility" class="bg wu wv ww wx bw" src="https://miro.medium.com/v2/resize:fit:1358/0*ZWlF4k6QDlG_hQTr" loading="lazy"/></div></a></div><div class="wo ab ca cn"><div class="wy wz xa xb xc ab"><div class="rg l"><div><div class="bl" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@evertongomede?source=read_next_recirc-----4c6da3259463----3---------------------131088ad_b7ec_4260_ba56_4e3eb3e6766d-------" rel="noopener follow"><div class="l hi"><img alt="Everton Gomede, PhD" class="l ec bx xe xf cw" src="https://miro.medium.com/v2/resize:fill:40:40/1*9F4mg-YJd_HAkAySko5TWg.png" width="20" height="20" loading="lazy"/><div class="xd bx l xe xf eh n ax hh"></div></div></a></div></div></div><div class="xg l"><div><div class="bl" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hs ab q" href="https://medium.com/@evertongomede?source=read_next_recirc-----4c6da3259463----3---------------------131088ad_b7ec_4260_ba56_4e3eb3e6766d-------" rel="noopener follow"><p class="be b ew z ii ij ik il im in io ip bj">Everton Gomede, PhD</p></a></div></div></div></div><div class="xh xi xj xk xl xm xn xo xp xq l eo"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@evertongomede/multi-label-classification-in-python-empowering-machine-learning-with-versatility-9dbae34aacdb?source=read_next_recirc-----4c6da3259463----3---------------------131088ad_b7ec_4260_ba56_4e3eb3e6766d-------" rel="noopener follow"><div title="Multi-Label Classification in Python: Empowering Machine Learning with Versatility"><h2 class="be fi ny gd xr xs oa ob gg xt xu od ni pu xv xw pv nm px xx xy py nq qa xz ya qb ii ik il in ip bj">Multi-Label Classification in Python: Empowering Machine Learning with Versatility</h2></div><div class="yb l"><h3 class="be b hq z ii yc ik il yd in ip fd">Introduction</h3></div></a></div><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@evertongomede/multi-label-classification-in-python-empowering-machine-learning-with-versatility-9dbae34aacdb?source=read_next_recirc-----4c6da3259463----3---------------------131088ad_b7ec_4260_ba56_4e3eb3e6766d-------" rel="noopener follow"><span class="be b ew z fd"><div class="ab q"><span>6 min read</span><span class="ht l" aria-hidden="true"><span class="be b bf z fd">·</span></span><span>May 24</span></div></span></a><div class="ye yf yg yh yi l"><div class="ab co"><div class="am yj yk yl ym yn yo yp yq yr ys ab q"><div class="ab q ka"><div class="pw-multi-vote-icon hi ih kb kc kd"><div class=""><div class="ke kf kg kh ki kj kk am kl km kn kd"><svg width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" clip-rule="evenodd" d="M11.37.83L12 3.28l.63-2.45h-1.26zM15.42 1.84l-1.18-.39-.34 2.5 1.52-2.1zM9.76 1.45l-1.19.4 1.53 2.1-.34-2.5zM20.25 11.84l-2.5-4.4a1.42 1.42 0 0 0-.93-.64.96.96 0 0 0-.75.18c-.25.19-.4.42-.45.7l.05.05 2.35 4.13c1.62 2.95 1.1 5.78-1.52 8.4l-.46.41c1-.13 1.93-.6 2.78-1.45 2.7-2.7 2.51-5.59 1.43-7.38zM12.07 9.01c-.13-.69.08-1.3.57-1.77l-2.06-2.07a1.12 1.12 0 0 0-1.56 0c-.15.15-.22.34-.27.53L12.07 9z"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.74 8.3a1.13 1.13 0 0 0-.73-.5.67.67 0 0 0-.53.13c-.15.12-.59.46-.2 1.3l1.18 2.5a.45.45 0 0 1-.23.76.44.44 0 0 1-.48-.25L7.6 6.11a.82.82 0 1 0-1.15 1.15l3.64 3.64a.45.45 0 1 1-.63.63L5.83 7.9 4.8 6.86a.82.82 0 0 0-1.33.9c.04.1.1.18.18.26l1.02 1.03 3.65 3.64a.44.44 0 0 1-.15.73.44.44 0 0 1-.48-.1L4.05 9.68a.82.82 0 0 0-1.4.57.81.81 0 0 0 .24.58l1.53 1.54 2.3 2.28a.45.45 0 0 1-.64.63L3.8 13a.81.81 0 0 0-1.39.57c0 .22.09.43.24.58l4.4 4.4c2.8 2.8 5.5 4.12 8.68.94 2.27-2.28 2.71-4.6 1.34-7.1l-2.32-4.08z"></path></svg></div></div></div><div class="pw-multi-vote-count l ko kp kq kr ks kt ku"><p class="be b ew z fd"><span class="kf">--</span></p></div></div><div class="yt l"><div><div class="bl" aria-hidden="false"><a class="af kx ah ke aj ak al kw an ao ap aq ar as at kv ab q ky kz" aria-label="responses" href="https://medium.com/@evertongomede/multi-label-classification-in-python-empowering-machine-learning-with-versatility-9dbae34aacdb?responsesOpen=true&amp;sortBy=REVERSE_CHRON&amp;source=read_next_recirc-----4c6da3259463----3---------------------131088ad_b7ec_4260_ba56_4e3eb3e6766d-------" rel="noopener follow"><svg width="24" height="24" viewBox="0 0 24 24" class=""><path d="M18 16.8a7.14 7.14 0 0 0 2.24-5.32c0-4.12-3.53-7.48-8.05-7.48C7.67 4 4 7.36 4 11.48c0 4.13 3.67 7.48 8.2 7.48a8.9 8.9 0 0 0 2.38-.32c.23.2.48.39.75.56 1.06.69 2.2 1.04 3.4 1.04.22 0 .4-.11.48-.29a.5.5 0 0 0-.04-.52 6.4 6.4 0 0 1-1.16-2.65v.02zm-3.12 1.06l-.06-.22-.32.1a8 8 0 0 1-2.3.33c-4.03 0-7.3-2.96-7.3-6.59S8.17 4.9 12.2 4.9c4 0 7.1 2.96 7.1 6.6 0 1.8-.6 3.47-2.02 4.72l-.2.16v.26l.02.3a6.74 6.74 0 0 0 .88 2.4 5.27 5.27 0 0 1-2.17-.86c-.28-.17-.72-.38-.94-.59l.01-.02z"></path></svg></a></div></div></div></div><div class="ab q yv yw"></div></div></div></div></div></div></div></div></article></div></div></div><div class="tv bg tw dj dk yy yz za"></div><a class="be b bf z bj rh ze zf zg zh ky zi sx sy hx zj zk zl tb zm zn zo zp zq td te ec bl ff mu" href="https://medium.com/?source=post_page-----4c6da3259463--------------------------------" rel="noopener follow"><div class="l mu">See more recommendations</div></a></div></div></div><div class="h k j"><div class="tv bg tw uc"></div><div class="ab ca"><div class="ch bg dx dy dz ea"><div class="ud ab ka id"><div class="ue uf l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://help.medium.com/hc/en-us?source=post_page-----4c6da3259463--------------------------------" rel="noopener follow"><p class="be b ew z fd">Help</p></a></div><div class="ue uf l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.statuspage.io/?source=post_page-----4c6da3259463--------------------------------" rel="noopener follow"><p class="be b ew z fd">Status</p></a></div><div class="ue uf l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/about?autoplay=1&amp;source=post_page-----4c6da3259463--------------------------------" rel="noopener follow"><p class="be b ew z fd">About</p></a></div><div class="ue uf l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----4c6da3259463--------------------------------" rel="noopener follow"><p class="be b ew z fd">Careers</p></a></div><div class="ue uf l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://blog.medium.com/?source=post_page-----4c6da3259463--------------------------------" rel="noopener follow"><p class="be b ew z fd">Blog</p></a></div><div class="ue uf l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----4c6da3259463--------------------------------" rel="noopener follow"><p class="be b ew z fd">Privacy</p></a></div><div class="ue uf l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----4c6da3259463--------------------------------" rel="noopener follow"><p class="be b ew z fd">Terms</p></a></div><div class="ue uf l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://speechify.com/medium?source=post_page-----4c6da3259463--------------------------------" rel="noopener follow"><p class="be b ew z fd">Text to speech</p></a></div><div class="ue l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/business?source=post_page-----4c6da3259463--------------------------------" rel="noopener follow"><p class="be b ew z fd">Teams</p></a></div></div></div></div></div></div></div></div></div></div><script>window.__BUILD_ID__="main-20231019-200931-1beefc7074"</script><script>window.__GRAPHQL_URI__ = "https://towardsdatascience.com/_/graphql"</script><script>window.__PRELOADED_STATE__ = {"algolia":{"queries":{}},"cache":{"experimentGroupSet":true,"reason":"","group":"enabled","tags":["group-edgeCachePosts","post-4c6da3259463","user-9eff5ca21d80","collection-7f60cf5620c9"],"serverVariantState":"44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a","middlewareEnabled":true,"cacheStatus":"DYNAMIC","shouldUseCache":true,"vary":[],"inDisabledExperiment":false,"loHomepageEnabled":false},"client":{"hydrated":false,"isUs":false,"isNativeMedium":false,"isSafariMobile":false,"isSafari":true,"isFirefox":false,"routingEntity":{"type":"COLLECTION","id":"7f60cf5620c9","explicit":true},"viewerIsBot":false},"debug":{"requestId":"5fcb98ac-d9f2-4310-adee-85cb480dac72","hybridDevServices":[],"originalSpanCarrier":{"ot-tracer-spanid":"5792f74156156819","ot-tracer-traceid":"2b699828acd6a93a","ot-tracer-sampled":"true"}},"multiVote":{"clapsPerPost":{}},"navigation":{"branch":{"show":null,"hasRendered":null,"blockedByCTA":false},"hideGoogleOneTap":false,"hasRenderedAlternateUserBanner":null,"currentLocation":"https:\u002F\u002Ftowardsdatascience.com\u002Fhow-to-train-your-siamese-neural-network-4c6da3259463","host":"towardsdatascience.com","hostname":"towardsdatascience.com","referrer":"","hasSetReferrer":false,"susiModal":{"step":null,"operation":"register"},"postRead":false,"queryString":"","currentHash":""},"config":{"nodeEnv":"production","version":"main-20231019-200931-1beefc7074","target":"production","productName":"Medium","publicUrl":"https:\u002F\u002Fcdn-client.medium.com\u002Flite","authDomain":"medium.com","authGoogleClientId":"216296035834-k1k6qe060s2tp2a2jam4ljdcms00sttg.apps.googleusercontent.com","favicon":"production","glyphUrl":"https:\u002F\u002Fglyph.medium.com","branchKey":"key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm","algolia":{"appId":"MQ57UUUQZ2","apiKeySearch":"394474ced050e3911ae2249ecc774921","indexPrefix":"medium_","host":"-dsn.algolia.net"},"recaptchaKey":"6Lfc37IUAAAAAKGGtC6rLS13R1Hrw_BqADfS1LRk","recaptcha3Key":"6Lf8R9wUAAAAABMI_85Wb8melS7Zj6ziuf99Yot5","datadog":{"applicationId":"6702d87d-a7e0-42fe-bbcb-95b469547ea0","clientToken":"pub853ea8d17ad6821d9f8f11861d23dfed","rumToken":"pubf9cc52896502b9413b68ba36fc0c7162","context":{"deployment":{"target":"production","tag":"main-20231019-200931-1beefc7074","commit":"1beefc707464ede944ab562b123e76f4611cd170"}},"datacenter":"us"},"googleAnalyticsCode":"G-7JY7T788PK","googlePay":{"apiVersion":"2","apiVersionMinor":"0","merchantId":"BCR2DN6TV7EMTGBM","merchantName":"Medium","instanceMerchantId":"13685562959212738550"},"applePay":{"version":3},"signInWallCustomDomainCollectionIds":["3a8144eabfe3","336d898217ee","61061eb0c96b","138adf9c44c","819cc2aaeee0"],"mediumMastodonDomainName":"me.dm","mediumOwnedAndOperatedCollectionIds":["8a9336e5bb4","b7e45b22fec3","193b68bd4fba","8d6b8a439e32","54c98c43354d","3f6ecf56618","d944778ce714","92d2092dc598","ae2a65f35510","1285ba81cada","544c7006046e","fc8964313712","40187e704f1c","88d9857e584e","7b6769f2748b","bcc38c8f6edf","cef6983b292","cb8577c9149e","444d13b52878","713d7dbc99b0","ef8e90590e66","191186aaafa0","55760f21cdc5","9dc80918cc93","bdc4052bbdba","8ccfed20cbb2"],"tierOneDomains":["medium.com","thebolditalic.com","arcdigital.media","towardsdatascience.com","uxdesign.cc","codeburst.io","psiloveyou.xyz","writingcooperative.com","entrepreneurshandbook.co","prototypr.io","betterhumans.coach.me","theascent.pub"],"topicsToFollow":["d61cf867d93f","8a146bc21b28","1eca0103fff3","4d562ee63426","aef1078a3ef5","e15e46793f8d","6158eb913466","55f1c20aba7a","3d18b94f6858","4861fee224fd","63c6f1f93ee","1d98b3a9a871","decb52b64abf","ae5d4995e225","830cded25262"],"topicToTagMappings":{"accessibility":"accessibility","addiction":"addiction","android-development":"android-development","art":"art","artificial-intelligence":"artificial-intelligence","astrology":"astrology","basic-income":"basic-income","beauty":"beauty","biotech":"biotech","blockchain":"blockchain","books":"books","business":"business","cannabis":"cannabis","cities":"cities","climate-change":"climate-change","comics":"comics","coronavirus":"coronavirus","creativity":"creativity","cryptocurrency":"cryptocurrency","culture":"culture","cybersecurity":"cybersecurity","data-science":"data-science","design":"design","digital-life":"digital-life","disability":"disability","economy":"economy","education":"education","equality":"equality","family":"family","feminism":"feminism","fiction":"fiction","film":"film","fitness":"fitness","food":"food","freelancing":"freelancing","future":"future","gadgets":"gadgets","gaming":"gaming","gun-control":"gun-control","health":"health","history":"history","humor":"humor","immigration":"immigration","ios-development":"ios-development","javascript":"javascript","justice":"justice","language":"language","leadership":"leadership","lgbtqia":"lgbtqia","lifestyle":"lifestyle","machine-learning":"machine-learning","makers":"makers","marketing":"marketing","math":"math","media":"media","mental-health":"mental-health","mindfulness":"mindfulness","money":"money","music":"music","neuroscience":"neuroscience","nonfiction":"nonfiction","outdoors":"outdoors","parenting":"parenting","pets":"pets","philosophy":"philosophy","photography":"photography","podcasts":"podcast","poetry":"poetry","politics":"politics","privacy":"privacy","product-management":"product-management","productivity":"productivity","programming":"programming","psychedelics":"psychedelics","psychology":"psychology","race":"race","relationships":"relationships","religion":"religion","remote-work":"remote-work","san-francisco":"san-francisco","science":"science","self":"self","self-driving-cars":"self-driving-cars","sexuality":"sexuality","social-media":"social-media","society":"society","software-engineering":"software-engineering","space":"space","spirituality":"spirituality","sports":"sports","startups":"startup","style":"style","technology":"technology","transportation":"transportation","travel":"travel","true-crime":"true-crime","tv":"tv","ux":"ux","venture-capital":"venture-capital","visual-design":"visual-design","work":"work","world":"world","writing":"writing"},"defaultImages":{"avatar":{"imageId":"1*dmbNkD5D-u45r44go_cf0g.png","height":150,"width":150},"orgLogo":{"imageId":"1*OMF3fSqH8t4xBJ9-6oZDZw.png","height":106,"width":545},"postLogo":{"imageId":"1*kFrc4tBFM_tCis-2Ic87WA.png","height":810,"width":1440},"postPreviewImage":{"imageId":"1*hn4v1tCaJy7cWMyb0bpNpQ.png","height":386,"width":579}},"collectionStructuredData":{"8d6b8a439e32":{"name":"Elemental","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F980\u002F1*9ygdqoKprhwuTVKUM0DLPA@2x.png","width":980,"height":159}}},"3f6ecf56618":{"name":"Forge","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F596\u002F1*uULpIlImcO5TDuBZ6lm7Lg@2x.png","width":596,"height":183}}},"ae2a65f35510":{"name":"GEN","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F264\u002F1*RdVZMdvfV3YiZTw6mX7yWA.png","width":264,"height":140}}},"88d9857e584e":{"name":"LEVEL","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*JqYMhNX6KNNb2UlqGqO2WQ.png","width":540,"height":108}}},"7b6769f2748b":{"name":"Marker","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F383\u002F1*haCUs0wF6TgOOvfoY-jEoQ@2x.png","width":383,"height":92}}},"444d13b52878":{"name":"OneZero","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*cw32fIqCbRWzwJaoQw6BUg.png","width":540,"height":123}}},"8ccfed20cbb2":{"name":"Zora","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*tZUQqRcCCZDXjjiZ4bDvgQ.png","width":540,"height":106}}}},"embeddedPostIds":{"coronavirus":"cd3010f9d81f"},"sharedCdcMessaging":{"COVID_APPLICABLE_TAG_SLUGS":[],"COVID_APPLICABLE_TOPIC_NAMES":[],"COVID_APPLICABLE_TOPIC_NAMES_FOR_TOPIC_PAGE":[],"COVID_MESSAGES":{"tierA":{"text":"For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":66,"end":73,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"tierB":{"text":"Anyone can publish on Medium per our Policies, but we don’t fact-check every story. For more info about the coronavirus, see cdc.gov.","markups":[{"start":37,"end":45,"href":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Fcategories\u002F201931128-Policies-Safety"},{"start":125,"end":132,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"paywall":{"text":"This article has been made free for everyone, thanks to Medium Members. For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":56,"end":70,"href":"https:\u002F\u002Fmedium.com\u002Fmembership"},{"start":138,"end":145,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"unbound":{"text":"This article is free for everyone, thanks to Medium Members. For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":45,"end":59,"href":"https:\u002F\u002Fmedium.com\u002Fmembership"},{"start":127,"end":134,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]}},"COVID_BANNER_POST_ID_OVERRIDE_WHITELIST":["3b31a67bff4a"]},"sharedVoteMessaging":{"TAGS":["politics","election-2020","government","us-politics","election","2020-presidential-race","trump","donald-trump","democrats","republicans","congress","republican-party","democratic-party","biden","joe-biden","maga"],"TOPICS":["politics","election"],"MESSAGE":{"text":"Find out more about the U.S. election results here.","markups":[{"start":46,"end":50,"href":"https:\u002F\u002Fcookpolitical.com\u002F2020-national-popular-vote-tracker"}]},"EXCLUDE_POSTS":["397ef29e3ca5"]},"embedPostRules":[],"recircOptions":{"v1":{"limit":3},"v2":{"limit":8}},"braintreeClientKey":"production_zjkj96jm_m56f8fqpf7ngnrd4","braintree":{"enabled":true,"merchantId":"m56f8fqpf7ngnrd4","merchantAccountId":{"usd":"AMediumCorporation_instant","eur":"amediumcorporation_EUR","cad":"amediumcorporation_CAD"},"publicKey":"ds2nn34bg2z7j5gd","braintreeEnvironment":"production","dashboardUrl":"https:\u002F\u002Fwww.braintreegateway.com\u002Fmerchants","gracePeriodDurationInDays":14,"mediumMembershipPlanId":{"monthly":"ce105f8c57a3","monthlyWithTrial":"d5ee3dbe3db8","monthlyPremium":"fa741a9b47a2","yearly":"a40ad4a43185","yearlyStaff":"d74fb811198a","yearlyWithTrial":"b3bc7350e5c7","yearlyPremium":"e21bd2c12166","monthlyCad":"p52orjkaceei","yearlyCad":"h4q9g2up9ktt"},"braintreeDiscountId":{"oneMonthFree":"MONTHS_FREE_01","threeMonthsFree":"MONTHS_FREE_03","sixMonthsFree":"MONTHS_FREE_06","fiftyPercentOffOneYear":"FIFTY_PERCENT_OFF_ONE_YEAR"},"3DSecureVersion":"2","defaultCurrency":"usd","providerPlanIdCurrency":{"4ycw":"usd","rz3b":"usd","3kqm":"usd","jzw6":"usd","c2q2":"usd","nnsw":"usd","q8qw":"usd","d9y6":"usd","fx7w":"cad","nwf2":"cad"}},"paypalClientId":"AXj1G4fotC2GE8KzWX9mSxCH1wmPE3nJglf4Z2ig_amnhvlMVX87otaq58niAg9iuLktVNF_1WCMnN7v","paypal":{"host":"https:\u002F\u002Fapi.paypal.com:443","clientMode":"production","serverMode":"live","webhookId":"4G466076A0294510S","monthlyPlan":{"planId":"P-9WR0658853113943TMU5FDQA","name":"Medium Membership (Monthly) with setup fee","description":"Unlimited access to the best and brightest stories on Medium. Membership billed monthly."},"yearlyPlan":{"planId":"P-7N8963881P8875835MU5JOPQ","name":"Medium Membership (Annual) with setup fee","description":"Unlimited access to the best and brightest stories on Medium. Membership billed annually."},"oneYearGift":{"name":"Medium Membership (1 Year, Digital Gift Code)","description":"Unlimited access to the best and brightest stories on Medium. Gift codes can be redeemed at medium.com\u002Fredeem.","price":"50.00","currency":"USD","sku":"membership-gift-1-yr"},"oldMonthlyPlan":{"planId":"P-96U02458LM656772MJZUVH2Y","name":"Medium Membership (Monthly)","description":"Unlimited access to the best and brightest stories on Medium. Membership billed monthly."},"oldYearlyPlan":{"planId":"P-59P80963JF186412JJZU3SMI","name":"Medium Membership (Annual)","description":"Unlimited access to the best and brightest stories on Medium. Membership billed annually."},"monthlyPlanWithTrial":{"planId":"P-66C21969LR178604GJPVKUKY","name":"Medium Membership (Monthly) with setup fee","description":"Unlimited access to the best and brightest stories on Medium. Membership billed monthly."},"yearlyPlanWithTrial":{"planId":"P-6XW32684EX226940VKCT2MFA","name":"Medium Membership (Annual) with setup fee","description":"Unlimited access to the best and brightest stories on Medium. Membership billed annually."},"oldMonthlyPlanNoSetupFee":{"planId":"P-4N046520HR188054PCJC7LJI","name":"Medium Membership (Monthly)","description":"Unlimited access to the best and brightest stories on Medium. Membership billed monthly."},"oldYearlyPlanNoSetupFee":{"planId":"P-7A4913502Y5181304CJEJMXQ","name":"Medium Membership (Annual)","description":"Unlimited access to the best and brightest stories on Medium. Membership billed annually."},"sdkUrl":"https:\u002F\u002Fwww.paypal.com\u002Fsdk\u002Fjs"},"stripePublishableKey":"pk_live_7FReX44VnNIInZwrIIx6ghjl","log":{"json":true,"level":"info"},"imageUploadMaxSizeMb":25,"staffPicks":{"title":"Staff Picks","catalogId":"c7bc6e1ee00f"}},"session":{"xsrf":""}}</script><script>window.__APOLLO_STATE__ = {"ROOT_QUERY":{"__typename":"Query","isLoggedIn":false,"collectionByDomainOrSlug({\"domainOrSlug\":\"towardsdatascience.com\"})":{"__ref":"Collection:7f60cf5620c9"},"postResult({\"id\":\"4c6da3259463\"})":{"__ref":"Post:4c6da3259463"},"post({\"id\":\"4c6da3259463\"})":{"__ref":"Post:4c6da3259463"},"authorCollectionRecircFeed({\"input\":{\"authorId\":\"9eff5ca21d80\",\"collectionId\":\"7f60cf5620c9\",\"paging\":{\"limit\":4},\"postId\":\"4c6da3259463\"}})":{"__typename":"AuthorCollectionRecircFeedResult","items":[{"__typename":"HomeFeedItem","post":{"__ref":"Post:3d9008235f41"},"feedId":"3a083127-c893-4424-b19c-c1eccf1006a9"},{"__typename":"HomeFeedItem","post":{"__ref":"Post:c9cec11fd1b"},"feedId":"3a083127-c893-4424-b19c-c1eccf1006a9"},{"__typename":"HomeFeedItem","post":{"__ref":"Post:1147298d8ad1"},"feedId":"3a083127-c893-4424-b19c-c1eccf1006a9"},{"__typename":"HomeFeedItem","post":{"__ref":"Post:1219840d0a0a"},"feedId":"3a083127-c893-4424-b19c-c1eccf1006a9"}]},"recirc({\"paging\":{\"limit\":6},\"postId\":\"4c6da3259463\"})":{"__typename":"RexRecircResult","items":[{"__typename":"RexRecircItem","feedId":"131088ad-b7ec-4260-ba56-4e3eb3e6766d","post":{"__ref":"Post:4f00e2dd8256"}},{"__typename":"RexRecircItem","feedId":"131088ad-b7ec-4260-ba56-4e3eb3e6766d","post":{"__ref":"Post:ebb2bb6efdb1"}},{"__typename":"RexRecircItem","feedId":"131088ad-b7ec-4260-ba56-4e3eb3e6766d","post":{"__ref":"Post:225908e59bda"}},{"__typename":"RexRecircItem","feedId":"131088ad-b7ec-4260-ba56-4e3eb3e6766d","post":{"__ref":"Post:3644f0cad85b"}},{"__typename":"RexRecircItem","feedId":"131088ad-b7ec-4260-ba56-4e3eb3e6766d","post":{"__ref":"Post:867e722c8572"}},{"__typename":"RexRecircItem","feedId":"131088ad-b7ec-4260-ba56-4e3eb3e6766d","post":{"__ref":"Post:9dbae34aacdb"}}]},"postCatalogRecirc({\"pagingOptions\":{\"limit\":4},\"postId\":\"4c6da3259463\"})":{"__typename":"CatalogsConnection","catalogs":[{"__ref":"Catalog:e3668ea008e1"},{"__ref":"Catalog:a877c2a39884"},{"__ref":"Catalog:0a856388a93a"},{"__ref":"Catalog:5969c7449b7f"}]}},"ImageMetadata:1*VzTUkfeGymHP4Bvav-T-lA.png":{"__typename":"ImageMetadata","id":"1*VzTUkfeGymHP4Bvav-T-lA.png"},"Collection:7f60cf5620c9":{"__typename":"Collection","id":"7f60cf5620c9","favicon":{"__ref":"ImageMetadata:1*VzTUkfeGymHP4Bvav-T-lA.png"},"customStyleSheet":null,"colorPalette":{"__typename":"ColorPalette","highlightSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FFFFFFFF","colorPoints":[{"__typename":"ColorPoint","color":"#FFEDF4FC","point":0},{"__typename":"ColorPoint","color":"#FFE9F2FD","point":0.1},{"__typename":"ColorPoint","color":"#FFE6F1FD","point":0.2},{"__typename":"ColorPoint","color":"#FFE2EFFD","point":0.3},{"__typename":"ColorPoint","color":"#FFDFEEFD","point":0.4},{"__typename":"ColorPoint","color":"#FFDBECFE","point":0.5},{"__typename":"ColorPoint","color":"#FFD7EBFE","point":0.6},{"__typename":"ColorPoint","color":"#FFD4E9FE","point":0.7},{"__typename":"ColorPoint","color":"#FFD0E7FF","point":0.8},{"__typename":"ColorPoint","color":"#FFCCE6FF","point":0.9},{"__typename":"ColorPoint","color":"#FFC8E4FF","point":1}]},"defaultBackgroundSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FFFFFFFF","colorPoints":[{"__typename":"ColorPoint","color":"#FF668AAA","point":0},{"__typename":"ColorPoint","color":"#FF61809D","point":0.1},{"__typename":"ColorPoint","color":"#FF5A7690","point":0.2},{"__typename":"ColorPoint","color":"#FF546C83","point":0.3},{"__typename":"ColorPoint","color":"#FF4D6275","point":0.4},{"__typename":"ColorPoint","color":"#FF455768","point":0.5},{"__typename":"ColorPoint","color":"#FF3D4C5A","point":0.6},{"__typename":"ColorPoint","color":"#FF34414C","point":0.7},{"__typename":"ColorPoint","color":"#FF2B353E","point":0.8},{"__typename":"ColorPoint","color":"#FF21282F","point":0.9},{"__typename":"ColorPoint","color":"#FF161B1F","point":1}]},"tintBackgroundSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FF355876","colorPoints":[{"__typename":"ColorPoint","color":"#FF355876","point":0},{"__typename":"ColorPoint","color":"#FF4D6C88","point":0.1},{"__typename":"ColorPoint","color":"#FF637F99","point":0.2},{"__typename":"ColorPoint","color":"#FF7791A8","point":0.3},{"__typename":"ColorPoint","color":"#FF8CA2B7","point":0.4},{"__typename":"ColorPoint","color":"#FF9FB3C6","point":0.5},{"__typename":"ColorPoint","color":"#FFB2C3D4","point":0.6},{"__typename":"ColorPoint","color":"#FFC5D2E1","point":0.7},{"__typename":"ColorPoint","color":"#FFD7E2EE","point":0.8},{"__typename":"ColorPoint","color":"#FFE9F1FA","point":0.9},{"__typename":"ColorPoint","color":"#FFFBFFFF","point":1}]}},"googleAnalyticsId":null,"editors":[{"__typename":"CollectionMastheadUserItem","user":{"__ref":"User:7e12c71dfa81"}},{"__typename":"CollectionMastheadUserItem","user":{"__ref":"User:e6ad8abedec9"}},{"__typename":"CollectionMastheadUserItem","user":{"__ref":"User:895063a310f4"}}],"name":"Towards Data Science","avatar":{"__ref":"ImageMetadata:1*CJe3891yB1A1mzMdqemkdg.jpeg"},"domain":"towardsdatascience.com","slug":"towards-data-science","description":"Your home for data science. A Medium publication sharing concepts, ideas and codes.","subscriberCount":675516,"viewerEdge":{"__ref":"CollectionViewerEdge:collectionId:7f60cf5620c9-viewerId:lo_98c5c93f7e53"},"twitterUsername":"TDataScience","facebookPageId":null,"logo":{"__ref":"ImageMetadata:1*cFFKn8rFH4ZndmaYeAs6iQ.png"}},"User:7e12c71dfa81":{"__typename":"User","id":"7e12c71dfa81"},"User:e6ad8abedec9":{"__typename":"User","id":"e6ad8abedec9"},"User:895063a310f4":{"__typename":"User","id":"895063a310f4"},"ImageMetadata:1*CJe3891yB1A1mzMdqemkdg.jpeg":{"__typename":"ImageMetadata","id":"1*CJe3891yB1A1mzMdqemkdg.jpeg"},"LinkedAccounts:9eff5ca21d80":{"__typename":"LinkedAccounts","mastodon":null,"id":"9eff5ca21d80"},"UserViewerEdge:userId:9eff5ca21d80-viewerId:lo_98c5c93f7e53":{"__typename":"UserViewerEdge","id":"userId:9eff5ca21d80-viewerId:lo_98c5c93f7e53","isFollowing":false,"isUser":false},"NewsletterV3:f1843dddb687":{"__typename":"NewsletterV3","id":"f1843dddb687","type":"NEWSLETTER_TYPE_AUTHOR","slug":"9eff5ca21d80","name":"9eff5ca21d80","collection":null,"user":{"__ref":"User:9eff5ca21d80"}},"User:9eff5ca21d80":{"__typename":"User","id":"9eff5ca21d80","name":"Cameron Trotter","username":"c.trotter2","newsletterV3":{"__ref":"NewsletterV3:f1843dddb687"},"linkedAccounts":{"__ref":"LinkedAccounts:9eff5ca21d80"},"isSuspended":false,"imageId":"1*tA2cVcxhp0PDoY9CDtvL0g.png","mediumMemberAt":0,"verifications":{"__typename":"VerifiedInfo","isBookAuthor":false},"socialStats":{"__typename":"SocialStats","followerCount":28},"customDomainState":null,"hasSubdomain":false,"bio":"Computer Vision PhD Student @ Newcastle University (UK)","isPartnerProgramEnrolled":false,"viewerEdge":{"__ref":"UserViewerEdge:userId:9eff5ca21d80-viewerId:lo_98c5c93f7e53"},"viewerIsUser":false,"postSubscribeMembershipUpsellShownAt":0,"allowNotes":true,"membership":null,"twitterScreenName":""},"Topic:ae5d4995e225":{"__typename":"Topic","slug":"data-science","id":"ae5d4995e225","name":"Data Science"},"Paragraph:314b8592efe0_0":{"__typename":"Paragraph","id":"314b8592efe0_0","name":"8e4f","type":"H4","href":null,"layout":null,"metadata":null,"text":"Hands-on Tutorials","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":0,"end":18,"href":"https:\u002F\u002Ftowardsdatascience.com\u002Ftagged\u002Fhands-on-tutorials","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_1":{"__typename":"Paragraph","id":"314b8592efe0_1","name":"a587","type":"H3","href":null,"layout":null,"metadata":null,"text":"How To Train Your Siamese Neural Network","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_2":{"__typename":"Paragraph","id":"314b8592efe0_2","name":"dab5","type":"H4","href":null,"layout":null,"metadata":null,"text":"The easy way to work with classes not seen at train time","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:0*Qu0mmtNKS2OjKTud":{"__typename":"ImageMetadata","id":"0*Qu0mmtNKS2OjKTud","originalHeight":2667,"originalWidth":4000,"focusPercentX":null,"focusPercentY":null,"alt":"A very small siamese kitten staring at a laptop"},"Paragraph:314b8592efe0_3":{"__typename":"Paragraph","id":"314b8592efe0_3","name":"172a","type":"IMG","href":null,"layout":"OUTSET_CENTER","metadata":{"__ref":"ImageMetadata:0*Qu0mmtNKS2OjKTud"},"text":"Photo by Sereja Ris on Unsplash","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":9,"end":19,"href":"https:\u002F\u002Funsplash.com\u002F@serejaris?utm_source=medium&utm_medium=referral","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":23,"end":31,"href":"https:\u002F\u002Funsplash.com?utm_source=medium&utm_medium=referral","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_4":{"__typename":"Paragraph","id":"314b8592efe0_4","name":"aff4","type":"P","href":null,"layout":null,"metadata":null,"text":"When you first start out with machine learning, it becomes clear from the offset that massive amounts of data are required for accurate and robust model training. And whilst this is true, when training models for purposes where a custom dataset is required you often need to compromise on the level of data your model sees.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_5":{"__typename":"Paragraph","id":"314b8592efe0_5","name":"218a","type":"P","href":null,"layout":null,"metadata":null,"text":"This was the case for myself; working in conservation tech, any models we deploy to an area are built using data collected from previous years’ surveys, which in some cases may be sparse (certainly nowhere near the levels of benchmarking datasets such as ImageNet [1]). To make matters worse, working in conservation tech means working with open-ended datasets. Because the animals we work with are free roaming, there is no guarantee that a dataset we use for model training will contain examples of everything the model will see in the field.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_6":{"__typename":"Paragraph","id":"314b8592efe0_6","name":"228d","type":"P","href":null,"layout":null,"metadata":null,"text":"This results in somewhat of an uphill battle when trying to deploy models using traditional machine learning approaches. Building a model for conservation is useless if you need thousands of examples for each class and you need to retrain your model each year as the classes change. But this problem isn’t confined to conservation, lots of areas outside of benchmarking have similar issues with amounts of data and rates of change.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_7":{"__typename":"Paragraph","id":"314b8592efe0_7","name":"b82b","type":"P","href":null,"layout":null,"metadata":null,"text":"In this article I will discuss a type of model known as a Siamese Neural Network. Hopefully after reading, you will have a better understanding of how this architecture can help not just in conservation, but in any area where data quantities are limited and rates of class change are fast.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_8":{"__typename":"Paragraph","id":"314b8592efe0_8","name":"6326","type":"H3","href":null,"layout":null,"metadata":null,"text":"Prerequisites","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_9":{"__typename":"Paragraph","id":"314b8592efe0_9","name":"3d65","type":"P","href":null,"layout":null,"metadata":null,"text":"Before getting started you should probably have an understanding of machine learning, specifically Convolutional Neural Networks. If you don’t, I found Sumit Saha’s post A Comprehensive Guide to Convolutional Neural Networks — the ELI5 way to be a good starting point in lieu of a formal education in the area. You should probably read that first.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":152,"end":162,"href":null,"anchorType":"USER","userId":"631ee5e6343e","linkMetadata":null},{"__typename":"Markup","type":"A","start":170,"end":239,"href":"https:\u002F\u002Ftowardsdatascience.com\u002Fa-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":170,"end":240,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_10":{"__typename":"Paragraph","id":"314b8592efe0_10","name":"d9f5","type":"P","href":null,"layout":null,"metadata":null,"text":"You should also be comfortable with Python, Keras, and TensorFlow. We will be working through examples of code during this article, as I find doing this gives a better understanding than just free form text on its own. All code in this guide was written in TensorFlow 1.14, but there is no reason why the code shouldn’t work in newer versions (possibly with a few modifications), or indeed ported to other deep learning frameworks such as PyTorch.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_11":{"__typename":"Paragraph","id":"314b8592efe0_11","name":"bec9","type":"H3","href":null,"layout":null,"metadata":null,"text":"What is a Siamese Neural Network?","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_12":{"__typename":"Paragraph","id":"314b8592efe0_12","name":"7aa0","type":"P","href":null,"layout":null,"metadata":null,"text":"In short, a Siamese Neural Network is any model architecture which contains at least two parallel, identical, Convolutional Neural Networks. We’ll call these SNNs and CNNs from now on. This parallel CNN architecture allows for the model to learn similarity, which can be used instead of a direct classification. SNNs have found uptake primarily for image data, such as in facial recognition, although they do have their uses outside of this domain. For example, Selen Uguroglu gave a great talk at NeurIPS 2020 about how Netflix utilises SNNs to generate user recommendations based on film metadata. For this guide, we will focus on image data.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":517,"end":575,"href":"https:\u002F\u002Fslideslive.com\u002F38943514\u002Fsimilarity-at-netflix","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":246,"end":256,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_13":{"__typename":"Paragraph","id":"314b8592efe0_13","name":"be34","type":"P","href":null,"layout":null,"metadata":null,"text":"Each parallel CNN which forms a part of the SNN is designed to produce an embedding, or a reduced dimensional representation, of the input. For example, if we specify an embedding size of 10 we may input a high dimensional image of size width * height * channels and receive as output a float value vector of size 10 which directly represents the image.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":237,"end":242,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":245,"end":251,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":254,"end":262,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_14":{"__typename":"Paragraph","id":"314b8592efe0_14","name":"c323","type":"P","href":null,"layout":null,"metadata":null,"text":"These embeddings can then be used to optimise a Ranking Loss, and at test time used to generate a similarity score. The parallel CNNs can, in theory, take any form. One important point however is that they must be completely identical; they must share the same architecture, share the same initial and updated weights, and have the same hyperparameters. This consistency allows the model to compare the inputs it receives, usually one per CNN branch. The SigNet paper from Dey et al. [2] provides an excellent visualisation of this, which can be seen below.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":477,"end":484,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*IZbLdsdYRISf_86991WqXQ.png":{"__typename":"ImageMetadata","id":"1*IZbLdsdYRISf_86991WqXQ.png","originalHeight":483,"originalWidth":720,"focusPercentX":null,"focusPercentY":null,"alt":"The signet architecture from Dey et al. [1]. Two signatures enter two identical CNNs, which produce an output sent to a loss."},"Paragraph:314b8592efe0_15":{"__typename":"Paragraph","id":"314b8592efe0_15","name":"051e","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*IZbLdsdYRISf_86991WqXQ.png"},"text":"The SigNet architecture. Image from Dey et al. [2].","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":40,"end":51,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_16":{"__typename":"Paragraph","id":"314b8592efe0_16","name":"297a","type":"P","href":null,"layout":null,"metadata":null,"text":"The goal of SigNet is to determine if a given signature is genuine or a forgery. This can be achieved through the use of two parallel CNNs, trained on genuine and forged signature pairs. Each signature is fed through one branch of the SNN which generates a d-dimensional embedding for the image. It is these embeddings which are used to optimise a loss function rather than the images themselves. More recent versions of SNNs will most likely utilise triple or even quadruple branching, containing three or four parallel CNNs respectively.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":257,"end":259,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_17":{"__typename":"Paragraph","id":"314b8592efe0_17","name":"bdc7","type":"H3","href":null,"layout":null,"metadata":null,"text":"What’s the Point of SNNs?","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_18":{"__typename":"Paragraph","id":"314b8592efe0_18","name":"61b6","type":"P","href":null,"layout":null,"metadata":null,"text":"Now that we understand the make-up of an SNN, we can highlight their value. Using the generated d-dimensional embeddings, we can create some d-dimensional hyperspace that allows the embeddings to be plotted creating clusters. This hyperspace can then be projected down to 2-dimensions for plotting using Principle Component Analysis, or PCA.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":96,"end":98,"href":"","anchorType":"LINK","userId":"","linkMetadata":null},{"__typename":"Markup","type":"EM","start":141,"end":143,"href":"","anchorType":"LINK","userId":"","linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*76eeE-SqxFMP-rtdD76KEw.png":{"__typename":"ImageMetadata","id":"1*76eeE-SqxFMP-rtdD76KEw.png","originalHeight":405,"originalWidth":400,"focusPercentX":null,"focusPercentY":null,"alt":"A graph showing multiple dots, each coloured 1 of 10 colours. Dots of the same colour are generally clustered together."},"Paragraph:314b8592efe0_19":{"__typename":"Paragraph","id":"314b8592efe0_19","name":"f7e0","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*76eeE-SqxFMP-rtdD76KEw.png"},"text":"A plot of an embedding hyperspace after 100 training epochs, projected down to 2-dimensions using PCA. Image generated using code based on this notebook.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":139,"end":152,"href":"https:\u002F\u002Fgithub.com\u002FAdrianUng\u002Fkeras-triplet-loss-mnist\u002Fblob\u002Fmaster\u002FTriplet_loss_KERAS_semi_hard_from_TF.ipynb","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_20":{"__typename":"Paragraph","id":"314b8592efe0_20","name":"de84","type":"P","href":null,"layout":null,"metadata":null,"text":"This plot shows the embedding locations for a subset of the MNIST test data [3]. Here, a model has been trained to generate embeddings of images for the 10 unique classes (images of handwritten digits between 0 and 9). Notice how, even after only 100 training epochs, the model is starting to generate similar embeddings for images of the same class. This can be seen by the clusterings of dots of the same colour in the graph above — some clusters in the plot are visualised on top of each other, this is due to the reduction down to 2-d through PCA. Other visualisations such as t-SNE plots, or reducing to a higher number of dimensions, can help in this situation.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_21":{"__typename":"Paragraph","id":"314b8592efe0_21","name":"d035","type":"P","href":null,"layout":null,"metadata":null,"text":"It’s this embedding clustering that makes SNNs such a powerful tool. If we suddenly decided that we wanted to add another class to the data, then there is no need to retrain the model. The new class embeddings should be generated in such a way that, when plotted into the hyperspace, they are far away from the existing clusters, but cluster together with other examples of the new class as they are added. By using this embedding similarity, we can begin to produce likely classifications for both seen and unseen classes using very little data.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_22":{"__typename":"Paragraph","id":"314b8592efe0_22","name":"24fd","type":"H3","href":null,"layout":null,"metadata":null,"text":"Model Training","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_23":{"__typename":"Paragraph","id":"314b8592efe0_23","name":"b7f2","type":"P","href":null,"layout":null,"metadata":null,"text":"Previously, I mentioned that SNNs consist of at least two parallel CNN branches, but modern implementations often rely on more. The number of branches in your SNN has a big influence on your model training. Not only do you need to ensure that your data is fed to the SNN in such a way that each branch receives training examples, but your choice of loss function must also take the number of branches into account. Regardless of the number of branches chosen, the type of loss function will likely stay consistent.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":45,"end":58,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":464,"end":468,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_24":{"__typename":"Paragraph","id":"314b8592efe0_24","name":"68af","type":"P","href":null,"layout":null,"metadata":null,"text":"Ranking Losses, also known as Contrastive Losses, aim to predict relative distances between model inputs when projected onto a hyperspace. This is in comparison to more traditional losses which aim to predict some set of class labels. Ranking Losses play an important role in SNNs, although they are also useful for other tasks such as Natural Language Processing. There are many different types of Ranking Loss, but they all work (generally) in the same way.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_25":{"__typename":"Paragraph","id":"314b8592efe0_25","name":"700d","type":"P","href":null,"layout":null,"metadata":null,"text":"Let’s assume we have two inputs, and we want to know how similar they are. Using a Ranking Loss, we would perform the following steps:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_26":{"__typename":"Paragraph","id":"314b8592efe0_26","name":"6eb1","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Extract the features from the input.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_27":{"__typename":"Paragraph","id":"314b8592efe0_27","name":"c5ff","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Embed the extracted features onto a d-dimensional hyperspace.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":36,"end":38,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_28":{"__typename":"Paragraph","id":"314b8592efe0_28","name":"ceae","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Calculate the distance between the embeddings (e.g. using Euclidean distance) to be used as a measure of similarity.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_29":{"__typename":"Paragraph","id":"314b8592efe0_29","name":"a138","type":"P","href":null,"layout":null,"metadata":null,"text":"It’s important to note here that we often don’t particularly care about the values of the embeddings, just the distances between them. Taking the plot of the embeddings shown earlier, notice all points lie between about -1.5, 2.0 on the x-axis and about -2.0, 2.0 on the y-axis. There is nothing inherently good or bad about a model that embeds within this range, all that matters is the points are clustering in their respective classes.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_30":{"__typename":"Paragraph","id":"314b8592efe0_30","name":"0164","type":"H4","href":null,"layout":null,"metadata":null,"text":"Triplet Ranking Loss","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_31":{"__typename":"Paragraph","id":"314b8592efe0_31","name":"71f7","type":"P","href":null,"layout":null,"metadata":null,"text":"One of the more common types of Ranking Loss used for SNNs is Triplet Ranking Loss. You’ll often see SNNs using this loss function called Triplet Networks as if they are their own thing (and indeed this is how they are defined by Hoffer et al. in the paper that first conceived them [4]) but really they’re just an SNN with three branches. Because Triplet Loss is so commonplace now, and it’s the loss function we’ll be using later in this post, it’s important to understand how it works.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":237,"end":238,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":240,"end":244,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*bJABur9wzFNACosQkim8kw.png":{"__typename":"ImageMetadata","id":"1*bJABur9wzFNACosQkim8kw.png","originalHeight":479,"originalWidth":903,"focusPercentX":47,"focusPercentY":49,"alt":"Left: 3 input imgs labelled anchor, positive, negative. Centre: An SNN embedding the imgs. Right: The images embedded, arrows showing positive and negative are pushing away, positive and centre are getting closer."},"Paragraph:314b8592efe0_32":{"__typename":"Paragraph","id":"314b8592efe0_32","name":"d79a","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*bJABur9wzFNACosQkim8kw.png"},"text":"An example showing how triplet ranking loss works to pull embedded images of the same class closer together, and different classes further apart. Image by author.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_33":{"__typename":"Paragraph","id":"314b8592efe0_33","name":"b32c","type":"P","href":null,"layout":null,"metadata":null,"text":"Triplet Ranking Loss requires, as the name suggests, three inputs which we call a triplet. Each data-point in the triplet has its own job. The Anchor is data of some class C which defines which class the triplet will train the model on. The Positive is another example of the class C. The Negative is a data-point of some class which is not C. At train time, each of our triplet components is fed to its own CNN branch to be embedded. These embeddings are passed to the Triplet Loss Function, which is defined as:","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":143,"end":150,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":241,"end":249,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":289,"end":298,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":337,"end":340,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*dPVJTB9t6uRr_-ij_GIFug.png":{"__typename":"ImageMetadata","id":"1*dPVJTB9t6uRr_-ij_GIFug.png","originalHeight":24,"originalWidth":400,"focusPercentX":null,"focusPercentY":null,"alt":"L equals the max of 0 or D(A,P) — D(A,N) + margin"},"Paragraph:314b8592efe0_34":{"__typename":"Paragraph","id":"314b8592efe0_34","name":"a5cc","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*dPVJTB9t6uRr_-ij_GIFug.png"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_35":{"__typename":"Paragraph","id":"314b8592efe0_35","name":"8302","type":"P","href":null,"layout":null,"metadata":null,"text":"Where D(A,P) is the embedding distance between the Anchor and the Positive, and D(A,N) is the embedding distance between the Anchor and the Negative. We also define some margin - an often used initial value for this is 0.2, the margin used in FaceNet [5].","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":6,"end":13,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":80,"end":87,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_36":{"__typename":"Paragraph","id":"314b8592efe0_36","name":"9852","type":"P","href":null,"layout":null,"metadata":null,"text":"The purpose of this function is to minimise the distance between the Anchor and the Positive, whilst maximising the distance between the Anchor and the Negative. For a more in-depth look at Triplet Ranking Loss, I’d suggest this excellent post from Raúl Gómez.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":224,"end":243,"href":"https:\u002F\u002Fgombru.github.io\u002F2019\u002F04\u002F03\u002Franking_loss\u002F","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_37":{"__typename":"Paragraph","id":"314b8592efe0_37","name":"6daa","type":"H4","href":null,"layout":null,"metadata":null,"text":"Semi-Hard Triplet Mining","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_38":{"__typename":"Paragraph","id":"314b8592efe0_38","name":"4078","type":"P","href":null,"layout":null,"metadata":null,"text":"Because of the importance of the triplet components, it is imperative that our SNN is provided only with triplets which will enable it to learn. More specifically, we want to provide Negatives such that our triplets allow the model to learn, but not be so difficult that learning takes too long.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_39":{"__typename":"Paragraph","id":"314b8592efe0_39","name":"a8d7","type":"P","href":null,"layout":null,"metadata":null,"text":"An easy way to do this is through a process known as Semi-Hard Triplet Mining. To perform this, we first define three categories of triplet:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_40":{"__typename":"Paragraph","id":"314b8592efe0_40","name":"9e7e","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Easy Triplets are those where D(A,P) + margin \u003C D(A,N), thus L = 0.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":13,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":30,"end":54,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":61,"end":67,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_41":{"__typename":"Paragraph","id":"314b8592efe0_41","name":"d611","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Hard Triplets are those where D(A,N) \u003C D(A,P).","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":13,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":30,"end":46,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_42":{"__typename":"Paragraph","id":"314b8592efe0_42","name":"6647","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Semi-Hard Triplets are those where D(A,P) \u003C D(A,N) \u003C D(A,P) + margin.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":19,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":35,"end":69,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_43":{"__typename":"Paragraph","id":"314b8592efe0_43","name":"a135","type":"P","href":null,"layout":null,"metadata":null,"text":"The goal is to find as many Semi-Hard Triplets as possible. These triplets have a positive loss, but the Positive embedding distance is closer to the Anchor embedding than the Negative. This allows for fast training, but is still difficult enough for the model to actually learn something during training.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_44":{"__typename":"Paragraph","id":"314b8592efe0_44","name":"6c27","type":"P","href":null,"layout":null,"metadata":null,"text":"Finding these Semi-Hard triplets can be performed in one of two ways. In Offline mining, the entire dataset is converted into triplets before training. In Online mining, batches of data are fed in, with random triplets generated on the fly.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":73,"end":81,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":155,"end":161,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_45":{"__typename":"Paragraph","id":"314b8592efe0_45","name":"b9bd","type":"P","href":null,"layout":null,"metadata":null,"text":"As a general rule of thumb, Online mining should be performed wherever possible as it allows for much faster training due to the ability to constantly update our threshold definition of a Semi-Hard Triplet as training progresses. This can be supplemented with data augmentation, which can also be performed in an Online fashion.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_46":{"__typename":"Paragraph","id":"314b8592efe0_46","name":"3ccd","type":"H3","href":null,"layout":null,"metadata":null,"text":"Using SNNs at Inference Time","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_47":{"__typename":"Paragraph","id":"314b8592efe0_47","name":"e2b5","type":"P","href":null,"layout":null,"metadata":null,"text":"Now that we understand how SNNs are trained, we next need to understand how they can be used at inference time. During training we used all of the branches of the SNN, whereas inference can be performed using a single CNN branch.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_48":{"__typename":"Paragraph","id":"314b8592efe0_48","name":"85a9","type":"P","href":null,"layout":null,"metadata":null,"text":"At inference time, the input image of an unknown class is processed by the CNN branch and has its features embedded. This embedding is then plotted onto the hyperspace and compared with the other clusters. This provides us with a list of similarity scores, or relative distances between the image of unknown class and all of the existing clusters. The clusters we compare our input image against are known as the support set. Let’s take a look at an example to help understand this.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":413,"end":424,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*_Ue4uUtayX2vW600HjGbyg.png":{"__typename":"ImageMetadata","id":"1*_Ue4uUtayX2vW600HjGbyg.png","originalHeight":826,"originalWidth":714,"focusPercentX":null,"focusPercentY":null,"alt":"L: image of a moth labelled ‘Test Image’. M: Multiple moths labelled ‘Support Set’. R: The first Support Set image on its own"},"Paragraph:314b8592efe0_49":{"__typename":"Paragraph","id":"314b8592efe0_49","name":"8bc1","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*_Ue4uUtayX2vW600HjGbyg.png"},"text":"Finding the most likely family class for the test image moth, based on data from Vetrova et al. [6]. Image generated using code based on this notebook.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":137,"end":150,"href":"https:\u002F\u002Fgithub.com\u002Fasagar60\u002FOne-Shot-Learning\u002Fblob\u002Fmaster\u002FOmniglot_data\u002FOne_shot_implementation.ipynb","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":89,"end":101,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_50":{"__typename":"Paragraph","id":"314b8592efe0_50","name":"ab0b","type":"P","href":null,"layout":null,"metadata":null,"text":"The plot above is the output of an SNN I created to determine the scientific family of moths. Each image in the dataset, adapted from Vetrova et al. [6], was labelled with one of four scientific family names or labelled as ‘larvae’, giving a total of five classes. For ease of visualisation (although in hindsight not necessarily ease of understanding) each of the known labelled classes is displayed in the support set, shown in the middle of the plot above, using a random example image from each class. On the left of the plot is a test image; this is a moth image unseen by the SNN, which is now tasked with determining the scientific family.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":142,"end":149,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_51":{"__typename":"Paragraph","id":"314b8592efe0_51","name":"4d14","type":"P","href":null,"layout":null,"metadata":null,"text":"First, the SNN embeds the test image using the embedding function learned during training. Next, it compares this embedding with the support set embeddings, which provides a most likely moth family for the test image. To the right of the plot, we can see the first image in the support set has been printed again.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_52":{"__typename":"Paragraph","id":"314b8592efe0_52","name":"838f","type":"P","href":null,"layout":null,"metadata":null,"text":"The code used to generate the plot above was told to show the corresponding example of the test image’s family first (the plotting code knows the correct class, the SNN does not). Because the first support set image is shown again on the right of the plot, this tells us that the SNN was correct in determining the scientific family of the test image moth! If this plot is a bit confusing to you, don’t worry as we’ll be working through creating the same plot on different, simpler, data later.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_53":{"__typename":"Paragraph","id":"314b8592efe0_53","name":"1dda","type":"P","href":null,"layout":null,"metadata":null,"text":"This code could be extended further to alert users if an embedding is placed in a new area of the hyperspace if if exceeds some predefined class distance threshold. This could be an indication that a new moth family has been seen by the SNN for the first time.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_54":{"__typename":"Paragraph","id":"314b8592efe0_54","name":"ccb6","type":"H3","href":null,"layout":null,"metadata":null,"text":"Where Do We Measure From?","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_55":{"__typename":"Paragraph","id":"314b8592efe0_55","name":"d6c1","type":"P","href":null,"layout":null,"metadata":null,"text":"In order to determine the distance between the test image and the classes in the support set, we need a location for each class to measure from. At first glance, it might seem okay to use a randomly selected embedding from each support set class; after all, if all embeddings are perfectly clustered surely it doesn’t matter which one we use?","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_56":{"__typename":"Paragraph","id":"314b8592efe0_56","name":"91d6","type":"P","href":null,"layout":null,"metadata":null,"text":"Whilst this assumption certainly holds if our class embeddings are perfectly clustered, in a real world system this won’t be the case. Let’s examine the toy example below.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":39,"end":42,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*fruv2ApgYYJy_0c27wHiaQ.png":{"__typename":"ImageMetadata","id":"1*fruv2ApgYYJy_0c27wHiaQ.png","originalHeight":405,"originalWidth":720,"focusPercentX":null,"focusPercentY":null,"alt":"Group of crosses top left, group of squares bottom right, 1 circled. 1 cross in square group, circled. Triangle above squares"},"Paragraph:314b8592efe0_57":{"__typename":"Paragraph","id":"314b8592efe0_57","name":"5dd9","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*fruv2ApgYYJy_0c27wHiaQ.png"},"text":"An example embedding space with two classes, crosses and squares, and a yet undetermined class embedding represented by a triangle. Image by author.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_58":{"__typename":"Paragraph","id":"314b8592efe0_58","name":"16f8","type":"P","href":null,"layout":null,"metadata":null,"text":"In this example we have a two class embedding space, one for crosses and one for squares. All of the square class embeddings are clustered to the right of the plot, however the class of crosses has one embedding which has not been clustered with the others in the top left. This erroneous cross has been embedded into the space where the squares usually cluster. There is also a triangle plotted in the top-right, this is the current test image, embedded into the space but not yet assigned to a class based on its distance to the other clusters.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_59":{"__typename":"Paragraph","id":"314b8592efe0_59","name":"28ad","type":"P","href":null,"layout":null,"metadata":null,"text":"In order to determine if the triangle should actually be a cross or a square, we randomly select an embedding to measure from for each class; the erroneous cross and the bottom-left square are chosen (both circled). If we compare the distances from these embeddings, the chosen cross is closest, so the triangle would be labelled a cross.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_60":{"__typename":"Paragraph","id":"314b8592efe0_60","name":"6fe2","type":"P","href":null,"layout":null,"metadata":null,"text":"However looking at the plot as a whole, it’s clear that the triangle should probably be labelled as a square, and the cross is an outlier. By selecting random embeddings to measure from we run the risk of having outliers skew the distance measurement, and thus the final outcome.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_61":{"__typename":"Paragraph","id":"314b8592efe0_61","name":"29b5","type":"P","href":null,"layout":null,"metadata":null,"text":"This can be solved using prototypes, an elegant and easy to understand solution to our problem. Prototypes are essentially generalised embeddings for each class, reducing the effect of outliers on the distance measurements. These can be calculated in a variety of ways, but simple techniques such as taking the median work well. Let’s update the toy example…","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":25,"end":35,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*Dfi3oss8UuI627yajuXw6Q.png":{"__typename":"ImageMetadata","id":"1*Dfi3oss8UuI627yajuXw6Q.png","originalHeight":405,"originalWidth":720,"focusPercentX":null,"focusPercentY":null,"alt":"Same plot as above, but now there are two ‘P’s in the middle of each cluster, which have been circled."},"Paragraph:314b8592efe0_62":{"__typename":"Paragraph","id":"314b8592efe0_62","name":"8cc1","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*Dfi3oss8UuI627yajuXw6Q.png"},"text":"The same embedding space as previous, but with the inclusion of prototypes. Image by author.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_63":{"__typename":"Paragraph","id":"314b8592efe0_63","name":"6877","type":"P","href":null,"layout":null,"metadata":null,"text":"Now, each class has been given a prototype near the centre of its cluster (e.g. Pₓ is the prototype for the cross class). If we select the prototypes when measuring similarity, our triangle is correctly labelled as a square. This simple solution can greatly reduce the effect outliers have when calculating similarities.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_64":{"__typename":"Paragraph","id":"314b8592efe0_64","name":"ad4c","type":"P","href":null,"layout":null,"metadata":null,"text":"Determining how prototypes should be calculated is difficult, and solutions such as using the median may break down with certain datasets. For example, if all of our cross class examples formed a circle of radius 1 around the origin and the square class examples formed a circle of radius 2, the prototypes would both now be formed at the origin, resulting in equal distance measurements. We’d need to find another way to calculate the prototypes for that dataset.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_65":{"__typename":"Paragraph","id":"314b8592efe0_65","name":"2370","type":"H3","href":null,"layout":null,"metadata":null,"text":"Building a Siamese Neural Network","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_66":{"__typename":"Paragraph","id":"314b8592efe0_66","name":"0368","type":"P","href":null,"layout":null,"metadata":null,"text":"Now that we have a grasp of the underlying theory of SNNs and why they are an important tool, let’s take a look at how we build one. As mentioned previously, we’ll be using Python, Keras, and TensorFlow 1.14 for this although there’s really nothing preventing this code being converted for use in another framework like PyTorch; I use TensorFlow out of personal preference rather than because it’s better for making SNNs. We’re also going to stick with using MNIST as our dataset, both for consistency and for ease of training.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_67":{"__typename":"Paragraph","id":"314b8592efe0_67","name":"6391","type":"P","href":null,"layout":null,"metadata":null,"text":"The code here is based on a variety of sources, which I will link as we go, but the underlying construction is based on the approach described in Amit Yadav’s Coursera, which is itself based on FaceNet [5].","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":159,"end":168,"href":"https:\u002F\u002Fwww.coursera.org\u002Fprojects\u002Fsiamese-network-triplet-loss-keras","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_68":{"__typename":"Paragraph","id":"314b8592efe0_68","name":"6bc6","type":"P","href":null,"layout":null,"metadata":null,"text":"If you prefer to have full code rather than snippets, this is available from my Github.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":80,"end":86,"href":"https:\u002F\u002Fgithub.com\u002FTrotts\u002FSiamese-Neural-Network-MNIST-Triplet-Loss","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_69":{"__typename":"Paragraph","id":"314b8592efe0_69","name":"d712","type":"H4","href":null,"layout":null,"metadata":null,"text":"Step 1: Importing packages","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_70":{"__typename":"Paragraph","id":"314b8592efe0_70","name":"c87b","type":"P","href":null,"layout":null,"metadata":null,"text":"First, we’re going to need to import the required packages. For a complete list of package versions used on the virtual machine to run this code, see here. I tested this code with Python 3.6.7.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":150,"end":154,"href":"https:\u002F\u002Fgithub.com\u002FTrotts\u002FSiamese-Neural-Network-MNIST-Triplet-Loss\u002Fblob\u002Fmain\u002Frequirements.txt","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"MediaResource:3e3adf2601ed6d99779643fd97bde3bb":{"__typename":"MediaResource","id":"3e3adf2601ed6d99779643fd97bde3bb","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"SNN-MNIST-Imports"},"Paragraph:314b8592efe0_71":{"__typename":"Paragraph","id":"314b8592efe0_71","name":"8609","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:3e3adf2601ed6d99779643fd97bde3bb"}},"mixtapeMetadata":null},"Paragraph:314b8592efe0_72":{"__typename":"Paragraph","id":"314b8592efe0_72","name":"6ff1","type":"H4","href":null,"layout":null,"metadata":null,"text":"Step 2: Importing data","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_73":{"__typename":"Paragraph","id":"314b8592efe0_73","name":"41b6","type":"P","href":null,"layout":null,"metadata":null,"text":"Next, we need to import a dataset for our SNN to work with. As previously mentioned we’ll be using MNIST, which can be loaded using TensorFlow’s mnist.load_data().","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":145,"end":162,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_74":{"__typename":"Paragraph","id":"314b8592efe0_74","name":"f7d8","type":"P","href":null,"layout":null,"metadata":null,"text":"After the data is loaded in, it is reshaped and flattened. This allows the data to be read into the SNN more easily.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"MediaResource:7411ed47a26e21c9aab7a63a6ca3c9d9":{"__typename":"MediaResource","id":"7411ed47a26e21c9aab7a63a6ca3c9d9","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"SNN-MNIST-data"},"Paragraph:314b8592efe0_75":{"__typename":"Paragraph","id":"314b8592efe0_75","name":"463d","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:7411ed47a26e21c9aab7a63a6ca3c9d9"}},"mixtapeMetadata":null},"Paragraph:314b8592efe0_76":{"__typename":"Paragraph","id":"314b8592efe0_76","name":"42e7","type":"P","href":null,"layout":null,"metadata":null,"text":"Note that we only have a height and width here as MNIST is greyscale, therefore only has 1 colour channel. If we had a dataset with multiple colour channels we would need to adapt our code for this, for example using x_train_w_h_c instead.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":217,"end":230,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_77":{"__typename":"Paragraph","id":"314b8592efe0_77","name":"9ff2","type":"H4","href":null,"layout":null,"metadata":null,"text":"Step 3: Create the triplets","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_78":{"__typename":"Paragraph","id":"314b8592efe0_78","name":"0bed","type":"P","href":null,"layout":null,"metadata":null,"text":"Now we need to create our MNIST triplets. Two methods are required for this.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_79":{"__typename":"Paragraph","id":"314b8592efe0_79","name":"b6b8","type":"P","href":null,"layout":null,"metadata":null,"text":"The first, create_batch(), generates triplets by randomly selecting two class labels, one for the Anchor\u002FPositive and one for the Negative, before randomly selecting a class example for each.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":11,"end":25,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"MediaResource:2158d59d5c4965682ef38390f6c1beed":{"__typename":"MediaResource","id":"2158d59d5c4965682ef38390f6c1beed","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"SNN-MNIST-create-batch"},"Paragraph:314b8592efe0_80":{"__typename":"Paragraph","id":"314b8592efe0_80","name":"21c1","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:2158d59d5c4965682ef38390f6c1beed"}},"mixtapeMetadata":null},"Paragraph:314b8592efe0_81":{"__typename":"Paragraph","id":"314b8592efe0_81","name":"dc73","type":"P","href":null,"layout":null,"metadata":null,"text":"The second, create_hard_batch(), creates a batch of random triplets using create_batch(), and embeds them using the current SNN. This allows us to determine which triplets in the batch are Semi-Hard; if they are we keep num_hard of them, populating the rest of the batch with other random triplets. By padding with random triplets, we allow for training to begin as well as ensure our batches are of a consistent size.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":12,"end":31,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":74,"end":88,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":220,"end":228,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"MediaResource:1839c74eb4528d927ae92043ff0a8cfe":{"__typename":"MediaResource","id":"1839c74eb4528d927ae92043ff0a8cfe","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"SNN-MNIST-create-hard-batches"},"Paragraph:314b8592efe0_82":{"__typename":"Paragraph","id":"314b8592efe0_82","name":"6261","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:1839c74eb4528d927ae92043ff0a8cfe"}},"mixtapeMetadata":null},"Paragraph:314b8592efe0_83":{"__typename":"Paragraph","id":"314b8592efe0_83","name":"4f7a","type":"H4","href":null,"layout":null,"metadata":null,"text":"Step 4: Defining the SNN","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_84":{"__typename":"Paragraph","id":"314b8592efe0_84","name":"ef5a","type":"P","href":null,"layout":null,"metadata":null,"text":"The SNN is defined in two parts. First, we must create the embedding model. This model receives an input image and generates a d-dimensional embedding. We create a very shallow embedding model here, but more complex models can be created.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":127,"end":128,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"MediaResource:2f86fa8fc8994d401c1108ffef515bc6":{"__typename":"MediaResource","id":"2f86fa8fc8994d401c1108ffef515bc6","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"SNN-MNIST-embedding-model"},"Paragraph:314b8592efe0_85":{"__typename":"Paragraph","id":"314b8592efe0_85","name":"91af","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:2f86fa8fc8994d401c1108ffef515bc6"}},"mixtapeMetadata":null},"Paragraph:314b8592efe0_86":{"__typename":"Paragraph","id":"314b8592efe0_86","name":"ca48","type":"P","href":null,"layout":null,"metadata":null,"text":"Next, we create a model which receives a triplet, passes it to the embedding model sequentially for embedding, then passes the resultant embeddings to the triplet loss function.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"MediaResource:83d3674b80b8083ddda1620fd05ca6f2":{"__typename":"MediaResource","id":"83d3674b80b8083ddda1620fd05ca6f2","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"SNN-MNIST-SNN-model"},"Paragraph:314b8592efe0_87":{"__typename":"Paragraph","id":"314b8592efe0_87","name":"4450","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:83d3674b80b8083ddda1620fd05ca6f2"}},"mixtapeMetadata":null},"Paragraph:314b8592efe0_88":{"__typename":"Paragraph","id":"314b8592efe0_88","name":"7b92","type":"H4","href":null,"layout":null,"metadata":null,"text":"Step 5: Defining the triplet loss function","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_89":{"__typename":"Paragraph","id":"314b8592efe0_89","name":"40de","type":"P","href":null,"layout":null,"metadata":null,"text":"In order for the SNN to train using the triplets, we need to define the triplet loss function. This mirrors the triplet loss function equation shown previously.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"MediaResource:fd465575ed398b6dbd6212e9f33899c0":{"__typename":"MediaResource","id":"fd465575ed398b6dbd6212e9f33899c0","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"SNN-MNIST-triplet-loss-function"},"Paragraph:314b8592efe0_90":{"__typename":"Paragraph","id":"314b8592efe0_90","name":"8f65","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:fd465575ed398b6dbd6212e9f33899c0"}},"mixtapeMetadata":null},"Paragraph:314b8592efe0_91":{"__typename":"Paragraph","id":"314b8592efe0_91","name":"9b9e","type":"H4","href":null,"layout":null,"metadata":null,"text":"Step 6: Defining the data generator","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_92":{"__typename":"Paragraph","id":"314b8592efe0_92","name":"e750","type":"P","href":null,"layout":null,"metadata":null,"text":"In order to pass our triplets to the network, we need to create a data generator function. Both an x and y is required here by TensorFlow, but we don’t need a y value, so we pass a filler.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":99,"end":100,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":105,"end":106,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":159,"end":160,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"MediaResource:b357d45913d2b2a3d1d76ee8ef9a113f":{"__typename":"MediaResource","id":"b357d45913d2b2a3d1d76ee8ef9a113f","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"SNN-MNIST-data-generator"},"Paragraph:314b8592efe0_93":{"__typename":"Paragraph","id":"314b8592efe0_93","name":"95f0","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:b357d45913d2b2a3d1d76ee8ef9a113f"}},"mixtapeMetadata":null},"Paragraph:314b8592efe0_94":{"__typename":"Paragraph","id":"314b8592efe0_94","name":"7cbf","type":"H4","href":null,"layout":null,"metadata":null,"text":"Step 7: Setting up for training and evaluation","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_95":{"__typename":"Paragraph","id":"314b8592efe0_95","name":"c2e6","type":"P","href":null,"layout":null,"metadata":null,"text":"Now that we have defined the basics of the SNN, we can set up the model for training. First, we define our hyperparameters. Next, we create and compile the models. I specify that this is performed using the CPU, but this may not be required depending on your setup.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_96":{"__typename":"Paragraph","id":"314b8592efe0_96","name":"db63","type":"P","href":null,"layout":null,"metadata":null,"text":"Once the models are compiled, we store a subset of the test image embeddings. The model hasn’t been trained yet, so this gives us a good baseline to show how the embeddings have changed through the training process. Embedding visualisations via PCA are based on this notebook by AdrianUng.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":262,"end":275,"href":"https:\u002F\u002Fgithub.com\u002FAdrianUng\u002Fkeras-triplet-loss-mnist\u002Fblob\u002Fmaster\u002FTriplet_loss_KERAS_semi_hard_from_TF.ipynb","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"MediaResource:f3059b141056bfed853ca8986e7d45ae":{"__typename":"MediaResource","id":"f3059b141056bfed853ca8986e7d45ae","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"SNN-MNIST-setup"},"Paragraph:314b8592efe0_97":{"__typename":"Paragraph","id":"314b8592efe0_97","name":"360a","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:f3059b141056bfed853ca8986e7d45ae"}},"mixtapeMetadata":null},"Paragraph:314b8592efe0_98":{"__typename":"Paragraph","id":"314b8592efe0_98","name":"f687","type":"P","href":null,"layout":null,"metadata":null,"text":"Further evaluation can be performed on our SNN. Code used in this step is heavily influenced by Eric Craeymeersch’s One Shot Learning, Siamese Networks and Triplet Loss with Keras and this notebook from asagar60.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":96,"end":113,"href":null,"anchorType":"USER","userId":"d0c371fcaf92","linkMetadata":null},{"__typename":"Markup","type":"A","start":116,"end":179,"href":"https:\u002F\u002Fmedium.com\u002F@crimy\u002Fone-shot-learning-siamese-networks-and-triplet-loss-with-keras-2885ed022352","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":184,"end":197,"href":"https:\u002F\u002Fgithub.com\u002Fasagar60\u002FOne-Shot-Learning\u002Fblob\u002Fmaster\u002FOmniglot_data\u002FOne_shot_implementation.ipynb","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"MediaResource:86a795bd49c5d3093efba696a204f5c5":{"__typename":"MediaResource","id":"86a795bd49c5d3093efba696a204f5c5","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"SNN-MNIST-evaluation"},"Paragraph:314b8592efe0_99":{"__typename":"Paragraph","id":"314b8592efe0_99","name":"91c3","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:86a795bd49c5d3093efba696a204f5c5"}},"mixtapeMetadata":null},"Paragraph:314b8592efe0_100":{"__typename":"Paragraph","id":"314b8592efe0_100","name":"a2e0","type":"P","href":null,"layout":null,"metadata":null,"text":"Let’s take a look at the evaluation of the untrained model. From the plots, we can see our model is unable to distinguish between similar and dissimilar images. This is most pronounced in the third plot, highlighting the test images and their most likely classes, with very little difference between their scores.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*-WzkLsvSdq19xw-PItdlqA.png":{"__typename":"ImageMetadata","id":"1*-WzkLsvSdq19xw-PItdlqA.png","originalHeight":1063,"originalWidth":964,"focusPercentX":null,"focusPercentY":null,"alt":"1: an AUC plot showing an AUC @ 0.663, 2: boxplot showing embedding distances between classes, 3: Images and likely classes"},"Paragraph:314b8592efe0_101":{"__typename":"Paragraph","id":"314b8592efe0_101","name":"e16c","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*-WzkLsvSdq19xw-PItdlqA.png"},"text":"Image generated using code based on this notebook.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":36,"end":49,"href":"https:\u002F\u002Fmedium.com\u002F@crimy\u002Fone-shot-learning-siamese-networks-and-triplet-loss-with-keras-2885ed022352","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_102":{"__typename":"Paragraph","id":"314b8592efe0_102","name":"5476","type":"P","href":null,"layout":null,"metadata":null,"text":"Now we have compiled the models, we can also generate example random and Semi-Hard triplets. This code is based on a blog post by Ruochi Zang.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":117,"end":126,"href":"https:\u002F\u002Fzhangruochi.com\u002FCreate-a-Siamese-Network-with-Triplet-Loss-in-Keras\u002F2020\u002F08\u002F11\u002F","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"MediaResource:beabb9194e5ee08872786cd62f3463ec":{"__typename":"MediaResource","id":"beabb9194e5ee08872786cd62f3463ec","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"SNN-MNIST-example-triplets"},"Paragraph:314b8592efe0_103":{"__typename":"Paragraph","id":"314b8592efe0_103","name":"0ebb","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:beabb9194e5ee08872786cd62f3463ec"}},"mixtapeMetadata":null},"Paragraph:314b8592efe0_104":{"__typename":"Paragraph","id":"314b8592efe0_104","name":"8bf9","type":"P","href":null,"layout":null,"metadata":null,"text":"This produces the following:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*oDuvzAgmWtgaF3sBVyaPtQ.png":{"__typename":"ImageMetadata","id":"1*oDuvzAgmWtgaF3sBVyaPtQ.png","originalHeight":324,"originalWidth":379,"focusPercentX":null,"focusPercentY":null,"alt":"output of the above code showing an example random triplet (4, 4, 6) and a semi-hard triplet (8,8,6) all look similar."},"Paragraph:314b8592efe0_105":{"__typename":"Paragraph","id":"314b8592efe0_105","name":"fda4","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*oDuvzAgmWtgaF3sBVyaPtQ.png"},"text":"Image generated using code based on this blog post.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":36,"end":50,"href":"https:\u002F\u002Fzhangruochi.com\u002FCreate-a-Siamese-Network-with-Triplet-Loss-in-Keras\u002F2020\u002F08\u002F11\u002F","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_106":{"__typename":"Paragraph","id":"314b8592efe0_106","name":"a16b","type":"P","href":null,"layout":null,"metadata":null,"text":"Our example random triplet contains an Anchor and Positive of class 4, and a Negative of class 6. Our Semi-Hard triplet contains an Anchor and Positive of class 8, and a Negative of class 6, but note how similar they are in composition.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_107":{"__typename":"Paragraph","id":"314b8592efe0_107","name":"2239","type":"H4","href":null,"layout":null,"metadata":null,"text":"Step 8: Logging output from our model training","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_108":{"__typename":"Paragraph","id":"314b8592efe0_108","name":"c625","type":"P","href":null,"layout":null,"metadata":null,"text":"Let’s set up some logging and custom callbacks before we train our model, to help us if we need to come back at a later date. The Tensorboard logging callback is adapted from erenon’s helpful Stack Overflow answer, whilst the saving of the best model based on the validation loss is adapted from another Stack Overflow answer from OverLordGoldDragon.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":184,"end":214,"href":"https:\u002F\u002Fstackoverflow.com\u002Fquestions\u002F44861149\u002Fkeras-use-tensorboard-with-train-on-batch\u002F52581175#52581175","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":296,"end":325,"href":"https:\u002F\u002Fstackoverflow.com\u002Fquestions\u002F58103035\u002Fhow-can-we-perform-early-stopping-with-train-on-batch\u002F58103272#58103272","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"MediaResource:9f906792cafa7b670d35906a3be0d704":{"__typename":"MediaResource","id":"9f906792cafa7b670d35906a3be0d704","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"SNN-MNIST-Logging"},"Paragraph:314b8592efe0_109":{"__typename":"Paragraph","id":"314b8592efe0_109","name":"3979","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:9f906792cafa7b670d35906a3be0d704"}},"mixtapeMetadata":null},"Paragraph:314b8592efe0_110":{"__typename":"Paragraph","id":"314b8592efe0_110","name":"a155","type":"H4","href":null,"layout":null,"metadata":null,"text":"Step 9: Training the SNN","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_111":{"__typename":"Paragraph","id":"314b8592efe0_111","name":"6ec3","type":"P","href":null,"layout":null,"metadata":null,"text":"Now that all of our setup has been completed, it is time to start training! I first begin by selecting the total number of GPUs available, and parallelising the model training over them. You may need to amend this should you not have access to multiple GPUs.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"MediaResource:70528dd0f75986431dac4edf18a204ca":{"__typename":"MediaResource","id":"70528dd0f75986431dac4edf18a204ca","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"SNN-MNIST-training"},"Paragraph:314b8592efe0_112":{"__typename":"Paragraph","id":"314b8592efe0_112","name":"7e3f","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:70528dd0f75986431dac4edf18a204ca"}},"mixtapeMetadata":null},"Paragraph:314b8592efe0_113":{"__typename":"Paragraph","id":"314b8592efe0_113","name":"1bc3","type":"P","href":null,"layout":null,"metadata":null,"text":"Note that when running model.fit() we provide train and test data generators rather than the train and test data directly. This allows for online triplet mining to occur.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":23,"end":34,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_114":{"__typename":"Paragraph","id":"314b8592efe0_114","name":"9563","type":"H4","href":null,"layout":null,"metadata":null,"text":"Step 10: Evaluating the trained model","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_115":{"__typename":"Paragraph","id":"314b8592efe0_115","name":"df57","type":"P","href":null,"layout":null,"metadata":null,"text":"Once the model has trained, we can then evaluate it and compare its embeddings. First, we load in the trained model. I do this by reloading the saved logging files, but if you’re just running this all in one notebook as a closed system, there isn’t really a need to reload once the model is trained.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_116":{"__typename":"Paragraph","id":"314b8592efe0_116","name":"cff2","type":"P","href":null,"layout":null,"metadata":null,"text":"Once the models are loaded, we perform the same PCA decomposition we did on the untrained model to visualise how the embeddings have changed.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"MediaResource:79dbb9328e5d17254e9196d97b33e401":{"__typename":"MediaResource","id":"79dbb9328e5d17254e9196d97b33e401","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"SNN-MNIST-eval-trained"},"Paragraph:314b8592efe0_117":{"__typename":"Paragraph","id":"314b8592efe0_117","name":"f083","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:79dbb9328e5d17254e9196d97b33e401"}},"mixtapeMetadata":null},"Paragraph:314b8592efe0_118":{"__typename":"Paragraph","id":"314b8592efe0_118","name":"6744","type":"P","href":null,"layout":null,"metadata":null,"text":"At the end of the above code-block, we run evaluate() again, which produces the below graphs:","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":43,"end":53,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*PbgRcxPHbYyqxaNGTg845Q.png":{"__typename":"ImageMetadata","id":"1*PbgRcxPHbYyqxaNGTg845Q.png","originalHeight":1009,"originalWidth":964,"focusPercentX":null,"focusPercentY":null,"alt":"1: an AUC plot showing an AUC @ 0.985, 2: boxplot of embedding distances between classes, 3: Images and likely classes"},"Paragraph:314b8592efe0_119":{"__typename":"Paragraph","id":"314b8592efe0_119","name":"ea70","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*PbgRcxPHbYyqxaNGTg845Q.png"},"text":"Image generated using code based on this notebook.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":36,"end":49,"href":"https:\u002F\u002Fmedium.com\u002F@crimy\u002Fone-shot-learning-siamese-networks-and-triplet-loss-with-keras-2885ed022352","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_120":{"__typename":"Paragraph","id":"314b8592efe0_120","name":"3a6f","type":"P","href":null,"layout":null,"metadata":null,"text":"Note how the first plot now shows an AUC of 0.985 and increased distance between our classes. Interestingly, when looking at the test images and their most likely classes, we can see for the 2nd and 3rd test images the corresponding class has been correctly achieved (for example, taking the 2nd test image, of class 0, we can see the lowest score for all the support set classes is also at class 0), however looking at the 1st test image, all of the scores for the support set classes are very close, indicating the trained model has had difficulties classifying this image.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_121":{"__typename":"Paragraph","id":"314b8592efe0_121","name":"e788","type":"P","href":null,"layout":null,"metadata":null,"text":"To confirm our model has trained correctly and class clusters are now forming, lets plot the PCA decomposed embeddings we have stored previously.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"MediaResource:ab91b2bfce2f21d56ae313b86c2d7643":{"__typename":"MediaResource","id":"ab91b2bfce2f21d56ae313b86c2d7643","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"SNN-MNIST-embedding-vis"},"Paragraph:314b8592efe0_122":{"__typename":"Paragraph","id":"314b8592efe0_122","name":"6858","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:ab91b2bfce2f21d56ae313b86c2d7643"}},"mixtapeMetadata":null},"Paragraph:314b8592efe0_123":{"__typename":"Paragraph","id":"314b8592efe0_123","name":"6662","type":"P","href":null,"layout":null,"metadata":null,"text":"This code produces the following output:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*RAc1pg2Z03wMfzQf7tXK0Q.png":{"__typename":"ImageMetadata","id":"1*RAc1pg2Z03wMfzQf7tXK0Q.png","originalHeight":492,"originalWidth":969,"focusPercentX":null,"focusPercentY":null,"alt":"Left: data points before training, class examples are all mixed. Right: same data but after training, mostly clustered."},"Paragraph:314b8592efe0_124":{"__typename":"Paragraph","id":"314b8592efe0_124","name":"0edd","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*RAc1pg2Z03wMfzQf7tXK0Q.png"},"text":"Image generated using code based on this notebook.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":36,"end":49,"href":"https:\u002F\u002Fgithub.com\u002FAdrianUng\u002Fkeras-triplet-loss-mnist\u002Fblob\u002Fmaster\u002FTriplet_loss_KERAS_semi_hard_from_TF.ipynb","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_125":{"__typename":"Paragraph","id":"314b8592efe0_125","name":"317c","type":"P","href":null,"layout":null,"metadata":null,"text":"The left plot shows the embedding locations before training, decomposed into 2-dimensions using PCA for visualisation, and with each colour representing a distinct class as shown by the legend. Note how the embeddings are all jumbled up, there is no clear clustering structure, which makes sense as the model has not learned to separate the classes out. This is in contrast to the right plot, which shows the same data points embedded by a trained SNN. We can see clear clustering on the outskirts of the plot, but the middle is still looking a bit messy. Our plot indicates the model has learned very well to cluster embedded images of class 1 for example (the cluster in the bottom left), but still struggles with embedded images of class 5, which are still mostly in the centre. This is backed up by our previous plots, which shows the model struggling to determine a most likely match for the class 5 test image.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_126":{"__typename":"Paragraph","id":"314b8592efe0_126","name":"b5f2","type":"P","href":null,"layout":null,"metadata":null,"text":"It would be good to quantify how well our model is performing. This can be achieved using an n-way accuracy score, utilising the prototypes we discussed before. In n-way accuracy, val_steps number of randomly selected test images are compared to a support set of size n. This provides an indication of model accuracy when n is the same as the total number of classes, num_classes in the code below. MNIST has 10 classes, giving a 10-way accuracy.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":180,"end":189,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":368,"end":379,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":268,"end":271,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":322,"end":323,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"MediaResource:eb959a23d799ac3616b49a461302f405":{"__typename":"MediaResource","id":"eb959a23d799ac3616b49a461302f405","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"SNN-MNIST-n-way-acc-result"},"Paragraph:314b8592efe0_127":{"__typename":"Paragraph","id":"314b8592efe0_127","name":"be10","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:eb959a23d799ac3616b49a461302f405"}},"mixtapeMetadata":null},"Paragraph:314b8592efe0_128":{"__typename":"Paragraph","id":"314b8592efe0_128","name":"faa5","type":"P","href":null,"layout":null,"metadata":null,"text":"When we run the above code, the SNN achieves a 10-way accuracy of 97.4%, a commendable score. Due to the random nature with which the test images are chosen, you could perform cross validation here if you wish.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":28,"end":71,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_129":{"__typename":"Paragraph","id":"314b8592efe0_129","name":"93a1","type":"P","href":null,"layout":null,"metadata":null,"text":"Finally, lets take a look at producing a support set image, similar to that shown in the previous moths example, only this time generated using MNIST. Again, we will use a 10 class support set.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"MediaResource:14e63e8718b8d1599260d1d8142b8b41":{"__typename":"MediaResource","id":"14e63e8718b8d1599260d1d8142b8b41","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"SNN-MNIST-visualise-support-set"},"Paragraph:314b8592efe0_130":{"__typename":"Paragraph","id":"314b8592efe0_130","name":"d3de","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:14e63e8718b8d1599260d1d8142b8b41"}},"mixtapeMetadata":null},"Paragraph:314b8592efe0_131":{"__typename":"Paragraph","id":"314b8592efe0_131","name":"4ee8","type":"P","href":null,"layout":null,"metadata":null,"text":"This produces the following plot:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*WbvyHcsIsql3NaOJUDZe3w.png":{"__typename":"ImageMetadata","id":"1*WbvyHcsIsql3NaOJUDZe3w.png","originalHeight":880,"originalWidth":790,"focusPercentX":null,"focusPercentY":null,"alt":"L: image of a 2 labelled ‘Test Image’. M: Support set with numbers 0–9, 2 is first. R: The support set 2 image on its own."},"Paragraph:314b8592efe0_132":{"__typename":"Paragraph","id":"314b8592efe0_132","name":"0128","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*WbvyHcsIsql3NaOJUDZe3w.png"},"text":"Image generated using code based on this notebook.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":36,"end":49,"href":"https:\u002F\u002Fgithub.com\u002Fasagar60\u002FOne-Shot-Learning\u002Fblob\u002Fmaster\u002FOmniglot_data\u002FOne_shot_implementation.ipynb","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_133":{"__typename":"Paragraph","id":"314b8592efe0_133","name":"063c","type":"P","href":null,"layout":null,"metadata":null,"text":"If the moth example discussed earlier in this article was confusing, hopefully the same plot using MNIST is clearer. The code has randomly selected a class 2 test image to classify, which is compared to the prototypes of all other classes in the support set. Again, the plotting code knows the test image is of class 2, and so the support set 2 is shown first. On the right, the same support set 2 is shown again, indicating the SNN has correctly determined a most likely class of 2 for the test image, which it should do for other test images approximately 97.4% of the time!","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_134":{"__typename":"Paragraph","id":"314b8592efe0_134","name":"cdec","type":"H3","href":null,"layout":null,"metadata":null,"text":"Conclusion","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_135":{"__typename":"Paragraph","id":"314b8592efe0_135","name":"8876","type":"P","href":null,"layout":null,"metadata":null,"text":"In this article, we have learned what a Siamese Neural Network is, how to train them, and how to utilise them at inference time. Even though we have utilised a toy example through the use of MNIST, I hope that it is clear how powerful SNNs can be when working with open-ended datasets where you may not have all classes available to you at the time of dataset creation, and how new unseen-at-train-time classes would be handled by the model.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_136":{"__typename":"Paragraph","id":"314b8592efe0_136","name":"e9fa","type":"P","href":null,"layout":null,"metadata":null,"text":"I hope that I have provided you with a good balance of theoretical knowledge and practical application, and I’d like to thank everyone whom I have mentioned throughout for providing open-source code and Stack Overflow answers. This article, and indeed my own work in conservation tech, would not have been possible without it.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_137":{"__typename":"Paragraph","id":"314b8592efe0_137","name":"a5e1","type":"P","href":null,"layout":null,"metadata":null,"text":"When I first started writing, I was worried there wouldn’t be enough content for it to be worthwhile. Now that it is finished, I realise how wrong I was! Hopefully if you have made it this far (and haven’t just skipped to the end) this article has taught you something, and if this is the case please do let me know on Twitter or LinkedIn.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":319,"end":326,"href":"https:\u002F\u002Ftwitter.com\u002Fcamtrotts","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":330,"end":338,"href":"https:\u002F\u002Fwww.linkedin.com\u002Fin\u002Fcameron-trotter-0b6594109\u002F","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_138":{"__typename":"Paragraph","id":"314b8592efe0_138","name":"a69e","type":"H3","href":null,"layout":null,"metadata":null,"text":"References","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_139":{"__typename":"Paragraph","id":"314b8592efe0_139","name":"01ad","type":"P","href":null,"layout":null,"metadata":null,"text":"[1] Deng, J., Dong, W., Socher, R., Li, L.J., Li, K. and Fei-Fei, L., 2009, June. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition (pp. 248–255). IEEE.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":138,"end":201,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_140":{"__typename":"Paragraph","id":"314b8592efe0_140","name":"07a4","type":"P","href":null,"layout":null,"metadata":null,"text":"[2] Dey, S., Dutta, A., Toledo, J.I., Ghosh, S.K., Lladós, J. and Pal, U., 2017. Signet: Convolutional siamese network for writer independent offline signature verification. arXiv preprint arXiv:1707.02131.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":174,"end":205,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_141":{"__typename":"Paragraph","id":"314b8592efe0_141","name":"0bab","type":"P","href":null,"layout":null,"metadata":null,"text":"[3] LeCun, Y., Bottou, L., Bengio, Y. and Haffner, P., 1998. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), pp.2278–2324.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":118,"end":141,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":143,"end":145,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_142":{"__typename":"Paragraph","id":"314b8592efe0_142","name":"a8cb","type":"P","href":null,"layout":null,"metadata":null,"text":"[4] Hoffer, E. and Ailon, N., 2015, October. Deep metric learning using triplet network. In International Workshop on Similarity-Based Pattern Recognition (pp. 84–92). Springer, Cham.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":92,"end":154,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_143":{"__typename":"Paragraph","id":"314b8592efe0_143","name":"4f77","type":"P","href":null,"layout":null,"metadata":null,"text":"[5] Schroff, Florian, Dmitry Kalenichenko, and James Philbin. Facenet: A unified embedding for face recognition and clustering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 815–823. 2015.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":131,"end":208,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:314b8592efe0_144":{"__typename":"Paragraph","id":"314b8592efe0_144","name":"8cc9","type":"P","href":null,"layout":null,"metadata":null,"text":"[6] Vetrova, V., Coup, S., Frank, E. and Cree, M.J., 2018, November. Hidden features: Experiments with feature transfer for fine-grained multi-class and one-class image categorization. In 2018 International Conference on Image and Vision Computing New Zealand (IVCNZ) (pp. 1–6). IEEE.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":188,"end":267,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"CollectionViewerEdge:collectionId:7f60cf5620c9-viewerId:lo_98c5c93f7e53":{"__typename":"CollectionViewerEdge","id":"collectionId:7f60cf5620c9-viewerId:lo_98c5c93f7e53","isEditor":false},"ImageMetadata:1*cFFKn8rFH4ZndmaYeAs6iQ.png":{"__typename":"ImageMetadata","id":"1*cFFKn8rFH4ZndmaYeAs6iQ.png","originalWidth":2381,"originalHeight":743},"Tag:machine-learning":{"__typename":"Tag","id":"machine-learning","displayTitle":"Machine Learning","normalizedTagSlug":"machine-learning"},"Tag:siamese-networks":{"__typename":"Tag","id":"siamese-networks","displayTitle":"Siamese Networks","normalizedTagSlug":"siamese-networks"},"Tag:convolutional-network":{"__typename":"Tag","id":"convolutional-network","displayTitle":"Convolutional Network","normalizedTagSlug":"convolutional-network"},"Tag:editors-pick":{"__typename":"Tag","id":"editors-pick","displayTitle":"Editors Pick","normalizedTagSlug":"editors-pick"},"Tag:hands-on-tutorials":{"__typename":"Tag","id":"hands-on-tutorials","displayTitle":"Hands On Tutorials","normalizedTagSlug":"hands-on-tutorials"},"Post:4c6da3259463":{"__typename":"Post","id":"4c6da3259463","collection":{"__ref":"Collection:7f60cf5620c9"},"content({\"postMeteringOptions\":{}})":{"__typename":"PostContent","isLockedPreviewOnly":false,"bodyModel":{"__typename":"RichText","sections":[{"__typename":"Section","name":"73a5","startIndex":0,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null},{"__typename":"Section","name":"c3e9","startIndex":138,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null}],"paragraphs":[{"__ref":"Paragraph:314b8592efe0_0"},{"__ref":"Paragraph:314b8592efe0_1"},{"__ref":"Paragraph:314b8592efe0_2"},{"__ref":"Paragraph:314b8592efe0_3"},{"__ref":"Paragraph:314b8592efe0_4"},{"__ref":"Paragraph:314b8592efe0_5"},{"__ref":"Paragraph:314b8592efe0_6"},{"__ref":"Paragraph:314b8592efe0_7"},{"__ref":"Paragraph:314b8592efe0_8"},{"__ref":"Paragraph:314b8592efe0_9"},{"__ref":"Paragraph:314b8592efe0_10"},{"__ref":"Paragraph:314b8592efe0_11"},{"__ref":"Paragraph:314b8592efe0_12"},{"__ref":"Paragraph:314b8592efe0_13"},{"__ref":"Paragraph:314b8592efe0_14"},{"__ref":"Paragraph:314b8592efe0_15"},{"__ref":"Paragraph:314b8592efe0_16"},{"__ref":"Paragraph:314b8592efe0_17"},{"__ref":"Paragraph:314b8592efe0_18"},{"__ref":"Paragraph:314b8592efe0_19"},{"__ref":"Paragraph:314b8592efe0_20"},{"__ref":"Paragraph:314b8592efe0_21"},{"__ref":"Paragraph:314b8592efe0_22"},{"__ref":"Paragraph:314b8592efe0_23"},{"__ref":"Paragraph:314b8592efe0_24"},{"__ref":"Paragraph:314b8592efe0_25"},{"__ref":"Paragraph:314b8592efe0_26"},{"__ref":"Paragraph:314b8592efe0_27"},{"__ref":"Paragraph:314b8592efe0_28"},{"__ref":"Paragraph:314b8592efe0_29"},{"__ref":"Paragraph:314b8592efe0_30"},{"__ref":"Paragraph:314b8592efe0_31"},{"__ref":"Paragraph:314b8592efe0_32"},{"__ref":"Paragraph:314b8592efe0_33"},{"__ref":"Paragraph:314b8592efe0_34"},{"__ref":"Paragraph:314b8592efe0_35"},{"__ref":"Paragraph:314b8592efe0_36"},{"__ref":"Paragraph:314b8592efe0_37"},{"__ref":"Paragraph:314b8592efe0_38"},{"__ref":"Paragraph:314b8592efe0_39"},{"__ref":"Paragraph:314b8592efe0_40"},{"__ref":"Paragraph:314b8592efe0_41"},{"__ref":"Paragraph:314b8592efe0_42"},{"__ref":"Paragraph:314b8592efe0_43"},{"__ref":"Paragraph:314b8592efe0_44"},{"__ref":"Paragraph:314b8592efe0_45"},{"__ref":"Paragraph:314b8592efe0_46"},{"__ref":"Paragraph:314b8592efe0_47"},{"__ref":"Paragraph:314b8592efe0_48"},{"__ref":"Paragraph:314b8592efe0_49"},{"__ref":"Paragraph:314b8592efe0_50"},{"__ref":"Paragraph:314b8592efe0_51"},{"__ref":"Paragraph:314b8592efe0_52"},{"__ref":"Paragraph:314b8592efe0_53"},{"__ref":"Paragraph:314b8592efe0_54"},{"__ref":"Paragraph:314b8592efe0_55"},{"__ref":"Paragraph:314b8592efe0_56"},{"__ref":"Paragraph:314b8592efe0_57"},{"__ref":"Paragraph:314b8592efe0_58"},{"__ref":"Paragraph:314b8592efe0_59"},{"__ref":"Paragraph:314b8592efe0_60"},{"__ref":"Paragraph:314b8592efe0_61"},{"__ref":"Paragraph:314b8592efe0_62"},{"__ref":"Paragraph:314b8592efe0_63"},{"__ref":"Paragraph:314b8592efe0_64"},{"__ref":"Paragraph:314b8592efe0_65"},{"__ref":"Paragraph:314b8592efe0_66"},{"__ref":"Paragraph:314b8592efe0_67"},{"__ref":"Paragraph:314b8592efe0_68"},{"__ref":"Paragraph:314b8592efe0_69"},{"__ref":"Paragraph:314b8592efe0_70"},{"__ref":"Paragraph:314b8592efe0_71"},{"__ref":"Paragraph:314b8592efe0_72"},{"__ref":"Paragraph:314b8592efe0_73"},{"__ref":"Paragraph:314b8592efe0_74"},{"__ref":"Paragraph:314b8592efe0_75"},{"__ref":"Paragraph:314b8592efe0_76"},{"__ref":"Paragraph:314b8592efe0_77"},{"__ref":"Paragraph:314b8592efe0_78"},{"__ref":"Paragraph:314b8592efe0_79"},{"__ref":"Paragraph:314b8592efe0_80"},{"__ref":"Paragraph:314b8592efe0_81"},{"__ref":"Paragraph:314b8592efe0_82"},{"__ref":"Paragraph:314b8592efe0_83"},{"__ref":"Paragraph:314b8592efe0_84"},{"__ref":"Paragraph:314b8592efe0_85"},{"__ref":"Paragraph:314b8592efe0_86"},{"__ref":"Paragraph:314b8592efe0_87"},{"__ref":"Paragraph:314b8592efe0_88"},{"__ref":"Paragraph:314b8592efe0_89"},{"__ref":"Paragraph:314b8592efe0_90"},{"__ref":"Paragraph:314b8592efe0_91"},{"__ref":"Paragraph:314b8592efe0_92"},{"__ref":"Paragraph:314b8592efe0_93"},{"__ref":"Paragraph:314b8592efe0_94"},{"__ref":"Paragraph:314b8592efe0_95"},{"__ref":"Paragraph:314b8592efe0_96"},{"__ref":"Paragraph:314b8592efe0_97"},{"__ref":"Paragraph:314b8592efe0_98"},{"__ref":"Paragraph:314b8592efe0_99"},{"__ref":"Paragraph:314b8592efe0_100"},{"__ref":"Paragraph:314b8592efe0_101"},{"__ref":"Paragraph:314b8592efe0_102"},{"__ref":"Paragraph:314b8592efe0_103"},{"__ref":"Paragraph:314b8592efe0_104"},{"__ref":"Paragraph:314b8592efe0_105"},{"__ref":"Paragraph:314b8592efe0_106"},{"__ref":"Paragraph:314b8592efe0_107"},{"__ref":"Paragraph:314b8592efe0_108"},{"__ref":"Paragraph:314b8592efe0_109"},{"__ref":"Paragraph:314b8592efe0_110"},{"__ref":"Paragraph:314b8592efe0_111"},{"__ref":"Paragraph:314b8592efe0_112"},{"__ref":"Paragraph:314b8592efe0_113"},{"__ref":"Paragraph:314b8592efe0_114"},{"__ref":"Paragraph:314b8592efe0_115"},{"__ref":"Paragraph:314b8592efe0_116"},{"__ref":"Paragraph:314b8592efe0_117"},{"__ref":"Paragraph:314b8592efe0_118"},{"__ref":"Paragraph:314b8592efe0_119"},{"__ref":"Paragraph:314b8592efe0_120"},{"__ref":"Paragraph:314b8592efe0_121"},{"__ref":"Paragraph:314b8592efe0_122"},{"__ref":"Paragraph:314b8592efe0_123"},{"__ref":"Paragraph:314b8592efe0_124"},{"__ref":"Paragraph:314b8592efe0_125"},{"__ref":"Paragraph:314b8592efe0_126"},{"__ref":"Paragraph:314b8592efe0_127"},{"__ref":"Paragraph:314b8592efe0_128"},{"__ref":"Paragraph:314b8592efe0_129"},{"__ref":"Paragraph:314b8592efe0_130"},{"__ref":"Paragraph:314b8592efe0_131"},{"__ref":"Paragraph:314b8592efe0_132"},{"__ref":"Paragraph:314b8592efe0_133"},{"__ref":"Paragraph:314b8592efe0_134"},{"__ref":"Paragraph:314b8592efe0_135"},{"__ref":"Paragraph:314b8592efe0_136"},{"__ref":"Paragraph:314b8592efe0_137"},{"__ref":"Paragraph:314b8592efe0_138"},{"__ref":"Paragraph:314b8592efe0_139"},{"__ref":"Paragraph:314b8592efe0_140"},{"__ref":"Paragraph:314b8592efe0_141"},{"__ref":"Paragraph:314b8592efe0_142"},{"__ref":"Paragraph:314b8592efe0_143"},{"__ref":"Paragraph:314b8592efe0_144"}]},"validatedShareKey":"","shareKeyCreator":null},"creator":{"__ref":"User:9eff5ca21d80"},"inResponseToEntityType":null,"isLocked":false,"isMarkedPaywallOnly":false,"lockedSource":"LOCKED_POST_SOURCE_NONE","mediumUrl":"https:\u002F\u002Ftowardsdatascience.com\u002Fhow-to-train-your-siamese-neural-network-4c6da3259463","primaryTopic":{"__ref":"Topic:ae5d4995e225"},"topics":[{"__typename":"Topic","slug":"machine-learning"},{"__typename":"Topic","slug":"data-science"},{"__typename":"Topic","slug":"programming"}],"isPublished":true,"latestPublishedVersion":"314b8592efe0","visibility":"PUBLIC","postResponses":{"__typename":"PostResponses","count":0},"createdAt":1610706807900,"firstPublishedAt":1613738012337,"latestPublishedAt":1614804593162,"clapCount":260,"allowResponses":true,"isLimitedState":false,"title":"How To Train Your Siamese Neural Network","isSeries":false,"sequence":null,"uniqueSlug":"how-to-train-your-siamese-neural-network-4c6da3259463","socialTitle":"","socialDek":"","noIndex":null,"canonicalUrl":"","metaDescription":"","readingTime":19.750943396226415,"previewContent":{"__typename":"PreviewContent","subtitle":"When you first start out with machine learning, it becomes clear from the offset that massive amounts of data are required for accurate…"},"previewImage":{"__ref":"ImageMetadata:1*bJABur9wzFNACosQkim8kw.png"},"isShortform":false,"seoTitle":"","updatedAt":1648674938175,"shortformType":"SHORTFORM_TYPE_LINK","seoDescription":"","isSuspended":false,"license":"ALL_RIGHTS_RESERVED","tags":[{"__ref":"Tag:machine-learning"},{"__ref":"Tag:siamese-networks"},{"__ref":"Tag:convolutional-network"},{"__ref":"Tag:editors-pick"},{"__ref":"Tag:hands-on-tutorials"}],"pendingCollection":null,"statusForCollection":"APPROVED","layerCake":3,"detectedLanguage":"en","wordCount":4863,"inResponseToPostResult":null,"inResponseToCatalogResult":null,"curationEligibleAt":1613735963261,"isNewsletter":false,"isPublishToEmail":false},"ImageMetadata:1*Smqluk9f53V0NlpnknnHvw.png":{"__typename":"ImageMetadata","id":"1*Smqluk9f53V0NlpnknnHvw.png","focusPercentX":null,"focusPercentY":null,"alt":null},"User:87864cbc1dda":{"__typename":"User","id":"87864cbc1dda","name":"Damian Gil","username":"damiangilgonzalez","mediumMemberAt":1630172658000,"socialStats":{"__typename":"SocialStats","followerCount":926},"verifications":{"__typename":"VerifiedInfo","isBookAuthor":false},"customDomainState":null,"hasSubdomain":false,"bio":"Passionate about data, I transitioned from physics to data science. Worked at Telefonica, HP, and now CTO at Seniority.AI since 2022.","imageId":"1*g7rD02yuhg7PQyT7BIs0Ag.jpeg","membership":{"__ref":"Membership:9e43bc4673d3"}},"Membership:9e43bc4673d3":{"__typename":"Membership","tier":"MEMBER","id":"9e43bc4673d3"},"Post:3d9008235f41":{"__typename":"Post","id":"3d9008235f41","title":"Mastering Customer Segmentation with LLM","extendedPreviewContent":{"__typename":"PreviewContent","subtitle":"Unlock advanced customer segmentation techniques using LLMs, and improve your clustering models with advanced techniques","isFullContent":false},"previewImage":{"__ref":"ImageMetadata:1*Smqluk9f53V0NlpnknnHvw.png"},"creator":{"__ref":"User:87864cbc1dda"},"isPublished":true,"mediumUrl":"https:\u002F\u002Ftowardsdatascience.com\u002Fmastering-customer-segmentation-with-llm-3d9008235f41","collection":{"__ref":"Collection:7f60cf5620c9"},"isLimitedState":false,"allowResponses":true,"postResponses":{"__typename":"PostResponses","count":24},"visibility":"PUBLIC","clapCount":3246,"pendingCollection":null,"statusForCollection":"APPROVED","pinnedAt":0,"latestPublishedAt":1697528644236,"firstPublishedAt":1695757487673,"readingTime":23.07169811320755,"isLocked":false,"sequence":null,"isSeries":false,"uniqueSlug":"mastering-customer-segmentation-with-llm-3d9008235f41"},"ImageMetadata:1*xx1TdhbY_vupGMqhN0Z15A.png":{"__typename":"ImageMetadata","id":"1*xx1TdhbY_vupGMqhN0Z15A.png","focusPercentX":70,"focusPercentY":86,"alt":null},"User:9c6a36490614":{"__typename":"User","id":"9c6a36490614","name":"Khouloud El Alami","username":"elalamik","mediumMemberAt":1683234706000,"socialStats":{"__typename":"SocialStats","followerCount":2056},"verifications":{"__typename":"VerifiedInfo","isBookAuthor":false},"customDomainState":{"__typename":"CustomDomainState","live":null},"hasSubdomain":false,"bio":"Data Scientist @Spotify in Paris. Follow my journey as a DS in Tech & Spotify ✨ For more join me on 💌 medium.com\u002F@elalamik\u002Fsubscribe & linkedin.com\u002Fin\u002Felalamik","imageId":"1*f7K34KvvGl7J3iY5ts1dHQ@2x.jpeg","membership":{"__ref":"Membership:8925bf4c3667"}},"Membership:8925bf4c3667":{"__typename":"Membership","tier":"MEMBER","id":"8925bf4c3667"},"Post:c9cec11fd1b":{"__typename":"Post","id":"c9cec11fd1b","title":"Don’t Start Your Data Science Journey Without These 5 Must-Do Steps From a Spotify Data Scientist","extendedPreviewContent":{"__typename":"PreviewContent","subtitle":"A complete guide to everything I wish I’d done before starting my Data Science journey, here’s to acing your first year with data","isFullContent":false},"previewImage":{"__ref":"ImageMetadata:1*xx1TdhbY_vupGMqhN0Z15A.png"},"creator":{"__ref":"User:9c6a36490614"},"isPublished":true,"mediumUrl":"https:\u002F\u002Ftowardsdatascience.com\u002Fdont-start-your-data-science-journey-without-these-5-must-do-steps-from-a-spotify-data-scientist-c9cec11fd1b","collection":{"__ref":"Collection:7f60cf5620c9"},"isLimitedState":false,"allowResponses":true,"postResponses":{"__typename":"PostResponses","count":20},"visibility":"LOCKED","clapCount":2485,"pendingCollection":null,"statusForCollection":"APPROVED","pinnedAt":0,"latestPublishedAt":1696578362341,"firstPublishedAt":1695577456589,"readingTime":17.591509433962262,"isLocked":true,"sequence":null,"isSeries":false,"uniqueSlug":"dont-start-your-data-science-journey-without-these-5-must-do-steps-from-a-spotify-data-scientist-c9cec11fd1b"},"ImageMetadata:1*Fks46Zel9FUan3oxhgGz_Q.jpeg":{"__typename":"ImageMetadata","id":"1*Fks46Zel9FUan3oxhgGz_Q.jpeg","focusPercentX":null,"focusPercentY":null,"alt":"Cat holding a atom where the nucleus are documents."},"User:921c5fccdb85":{"__typename":"User","id":"921c5fccdb85","name":"Adrian H. Raudaschl","username":"araudaschl","mediumMemberAt":1578124025000,"socialStats":{"__typename":"SocialStats","followerCount":1190},"verifications":{"__typename":"VerifiedInfo","isBookAuthor":false},"customDomainState":{"__typename":"CustomDomainState","live":{"__typename":"CustomDomain","domain":"araudaschl.medium.com"}},"hasSubdomain":true,"bio":"The thoughts and lessons of a physician turned product manager driving search and generative AI innovations.","imageId":"2*a4-0O9zzHroih3uHMDmVEA.jpeg","membership":{"__ref":"Membership:ea6571784031"}},"Membership:ea6571784031":{"__typename":"Membership","tier":"MEMBER","id":"ea6571784031"},"Post:1147298d8ad1":{"__typename":"Post","id":"1147298d8ad1","title":"Forget RAG, the Future is RAG-Fusion","extendedPreviewContent":{"__typename":"PreviewContent","subtitle":"The Next Frontier of Search: Retrieval Augmented Generation meets Reciprocal Rank Fusion and Generated Queries","isFullContent":false},"previewImage":{"__ref":"ImageMetadata:1*Fks46Zel9FUan3oxhgGz_Q.jpeg"},"creator":{"__ref":"User:921c5fccdb85"},"isPublished":true,"mediumUrl":"https:\u002F\u002Ftowardsdatascience.com\u002Fforget-rag-the-future-is-rag-fusion-1147298d8ad1","collection":{"__ref":"Collection:7f60cf5620c9"},"isLimitedState":false,"allowResponses":true,"postResponses":{"__typename":"PostResponses","count":19},"visibility":"LOCKED","clapCount":1369,"pendingCollection":null,"statusForCollection":"APPROVED","pinnedAt":0,"latestPublishedAt":1696565665198,"firstPublishedAt":1696565665198,"readingTime":9.235220125786164,"isLocked":true,"sequence":null,"isSeries":false,"uniqueSlug":"forget-rag-the-future-is-rag-fusion-1147298d8ad1"},"ImageMetadata:1*KnV1cBSw-kWyh7Y6XEEzrA.jpeg":{"__typename":"ImageMetadata","id":"1*KnV1cBSw-kWyh7Y6XEEzrA.jpeg","focusPercentX":null,"focusPercentY":null,"alt":null},"User:6a2ef1b1f09d":{"__typename":"User","id":"6a2ef1b1f09d","name":"Natassha Selvaraj","username":"natassha6789","mediumMemberAt":1683747854000,"socialStats":{"__typename":"SocialStats","followerCount":13806},"verifications":{"__typename":"VerifiedInfo","isBookAuthor":false},"customDomainState":{"__typename":"CustomDomainState","live":{"__typename":"CustomDomain","domain":"natassha6789.medium.com"}},"hasSubdomain":true,"bio":"https:\u002F\u002Fwww.youtube.com\u002F@natassha_ds","imageId":"1*WHVVSRCz66KUX57Bo8oAWQ.jpeg","membership":{"__ref":"Membership:fb9baae6a189"}},"Membership:fb9baae6a189":{"__typename":"Membership","tier":"MEMBER","id":"fb9baae6a189"},"Post:1219840d0a0a":{"__typename":"Post","id":"1219840d0a0a","title":"Coding was Hard Until I Learned These 2 Things!","extendedPreviewContent":{"__typename":"PreviewContent","subtitle":"Here’s what helped me go from “aspiring programmer” to actually landing a job in the field.","isFullContent":false},"previewImage":{"__ref":"ImageMetadata:1*KnV1cBSw-kWyh7Y6XEEzrA.jpeg"},"creator":{"__ref":"User:6a2ef1b1f09d"},"isPublished":true,"mediumUrl":"https:\u002F\u002Ftowardsdatascience.com\u002Fcoding-was-hard-until-i-learned-these-2-things-1219840d0a0a","collection":{"__ref":"Collection:7f60cf5620c9"},"isLimitedState":false,"allowResponses":true,"postResponses":{"__typename":"PostResponses","count":22},"visibility":"LOCKED","clapCount":1971,"pendingCollection":null,"statusForCollection":"APPROVED","pinnedAt":0,"latestPublishedAt":1696260932147,"firstPublishedAt":1696260932147,"readingTime":6.592452830188679,"isLocked":true,"sequence":null,"isSeries":false,"uniqueSlug":"coding-was-hard-until-i-learned-these-2-things-1219840d0a0a"},"ImageMetadata:1*B6QOjW7zTpEg5NwhBGeJFg.png":{"__typename":"ImageMetadata","id":"1*B6QOjW7zTpEg5NwhBGeJFg.png","focusPercentX":null,"focusPercentY":null,"alt":null},"User:24daab033f1":{"__typename":"User","id":"24daab033f1","name":"Prabhat Kumar","username":"prabhattgs12345789","mediumMemberAt":0,"socialStats":{"__typename":"SocialStats","followerCount":1},"verifications":{"__typename":"VerifiedInfo","isBookAuthor":false},"customDomainState":null,"hasSubdomain":false,"bio":"","imageId":"0*5QFAm42C4ompIATz","membership":null},"Post:4f00e2dd8256":{"__typename":"Post","id":"4f00e2dd8256","title":"Siamese Networks Introduction and Implementation","extendedPreviewContent":{"__typename":"PreviewContent","subtitle":"Artificial Intelligence (AI) has revolutionized numerous industries, and its applications continue to expand. One of the key areas where AI…","isFullContent":false},"previewImage":{"__ref":"ImageMetadata:1*B6QOjW7zTpEg5NwhBGeJFg.png"},"creator":{"__ref":"User:24daab033f1"},"isPublished":true,"mediumUrl":"https:\u002F\u002Fmedium.com\u002F@prabhattgs12345789\u002Fsiamese-neural-network-enhancing-ai-capabilities-with-pairwise-comparisons-4f00e2dd8256","collection":null,"isLimitedState":false,"allowResponses":true,"postResponses":{"__typename":"PostResponses","count":1},"visibility":"PUBLIC","clapCount":5,"pendingCollection":null,"statusForCollection":null,"pinnedAt":0,"latestPublishedAt":1687708986256,"firstPublishedAt":1687708631140,"readingTime":4.692767295597485,"isLocked":false,"sequence":null,"isSeries":false,"uniqueSlug":"siamese-neural-network-enhancing-ai-capabilities-with-pairwise-comparisons-4f00e2dd8256"},"ImageMetadata:1*4C4f3HYbqjP5N2BNDWqzjw.png":{"__typename":"ImageMetadata","id":"1*4C4f3HYbqjP5N2BNDWqzjw.png","focusPercentX":null,"focusPercentY":null,"alt":null},"User:be3f74e4d188":{"__typename":"User","id":"be3f74e4d188","name":"Souvik Mandal","username":"mandalsouvik","mediumMemberAt":0,"socialStats":{"__typename":"SocialStats","followerCount":33},"verifications":{"__typename":"VerifiedInfo","isBookAuthor":false},"customDomainState":null,"hasSubdomain":false,"bio":"Senior AI Scientist @ Qure.ai, Ex Fractal, Deep learning, CSE IIT Indore, 20.","imageId":"1*O-8qMqfsL76mwwps2Kq2jA.jpeg","membership":null},"Post:ebb2bb6efdb1":{"__typename":"Post","id":"ebb2bb6efdb1","title":"Power of Siamese Networks and Triplet Loss: Tackling Unbalanced Datasets","extendedPreviewContent":{"__typename":"PreviewContent","subtitle":"Solve unbalanced Datasets and Image Recognition Tasks: Unveiling the Potential of Siamese Networks, Triplet Loss, and Contrastive Loss","isFullContent":false},"previewImage":{"__ref":"ImageMetadata:1*4C4f3HYbqjP5N2BNDWqzjw.png"},"creator":{"__ref":"User:be3f74e4d188"},"isPublished":true,"mediumUrl":"https:\u002F\u002Fmedium.com\u002F@mandalsouvik\u002Fpower-of-siamese-networks-and-triplet-loss-tackling-unbalanced-datasets-ebb2bb6efdb1","collection":null,"isLimitedState":false,"allowResponses":true,"postResponses":{"__typename":"PostResponses","count":0},"visibility":"PUBLIC","clapCount":7,"pendingCollection":null,"statusForCollection":null,"pinnedAt":0,"latestPublishedAt":1684357022215,"firstPublishedAt":1684357022215,"readingTime":7.3886792452830194,"isLocked":false,"sequence":null,"isSeries":false,"uniqueSlug":"power-of-siamese-networks-and-triplet-loss-tackling-unbalanced-datasets-ebb2bb6efdb1"},"ImageMetadata:1*ELi9QOrBPIixuSwBVctoMQ.jpeg":{"__typename":"ImageMetadata","id":"1*ELi9QOrBPIixuSwBVctoMQ.jpeg","focusPercentX":null,"focusPercentY":null,"alt":null},"User:1d1e37742e8c":{"__typename":"User","id":"1d1e37742e8c","name":"Mehran Ziadloo","username":"mehranziadloo","mediumMemberAt":1555105873140,"socialStats":{"__typename":"SocialStats","followerCount":7},"verifications":{"__typename":"VerifiedInfo","isBookAuthor":false},"customDomainState":null,"hasSubdomain":false,"bio":"A fellow human being, interested in Machine Learning and AI","imageId":"0*ez2-73Y93GYNDgg0.jpg","membership":{"__ref":"Membership:d5ca055902e9"}},"Membership:d5ca055902e9":{"__typename":"Membership","tier":"MEMBER","id":"d5ca055902e9"},"Post:225908e59bda":{"__typename":"Post","id":"225908e59bda","title":"Training a Siamese model with a triplet loss function on MNIST dataset using PyTorch","extendedPreviewContent":{"__typename":"PreviewContent","subtitle":"Let’s do an exercise and see how a simple Siamese model does on MNIST dataset when accompanied by a triplet loss function. I promise you…","isFullContent":false},"previewImage":{"__ref":"ImageMetadata:1*ELi9QOrBPIixuSwBVctoMQ.jpeg"},"creator":{"__ref":"User:1d1e37742e8c"},"isPublished":true,"mediumUrl":"https:\u002F\u002Fmedium.com\u002F@mehranziadloo\u002Ftraining-a-siamese-model-with-a-triplet-loss-function-on-mnist-dataset-using-pytorch-225908e59bda","collection":null,"isLimitedState":false,"allowResponses":true,"postResponses":{"__typename":"PostResponses","count":1},"visibility":"PUBLIC","clapCount":34,"pendingCollection":null,"statusForCollection":null,"pinnedAt":0,"latestPublishedAt":1695156297802,"firstPublishedAt":1693235472938,"readingTime":10.743396226415094,"isLocked":false,"sequence":null,"isSeries":false,"uniqueSlug":"training-a-siamese-model-with-a-triplet-loss-function-on-mnist-dataset-using-pytorch-225908e59bda"},"ImageMetadata:0*wA8EZHsEvgldNxbB":{"__typename":"ImageMetadata","id":"0*wA8EZHsEvgldNxbB","focusPercentX":null,"focusPercentY":null,"alt":null},"User:95530b8d5512":{"__typename":"User","id":"95530b8d5512","name":"Ankur Dhuriya","username":"ankurdhuriya","mediumMemberAt":0,"socialStats":{"__typename":"SocialStats","followerCount":64},"verifications":{"__typename":"VerifiedInfo","isBookAuthor":false},"customDomainState":{"__typename":"CustomDomainState","live":{"__typename":"CustomDomain","domain":"ankurdhuriya.medium.com"}},"hasSubdomain":true,"bio":"Data Scientist with experience on Automatic Speech Recognition (ASR), Natural Language Processing (NLP) and Reinforcement Learning (ML).","imageId":"1*rLZWKn5flcb2AJZ2NpXv2g.jpeg","membership":null},"Post:3644f0cad85b":{"__typename":"Post","id":"3644f0cad85b","title":"Audio Enhancement and Denoising Methods","extendedPreviewContent":{"__typename":"PreviewContent","subtitle":"Explore a range of powerful methods and techniques for audio enhancement and denoising. From spectral subtraction to deep learning…","isFullContent":false},"previewImage":{"__ref":"ImageMetadata:0*wA8EZHsEvgldNxbB"},"creator":{"__ref":"User:95530b8d5512"},"isPublished":true,"mediumUrl":"https:\u002F\u002Fankurdhuriya.medium.com\u002Faudio-enhancement-and-denoising-methods-3644f0cad85b","collection":null,"isLimitedState":false,"allowResponses":true,"postResponses":{"__typename":"PostResponses","count":0},"visibility":"PUBLIC","clapCount":17,"pendingCollection":null,"statusForCollection":null,"pinnedAt":0,"latestPublishedAt":1689046470396,"firstPublishedAt":1689046076937,"readingTime":5.639622641509434,"isLocked":false,"sequence":null,"isSeries":false,"uniqueSlug":"audio-enhancement-and-denoising-methods-3644f0cad85b"},"ImageMetadata:1*NJbNjabBD8qOMqU1gMzpEQ.png":{"__typename":"ImageMetadata","id":"1*NJbNjabBD8qOMqU1gMzpEQ.png","focusPercentX":null,"focusPercentY":null,"alt":null},"User:63f5d5495279":{"__typename":"User","id":"63f5d5495279","name":"Dhruv Matani","username":"dhruvbird","mediumMemberAt":0,"socialStats":{"__typename":"SocialStats","followerCount":135},"verifications":{"__typename":"VerifiedInfo","isBookAuthor":false},"customDomainState":null,"hasSubdomain":false,"bio":"Machine Learning, PyTorch, CNNs, Transformers, Vision, Speech, Text AI. On-Device AI, Model Optimization, ML and Data Infrastructure. My views are my own.","imageId":"1*FYD0sd3nwpCGl88e7tNYUQ@2x.jpeg","membership":null},"Post:867e722c8572":{"__typename":"Post","id":"867e722c8572","title":"All Pairs Cosine Similarity in PyTorch","extendedPreviewContent":{"__typename":"PreviewContent","subtitle":"PyTorch defines a cosine_similarity function to compute pairwise cosine similarity between pairs of vectors. However, there’s no method to…","isFullContent":false},"previewImage":{"__ref":"ImageMetadata:1*NJbNjabBD8qOMqU1gMzpEQ.png"},"creator":{"__ref":"User:63f5d5495279"},"isPublished":true,"mediumUrl":"https:\u002F\u002Fmedium.com\u002F@dhruvbird\u002Fall-pairs-cosine-similarity-in-pytorch-867e722c8572","collection":null,"isLimitedState":false,"allowResponses":true,"postResponses":{"__typename":"PostResponses","count":1},"visibility":"PUBLIC","clapCount":15,"pendingCollection":null,"statusForCollection":null,"pinnedAt":0,"latestPublishedAt":1686202273618,"firstPublishedAt":1686202273618,"readingTime":6.477672955974843,"isLocked":false,"sequence":null,"isSeries":false,"uniqueSlug":"all-pairs-cosine-similarity-in-pytorch-867e722c8572"},"ImageMetadata:0*ZWlF4k6QDlG_hQTr":{"__typename":"ImageMetadata","id":"0*ZWlF4k6QDlG_hQTr","focusPercentX":null,"focusPercentY":null,"alt":null},"User:b1a64eb107f0":{"__typename":"User","id":"b1a64eb107f0","name":"Everton Gomede, PhD","username":"evertongomede","mediumMemberAt":0,"socialStats":{"__typename":"SocialStats","followerCount":137},"verifications":{"__typename":"VerifiedInfo","isBookAuthor":false},"customDomainState":null,"hasSubdomain":false,"bio":"Computer Scientist developing algorithms, solutions, and tools that enable companies and their analysts to extract insights from data to decision-makers.","imageId":"1*9F4mg-YJd_HAkAySko5TWg.png","membership":null},"Post:9dbae34aacdb":{"__typename":"Post","id":"9dbae34aacdb","title":"Multi-Label Classification in Python: Empowering Machine Learning with Versatility","extendedPreviewContent":{"__typename":"PreviewContent","subtitle":"Introduction","isFullContent":false},"previewImage":{"__ref":"ImageMetadata:0*ZWlF4k6QDlG_hQTr"},"creator":{"__ref":"User:b1a64eb107f0"},"isPublished":true,"mediumUrl":"https:\u002F\u002Fmedium.com\u002F@evertongomede\u002Fmulti-label-classification-in-python-empowering-machine-learning-with-versatility-9dbae34aacdb","collection":null,"isLimitedState":false,"allowResponses":true,"postResponses":{"__typename":"PostResponses","count":0},"visibility":"PUBLIC","clapCount":22,"pendingCollection":null,"statusForCollection":null,"pinnedAt":0,"latestPublishedAt":1684968708238,"firstPublishedAt":1684968708238,"readingTime":5.052830188679246,"isLocked":false,"sequence":null,"isSeries":false,"uniqueSlug":"multi-label-classification-in-python-empowering-machine-learning-with-versatility-9dbae34aacdb"},"User:65e22f4ac01c":{"__typename":"User","username":"ben.putney","id":"65e22f4ac01c","customDomainState":null,"hasSubdomain":false},"CatalogViewerEdge:catalogId:e3668ea008e1-viewerId:lo_98c5c93f7e53":{"__typename":"CatalogViewerEdge","followersCount":503,"id":"catalogId:e3668ea008e1-viewerId:lo_98c5c93f7e53"},"ImageMetadata:0*r4yjMpEmqzHCUvWC.jpg":{"__typename":"ImageMetadata","id":"0*r4yjMpEmqzHCUvWC.jpg","alt":null},"Post:b82b9ea831e0":{"__typename":"Post","previewImage":{"__ref":"ImageMetadata:0*r4yjMpEmqzHCUvWC.jpg"},"id":"b82b9ea831e0"},"CatalogItemV2:{\"catalogItemId\":\"64dd0a1fa9164fa438851822\"}":{"__typename":"CatalogItemV2","catalogItemId":"64dd0a1fa9164fa438851822","entity":{"__ref":"Post:b82b9ea831e0"}},"ImageMetadata:1*bv2KUVNLi2sFNjBTdoBmWw.png":{"__typename":"ImageMetadata","id":"1*bv2KUVNLi2sFNjBTdoBmWw.png","alt":null},"Post:5bda439fa506":{"__typename":"Post","previewImage":{"__ref":"ImageMetadata:1*bv2KUVNLi2sFNjBTdoBmWw.png"},"id":"5bda439fa506"},"CatalogItemV2:{\"catalogItemId\":\"64dce8236851745fea469328\"}":{"__typename":"CatalogItemV2","catalogItemId":"64dce8236851745fea469328","entity":{"__ref":"Post:5bda439fa506"}},"ImageMetadata:0*zsngbTOmFCy6sUCx.jpeg":{"__typename":"ImageMetadata","id":"0*zsngbTOmFCy6sUCx.jpeg","alt":null},"Post:e377af0ffdaa":{"__typename":"Post","previewImage":{"__ref":"ImageMetadata:0*zsngbTOmFCy6sUCx.jpeg"},"id":"e377af0ffdaa"},"CatalogItemV2:{\"catalogItemId\":\"63d0199b5330ca62c672fa8e\"}":{"__typename":"CatalogItemV2","catalogItemId":"63d0199b5330ca62c672fa8e","entity":{"__ref":"Post:e377af0ffdaa"}},"ImageMetadata:1*EPYpvWdUxGbCkbgwLUAarg.png":{"__typename":"ImageMetadata","id":"1*EPYpvWdUxGbCkbgwLUAarg.png","alt":null},"Post:5f40d55184c4":{"__typename":"Post","previewImage":{"__ref":"ImageMetadata:1*EPYpvWdUxGbCkbgwLUAarg.png"},"id":"5f40d55184c4"},"CatalogItemV2:{\"catalogItemId\":\"63d0182884e5c1fa7c270bcb\"}":{"__typename":"CatalogItemV2","catalogItemId":"63d0182884e5c1fa7c270bcb","entity":{"__ref":"Post:5f40d55184c4"}},"ImageMetadata:0*4JkYIO0SWzjCCATB.jpg":{"__typename":"ImageMetadata","id":"0*4JkYIO0SWzjCCATB.jpg","alt":null},"Post:a432aa037ee1":{"__typename":"Post","previewImage":{"__ref":"ImageMetadata:0*4JkYIO0SWzjCCATB.jpg"},"id":"a432aa037ee1"},"CatalogItemV2:{\"catalogItemId\":\"63d0162d64907a64cb199dff\"}":{"__typename":"CatalogItemV2","catalogItemId":"63d0162d64907a64cb199dff","entity":{"__ref":"Post:a432aa037ee1"}},"Catalog:e3668ea008e1":{"__typename":"Catalog","id":"e3668ea008e1","name":"Predictive Modeling w\u002F Python","postItemsCount":20,"predefined":null,"creator":{"__ref":"User:65e22f4ac01c"},"viewerEdge":{"__ref":"CatalogViewerEdge:catalogId:e3668ea008e1-viewerId:lo_98c5c93f7e53"},"itemsConnection:(limit:5)":{"__typename":"CatalogItemsConnection","items":[{"__ref":"CatalogItemV2:{\"catalogItemId\":\"64dd0a1fa9164fa438851822\"}"},{"__ref":"CatalogItemV2:{\"catalogItemId\":\"64dce8236851745fea469328\"}"},{"__ref":"CatalogItemV2:{\"catalogItemId\":\"63d0199b5330ca62c672fa8e\"}"},{"__ref":"CatalogItemV2:{\"catalogItemId\":\"63d0182884e5c1fa7c270bcb\"}"},{"__ref":"CatalogItemV2:{\"catalogItemId\":\"63d0162d64907a64cb199dff\"}"}]}},"User:fa1913854e95":{"__typename":"User","username":"destingong","id":"fa1913854e95","customDomainState":{"__typename":"CustomDomainState","live":{"__typename":"CustomDomain","domain":"destingong.medium.com"}},"hasSubdomain":true},"CatalogViewerEdge:catalogId:a877c2a39884-viewerId:lo_98c5c93f7e53":{"__typename":"CatalogViewerEdge","followersCount":575,"id":"catalogId:a877c2a39884-viewerId:lo_98c5c93f7e53"},"ImageMetadata:1*swd_PY6vTCyPnsgBYoFZfA.png":{"__typename":"ImageMetadata","id":"1*swd_PY6vTCyPnsgBYoFZfA.png","alt":"Principal Component Analysis for ML"},"Post:cc9b345b75be":{"__typename":"Post","previewImage":{"__ref":"ImageMetadata:1*swd_PY6vTCyPnsgBYoFZfA.png"},"id":"cc9b345b75be"},"CatalogItemV2:{\"catalogItemId\":\"63d605ee074d62a19d55a5c6\"}":{"__typename":"CatalogItemV2","catalogItemId":"63d605ee074d62a19d55a5c6","entity":{"__ref":"Post:cc9b345b75be"}},"ImageMetadata:1*8sSAHftNwd_RNJ3k4VA0pA.png":{"__typename":"ImageMetadata","id":"1*8sSAHftNwd_RNJ3k4VA0pA.png","alt":"Time Series Analysis"},"Post:eea5cbf43c73":{"__typename":"Post","previewImage":{"__ref":"ImageMetadata:1*8sSAHftNwd_RNJ3k4VA0pA.png"},"id":"eea5cbf43c73"},"CatalogItemV2:{\"catalogItemId\":\"63a63a59ff47a8eae4844f6e\"}":{"__typename":"CatalogItemV2","catalogItemId":"63a63a59ff47a8eae4844f6e","entity":{"__ref":"Post:eea5cbf43c73"}},"ImageMetadata:1*uNyD4yNMH-DnOel1wzxOOA.png":{"__typename":"ImageMetadata","id":"1*uNyD4yNMH-DnOel1wzxOOA.png","alt":"deep learning cheatsheet for beginner"},"Post:3b976d0ee084":{"__typename":"Post","previewImage":{"__ref":"ImageMetadata:1*uNyD4yNMH-DnOel1wzxOOA.png"},"id":"3b976d0ee084"},"CatalogItemV2:{\"catalogItemId\":\"62acf84915e600098fe2ff8a\"}":{"__typename":"CatalogItemV2","catalogItemId":"62acf84915e600098fe2ff8a","entity":{"__ref":"Post:3b976d0ee084"}},"ImageMetadata:1*0V_GsMAi2zN0OcNi7hm-vw.png":{"__typename":"ImageMetadata","id":"1*0V_GsMAi2zN0OcNi7hm-vw.png","alt":null},"Post:c67258a2c0ac":{"__typename":"Post","previewImage":{"__ref":"ImageMetadata:1*0V_GsMAi2zN0OcNi7hm-vw.png"},"id":"c67258a2c0ac"},"CatalogItemV2:{\"catalogItemId\":\"62395f6adf59488c243c8439\"}":{"__typename":"CatalogItemV2","catalogItemId":"62395f6adf59488c243c8439","entity":{"__ref":"Post:c67258a2c0ac"}},"ImageMetadata:1*R6Rbcks-pGO0SkhCINrP0g.png":{"__typename":"ImageMetadata","id":"1*R6Rbcks-pGO0SkhCINrP0g.png","alt":"machine learning algorithms"},"Post:2197870ff501":{"__typename":"Post","previewImage":{"__ref":"ImageMetadata:1*R6Rbcks-pGO0SkhCINrP0g.png"},"id":"2197870ff501"},"CatalogItemV2:{\"catalogItemId\":\"6215d886744a1f187cd32093\"}":{"__typename":"CatalogItemV2","catalogItemId":"6215d886744a1f187cd32093","entity":{"__ref":"Post:2197870ff501"}},"Catalog:a877c2a39884":{"__typename":"Catalog","id":"a877c2a39884","name":"Practical Guides to Machine Learning","postItemsCount":10,"predefined":null,"creator":{"__ref":"User:fa1913854e95"},"viewerEdge":{"__ref":"CatalogViewerEdge:catalogId:a877c2a39884-viewerId:lo_98c5c93f7e53"},"itemsConnection:(limit:5)":{"__typename":"CatalogItemsConnection","items":[{"__ref":"CatalogItemV2:{\"catalogItemId\":\"63d605ee074d62a19d55a5c6\"}"},{"__ref":"CatalogItemV2:{\"catalogItemId\":\"63a63a59ff47a8eae4844f6e\"}"},{"__ref":"CatalogItemV2:{\"catalogItemId\":\"62acf84915e600098fe2ff8a\"}"},{"__ref":"CatalogItemV2:{\"catalogItemId\":\"62395f6adf59488c243c8439\"}"},{"__ref":"CatalogItemV2:{\"catalogItemId\":\"6215d886744a1f187cd32093\"}"}]}},"User:2eb23a991a63":{"__typename":"User","username":"AMGAS14","id":"2eb23a991a63","customDomainState":null,"hasSubdomain":false},"CatalogViewerEdge:catalogId:0a856388a93a-viewerId:lo_98c5c93f7e53":{"__typename":"CatalogViewerEdge","followersCount":322,"id":"catalogId:0a856388a93a-viewerId:lo_98c5c93f7e53"},"ImageMetadata:1*8cAKzo9aJEtQJ6JNO99Q_g.png":{"__typename":"ImageMetadata","id":"1*8cAKzo9aJEtQJ6JNO99Q_g.png","alt":null},"Post:ba61798942f8":{"__typename":"Post","previewImage":{"__ref":"ImageMetadata:1*8cAKzo9aJEtQJ6JNO99Q_g.png"},"id":"ba61798942f8"},"CatalogItemV2:{\"catalogItemId\":\"653116a516747c6237de4f02\"}":{"__typename":"CatalogItemV2","catalogItemId":"653116a516747c6237de4f02","entity":{"__ref":"Post:ba61798942f8"}},"ImageMetadata:1*YZLpgfgla1EEdXmmg6ebsA.png":{"__typename":"ImageMetadata","id":"1*YZLpgfgla1EEdXmmg6ebsA.png","alt":null},"Post:61d90f5ca000":{"__typename":"Post","previewImage":{"__ref":"ImageMetadata:1*YZLpgfgla1EEdXmmg6ebsA.png"},"id":"61d90f5ca000"},"CatalogItemV2:{\"catalogItemId\":\"65310f73d97b852b89b95ba0\"}":{"__typename":"CatalogItemV2","catalogItemId":"65310f73d97b852b89b95ba0","entity":{"__ref":"Post:61d90f5ca000"}},"ImageMetadata:1*oVpdNnl1VqexrfeXKOQa5A.png":{"__typename":"ImageMetadata","id":"1*oVpdNnl1VqexrfeXKOQa5A.png","alt":null},"Post:17ee62e5cbd9":{"__typename":"Post","previewImage":{"__ref":"ImageMetadata:1*oVpdNnl1VqexrfeXKOQa5A.png"},"id":"17ee62e5cbd9"},"CatalogItemV2:{\"catalogItemId\":\"652fc01b66092b181cebf880\"}":{"__typename":"CatalogItemV2","catalogItemId":"652fc01b66092b181cebf880","entity":{"__ref":"Post:17ee62e5cbd9"}},"ImageMetadata:1*tvlLo0nwIeAmO-wluJNyqA.png":{"__typename":"ImageMetadata","id":"1*tvlLo0nwIeAmO-wluJNyqA.png","alt":null},"Post:55e962ba5cf5":{"__typename":"Post","previewImage":{"__ref":"ImageMetadata:1*tvlLo0nwIeAmO-wluJNyqA.png"},"id":"55e962ba5cf5"},"CatalogItemV2:{\"catalogItemId\":\"652fbff0dbdaf90d96a1ff06\"}":{"__typename":"CatalogItemV2","catalogItemId":"652fbff0dbdaf90d96a1ff06","entity":{"__ref":"Post:55e962ba5cf5"}},"ImageMetadata:1*FQ5v07fr-3iU16ZmJlgnTw.jpeg":{"__typename":"ImageMetadata","id":"1*FQ5v07fr-3iU16ZmJlgnTw.jpeg","alt":null},"Post:90047ebf1606":{"__typename":"Post","previewImage":{"__ref":"ImageMetadata:1*FQ5v07fr-3iU16ZmJlgnTw.jpeg"},"id":"90047ebf1606"},"CatalogItemV2:{\"catalogItemId\":\"652fbfea4d0dc37cc327d519\"}":{"__typename":"CatalogItemV2","catalogItemId":"652fbfea4d0dc37cc327d519","entity":{"__ref":"Post:90047ebf1606"}},"Catalog:0a856388a93a":{"__typename":"Catalog","id":"0a856388a93a","name":"Natural Language Processing","postItemsCount":723,"predefined":null,"creator":{"__ref":"User:2eb23a991a63"},"viewerEdge":{"__ref":"CatalogViewerEdge:catalogId:0a856388a93a-viewerId:lo_98c5c93f7e53"},"itemsConnection:(limit:5)":{"__typename":"CatalogItemsConnection","items":[{"__ref":"CatalogItemV2:{\"catalogItemId\":\"653116a516747c6237de4f02\"}"},{"__ref":"CatalogItemV2:{\"catalogItemId\":\"65310f73d97b852b89b95ba0\"}"},{"__ref":"CatalogItemV2:{\"catalogItemId\":\"652fc01b66092b181cebf880\"}"},{"__ref":"CatalogItemV2:{\"catalogItemId\":\"652fbff0dbdaf90d96a1ff06\"}"},{"__ref":"CatalogItemV2:{\"catalogItemId\":\"652fbfea4d0dc37cc327d519\"}"}]}},"User:a32c340ea342":{"__typename":"User","username":"MediumStaff","id":"a32c340ea342","customDomainState":null,"hasSubdomain":false},"CatalogViewerEdge:catalogId:5969c7449b7f-viewerId:lo_98c5c93f7e53":{"__typename":"CatalogViewerEdge","followersCount":152,"id":"catalogId:5969c7449b7f-viewerId:lo_98c5c93f7e53"},"ImageMetadata:0*3OsUtsnlTx9Svm4c.jpg":{"__typename":"ImageMetadata","id":"0*3OsUtsnlTx9Svm4c.jpg","alt":"Image by vectorjuice on FreePik"},"Post:23a2173eecae":{"__typename":"Post","previewImage":{"__ref":"ImageMetadata:0*3OsUtsnlTx9Svm4c.jpg"},"id":"23a2173eecae"},"CatalogItemV2:{\"catalogItemId\":\"6482363e212e6c41a481d903\"}":{"__typename":"CatalogItemV2","catalogItemId":"6482363e212e6c41a481d903","entity":{"__ref":"Post:23a2173eecae"}},"ImageMetadata:1*IPZF1hcDWwpPqOz2vL7NxQ.png":{"__typename":"ImageMetadata","id":"1*IPZF1hcDWwpPqOz2vL7NxQ.png","alt":null},"Post:3bc2644d4507":{"__typename":"Post","previewImage":{"__ref":"ImageMetadata:1*IPZF1hcDWwpPqOz2vL7NxQ.png"},"id":"3bc2644d4507"},"CatalogItemV2:{\"catalogItemId\":\"641cac35672446f81159a840\"}":{"__typename":"CatalogItemV2","catalogItemId":"641cac35672446f81159a840","entity":{"__ref":"Post:3bc2644d4507"}},"ImageMetadata:1*0fHUKyg3xtpNWpop35PR4g.png":{"__typename":"ImageMetadata","id":"1*0fHUKyg3xtpNWpop35PR4g.png","alt":null},"Post:59c16ae76e3e":{"__typename":"Post","previewImage":{"__ref":"ImageMetadata:1*0fHUKyg3xtpNWpop35PR4g.png"},"id":"59c16ae76e3e"},"CatalogItemV2:{\"catalogItemId\":\"6422e78d7bc8cca169b3ce68\"}":{"__typename":"CatalogItemV2","catalogItemId":"6422e78d7bc8cca169b3ce68","entity":{"__ref":"Post:59c16ae76e3e"}},"ImageMetadata:1*FS8L4XLW_3mHDyRKNwjKvA.jpeg":{"__typename":"ImageMetadata","id":"1*FS8L4XLW_3mHDyRKNwjKvA.jpeg","alt":null},"Post:2bf00851551e":{"__typename":"Post","previewImage":{"__ref":"ImageMetadata:1*FS8L4XLW_3mHDyRKNwjKvA.jpeg"},"id":"2bf00851551e"},"CatalogItemV2:{\"catalogItemId\":\"6421a9dc3b730f2980719d64\"}":{"__typename":"CatalogItemV2","catalogItemId":"6421a9dc3b730f2980719d64","entity":{"__ref":"Post:2bf00851551e"}},"ImageMetadata:1*OhWzHPkxXbslpwYyCZ2HKA.png":{"__typename":"ImageMetadata","id":"1*OhWzHPkxXbslpwYyCZ2HKA.png","alt":null},"Post:1ce5fca96286":{"__typename":"Post","previewImage":{"__ref":"ImageMetadata:1*OhWzHPkxXbslpwYyCZ2HKA.png"},"id":"1ce5fca96286"},"CatalogItemV2:{\"catalogItemId\":\"641cac41b70e26f89f868177\"}":{"__typename":"CatalogItemV2","catalogItemId":"641cac41b70e26f89f868177","entity":{"__ref":"Post:1ce5fca96286"}},"Catalog:5969c7449b7f":{"__typename":"Catalog","id":"5969c7449b7f","name":"The New Chatbots: ChatGPT, Bard, and Beyond","postItemsCount":12,"predefined":null,"creator":{"__ref":"User:a32c340ea342"},"viewerEdge":{"__ref":"CatalogViewerEdge:catalogId:5969c7449b7f-viewerId:lo_98c5c93f7e53"},"itemsConnection:(limit:5)":{"__typename":"CatalogItemsConnection","items":[{"__ref":"CatalogItemV2:{\"catalogItemId\":\"6482363e212e6c41a481d903\"}"},{"__ref":"CatalogItemV2:{\"catalogItemId\":\"641cac35672446f81159a840\"}"},{"__ref":"CatalogItemV2:{\"catalogItemId\":\"6422e78d7bc8cca169b3ce68\"}"},{"__ref":"CatalogItemV2:{\"catalogItemId\":\"6421a9dc3b730f2980719d64\"}"},{"__ref":"CatalogItemV2:{\"catalogItemId\":\"641cac41b70e26f89f868177\"}"}]}}}</script><script>window.__MIDDLEWARE_STATE__={"session":{"xsrf":""},"cache":{"cacheStatus":"HIT","inDisabledExperiment":false}}</script><script src="https://cdn-client.medium.com/lite/static/js/manifest.b6e9078a.js"></script><script src="https://cdn-client.medium.com/lite/static/js/9614.0ec0c487.js"></script><script src="https://cdn-client.medium.com/lite/static/js/main.2f32d965.js"></script><script src="https://cdn-client.medium.com/lite/static/js/instrumentation.7cdafcd5.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/reporting.2021fe63.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/6068.466148a0.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/120.2598f8b6.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/1752.a348f767.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/6733.c6c17f3e.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/4711.eb865124.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/8695.88316669.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/4341.ef6c7cb2.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/3154.8627f8ed.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/5203.972fb599.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/1957.fb79a2a1.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/9599.27488969.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/1711.0e7a160d.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/5268.916ea32b.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/9114.0acbd6c8.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/5459.fefde3bc.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/6804.5c092f24.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/9174.9c380a1b.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/4129.12aaca9e.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/8580.2dd0c5ae.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/1802.266129dd.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/2295.deadbfb6.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/4078.9fb8a750.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/8883.821b6f5c.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/2550.2780af8f.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/9408.042c8c60.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/397.9f31a689.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/9150.e244f1b8.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/5005.4ccc91b2.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/2804.bf420f9e.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/1006.70484574.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/PostPage.MainContent.7e7142f7.chunk.js"></script><script>window.main();</script><script defer src="https://static.cloudflareinsights.com/beacon.min.js/v84a3a4012de94ce1a686ba8c167c359c1696973893317" integrity="sha512-euoFGowhlaLqXsPWQ48qSkBSCFs3DPRyiwVu3FjR96cMPx+Fr+gpWRhIafcHwqwCqWS42RZhIudOvEI+Ckf6MA==" data-cf-beacon='{"rayId":"818f557b8e9e6a6f","version":"2023.10.0","token":"0b5f665943484354a59c39c6833f7078"}' crossorigin="anonymous"></script>
</body></html>