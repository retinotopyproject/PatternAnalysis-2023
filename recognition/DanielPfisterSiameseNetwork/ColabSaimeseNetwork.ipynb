{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pRlbOGtVqm4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow.keras.backend as K"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "CYQ-48htV0MV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/drive/MyDrive/ColabNotebooks/ADNI.zip\" -d \"/content\""
      ],
      "metadata": {
        "id": "L1GMZPRTV1dq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define the function to read all the image names from folder\n",
        "#and count the image number\n",
        "def image_list(path_folder_images):\n",
        "    list_image = sorted(os.listdir(path_folder_images))\n",
        "    image_count = len(list_image)\n",
        "    print(\"Number of images: \" + str(image_count))\n",
        "\n",
        "    return list_image, image_count"
      ],
      "metadata": {
        "id": "nQSKdco2WTTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# splits the dataset in training and validation\n",
        "def split_data(list_image, split_ratio):\n",
        "    train_number_split = int(len(list_image)*split_ratio)\n",
        "    print(train_number_split)\n",
        "    train_data = list_image[:train_number_split] #takes the first to train_number and defines them as training images\n",
        "    valid_data = list_image[train_number_split:] #takes the train_number to the last element and defines them as validation images\n",
        "    print(train_data)\n",
        "    print(valid_data)\n",
        "    number_train = len(train_data)\n",
        "    number_valid = len(valid_data)\n",
        "    print(\"Number of training images: \" + str(number_train))\n",
        "    print(\"Number of validation images AD: \" + str(number_valid))\n",
        "    return train_data, valid_data, number_train, number_valid"
      ],
      "metadata": {
        "id": "QT3fY3ACWbgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the function to load the images\n",
        "def load_images(path_folder_images, chunk_images, height, width):\n",
        "    image_array = []\n",
        "    for a in chunk_images:\n",
        "        path_image = path_folder_images + a # create the path of the image\n",
        "        img = tf.keras.utils.load_img(path_image, target_size=(height, width), color_mode=\"grayscale\") # load the image\n",
        "        img = tf.keras.utils.img_to_array(img) # convert the image into an array\n",
        "        img = img / 255 #normalize the image\n",
        "        image_array.append(img) # append the array image to the list\n",
        "\n",
        "    image_array = np.array(image_array) # create a numpy array\n",
        "    return image_array"
      ],
      "metadata": {
        "id": "NrBxFg3HWhcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data generator training images\n",
        "def load_images_train_generator(path_folder_images1, path_folder_images2,  list_images1, list_images2, number_images1, number_images2, height, width, batch_size):\n",
        "    while True:\n",
        "        #list_images\n",
        "        random.shuffle(list_images1) #shuffels the images from the first list\n",
        "        random.shuffle(list_images2) #shuffels the images from the second list\n",
        "        for i in range(0, len(list_images1), batch_size): #takes the frist images and loads them into the chunk variable\n",
        "            chunk_AD = list_images1[i:i + batch_size]\n",
        "            chunk_NC = list_images2[i:i + batch_size]\n",
        "            image_array_AD = []\n",
        "            image_array_NC = []\n",
        "\n",
        "            for j in chunk_AD:\n",
        "                path_image_AD = path_folder_images1 + j # create the path of the image\n",
        "                imgAD = tf.keras.utils.load_img(path_image_AD, target_size=(height, width), color_mode=\"grayscale\") # load the images\n",
        "                imgAD = tf.keras.utils.img_to_array(imgAD) # convert the image into an array\n",
        "                imgAD = imgAD / 255.0 #normalize the image\n",
        "                image_array_AD.append(imgAD)\n",
        "            image_array_AD = np.array(image_array_AD) # create a numpy array\n",
        "\n",
        "            for k in chunk_NC:\n",
        "                path_image_NC = path_folder_images2 + k # create the path of the image\n",
        "                imgNC = tf.keras.utils.load_img(path_image_NC, target_size=(height, width), color_mode=\"grayscale\") # load the image\n",
        "                imgNC = tf.keras.utils.img_to_array(imgNC) # convert the image into an array\n",
        "                imgNC = imgNC / 255.0 #normalize the image\n",
        "                image_array_NC.append(imgNC)\n",
        "            image_array_NC = np.array(image_array_NC) # create a numpy array\n",
        "            image1_list = []\n",
        "            image2_list = []\n",
        "            label_list= []\n",
        "\n",
        "            for ad1_data, nc1_data in zip(image_array_AD, image_array_NC):\n",
        "                ad2_index = random.randint(0, len(list_images1)-1)\n",
        "                ad2 = list_images1[ad2_index]\n",
        "                ad2 = path_folder_images1 + ad2 # create the path of the image\n",
        "                ad2_data = tf.keras.utils.load_img(ad2, target_size=(height, width), color_mode=\"grayscale\") # load the image\n",
        "                ad2_data = tf.keras.utils.img_to_array(ad2_data)\n",
        "                ad2_data = ad2_data / 255.0 #normalize the image\n",
        "\n",
        "                nc2_index = random.randint(0, len(list_images2) -1)\n",
        "                # print(len(list_images2), nc2_index)\n",
        "                nc2 = list_images2[nc2_index]\n",
        "                nc2 = path_folder_images2 + nc2 # create the path of the image\n",
        "                nc2_data = tf.keras.utils.load_img(nc2, target_size=(height, width), color_mode=\"grayscale\") # load the image\n",
        "                nc2_data = tf.keras.utils.img_to_array(nc2_data)\n",
        "                nc2_data = nc2_data / 255.0 #normalize the image\n",
        "                #creates four pairs of images\n",
        "                #AD and AD with label 1, AD and NC with label 0,\n",
        "                #NC and NC with label 1, NC and AD with label 0\n",
        "                image1_list.append(ad1_data)\n",
        "                image2_list.append(ad2_data)\n",
        "                label_list.append(1)\n",
        "                image1_list.append(ad1_data)\n",
        "                image2_list.append(nc2_data)\n",
        "                label_list.append(0)\n",
        "                image1_list.append(nc1_data)\n",
        "                image2_list.append(nc2_data)\n",
        "                label_list.append(1)\n",
        "                image1_list.append(nc1_data)\n",
        "                image2_list.append(ad2_data)\n",
        "                label_list.append(0)\n",
        "\n",
        "            # shuffel the image pairs with the corresponding label\n",
        "            zipped_list = list(zip(image1_list, image2_list, label_list))\n",
        "            random.shuffle(zipped_list)\n",
        "            image1_list, image2_list, label_list = zip(*(zipped_list))\n",
        "\n",
        "            # convert to np array\n",
        "            image1_array = np.array(image1_list)\n",
        "            image2_array = np.array(image2_list)\n",
        "            label_array = np.array(label_list)\n",
        "            yield [image1_array, image2_array], label_array"
      ],
      "metadata": {
        "id": "Q1NaQRl_WiUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a list of number and shuffle the list\n",
        "# the shuffled list is the order of the validation images\n",
        "def valid_order(valid_data):\n",
        "    data_order = []\n",
        "    data_order = list(range(0,len(valid_data)))\n",
        "    print(data_order)\n",
        "    random.shuffle(data_order)\n",
        "    print(data_order)\n",
        "    return data_order"
      ],
      "metadata": {
        "id": "-hwjh3rYWmF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data generator validation images\n",
        "def load_images_valid_generator(path_folder_images1, path_folder_images2,  list_images1, list_images2, number_images1, number_images2, shuffel_list1, shuffel_list2, height, width, batch_size):\n",
        "    while True:\n",
        "        for i in range(0, len(list_images1), batch_size): #takes the frist images and loads them into the chunk variable\n",
        "            chunk_AD = list_images1[i:i + batch_size]\n",
        "            chunk_NC = list_images2[i:i + batch_size]\n",
        "            indexAD = shuffel_list1[i:i + batch_size]\n",
        "            indexNC = shuffel_list2[i:i + batch_size]\n",
        "            image_array_AD = []\n",
        "            image_array_NC = []\n",
        "\n",
        "            for j in chunk_AD:\n",
        "                path_image_AD = path_folder_images1 + j # create the path of the image\n",
        "                imgAD = tf.keras.utils.load_img(path_image_AD, target_size=(height, width), color_mode=\"grayscale\") # load the images\n",
        "                imgAD = tf.keras.utils.img_to_array(imgAD) # convert the image into an array\n",
        "                imgAD = imgAD / 255.0 #normalize the image\n",
        "                image_array_AD.append(imgAD)\n",
        "            image_array_AD = np.array(image_array_AD) # create a numpy array\n",
        "\n",
        "            for k in chunk_NC:\n",
        "                path_image_NC = path_folder_images2 + k # create the path of the image\n",
        "                imgNC = tf.keras.utils.load_img(path_image_NC, target_size=(height, width), color_mode=\"grayscale\") # load the image\n",
        "                imgNC = tf.keras.utils.img_to_array(imgNC) # convert the image into an array\n",
        "                imgNC = imgNC / 255.0 #normalize the image\n",
        "                image_array_NC.append(imgNC)\n",
        "            image_array_NC = np.array(image_array_NC) # create a numpy array\n",
        "            image1_list = []\n",
        "            image2_list = []\n",
        "            label_list= []\n",
        "\n",
        "            for ad1_data, nc1_data, AD_index, NC_index in zip(image_array_AD, image_array_NC, indexAD, indexNC):\n",
        "                ad2 = list_images1[AD_index]\n",
        "                ad2 = path_folder_images1 + ad2 # create the path of the image\n",
        "                ad2_data = tf.keras.utils.load_img(ad2, target_size=(height, width), color_mode=\"grayscale\") # load the image\n",
        "                ad2_data = tf.keras.utils.img_to_array(ad2_data)# convert the image into an array\n",
        "                ad2_data = ad2_data / 255.0 #normalize the image\n",
        "\n",
        "                nc2 = list_images2[NC_index]\n",
        "                nc2 = path_folder_images2 + nc2 # create the path of the image\n",
        "                nc2_data = tf.keras.utils.load_img(nc2, target_size=(height, width), color_mode=\"grayscale\") # load the image\n",
        "                nc2_data = tf.keras.utils.img_to_array(nc2_data)# convert the image into an array\n",
        "                nc2_data = nc2_data / 255.0 #normalize the image\n",
        "                #creates four pairs of images\n",
        "                #AD and AD with label 1, AD and NC with label 0,\n",
        "                #NC and NC with label 1, NC and AD with label 0\n",
        "                image1_list.append(ad1_data)\n",
        "                image2_list.append(ad2_data)\n",
        "                label_list.append(1)\n",
        "                image1_list.append(ad1_data)\n",
        "                image2_list.append(nc2_data)\n",
        "                label_list.append(0)\n",
        "                image1_list.append(nc1_data)\n",
        "                image2_list.append(nc2_data)\n",
        "                label_list.append(1)\n",
        "                image1_list.append(nc1_data)\n",
        "                image2_list.append(ad2_data)\n",
        "                label_list.append(0)\n",
        "\n",
        "            # convert lists to np array\n",
        "            image1_array = np.array(image1_list)\n",
        "            image2_array = np.array(image2_list)\n",
        "            label_array = np.array(label_list)\n",
        "\n",
        "            yield [image1_array, image2_array], label_array"
      ],
      "metadata": {
        "id": "TaEM3O_NWpKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Source from the website [Image similarity estimation using a Siamese Network with a contrastive loss]\n",
        "#https://keras.io/examples/vision/siamese_contrastive/\n",
        "def contrastive_loss(y_true_label, y_pred_label, margin=1.0):\n",
        "    y_true_label = tf.cast(y_true_label, dtype=tf.float32)  # Cast y_true_label to float32\n",
        "    sq_pred = tf.square(y_pred_label)\n",
        "    margin_square = tf.square(tf.maximum(margin - y_pred_label, 0))\n",
        "    return tf.reduce_mean((1-y_true_label) * sq_pred + y_true_label * margin_square)"
      ],
      "metadata": {
        "id": "VkSL73s8WsTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def siamese_network(height,width,dimension):\n",
        "\n",
        "    #define input of siamese network\n",
        "    input_shape = (height, width, dimension)\n",
        "    left_input = layers.Input(input_shape)\n",
        "    right_input = layers.Input(input_shape)\n",
        "    #define standard model of the left and right siamese network which is a VGG16 without all the dense layers\n",
        "    vgg16 = tf.keras.Sequential([ layers.Conv2D (filters =64, kernel_size =3, padding ='same', activation='relu'),\n",
        "                                layers.Conv2D (filters =64, kernel_size =3, padding ='same', activation='relu'),\n",
        "                                layers.MaxPool2D(pool_size =2, strides =2, padding ='same'),\n",
        "\n",
        "                                layers.Conv2D (filters =128, kernel_size =3, padding ='same', activation='relu'),\n",
        "                                layers.Conv2D (filters =128, kernel_size =3, padding ='same', activation='relu'),\n",
        "                                layers.MaxPool2D(pool_size =2, strides =2, padding ='same'),\n",
        "\n",
        "                                layers.Conv2D (filters =256, kernel_size =3, padding ='same', activation='relu'),\n",
        "                                layers.Conv2D (filters =256, kernel_size =3, padding ='same', activation='relu'),\n",
        "                                layers.Conv2D (filters =256, kernel_size =3, padding ='same', activation='relu'),\n",
        "                                layers.MaxPool2D(pool_size =2, strides =2, padding ='same'),\n",
        "\n",
        "                                layers.Conv2D (filters =512, kernel_size =3, padding ='same', activation='relu'),\n",
        "                                layers.Conv2D (filters =512, kernel_size =3, padding ='same', activation='relu'),\n",
        "                                layers.Conv2D (filters =512, kernel_size =3, padding ='same', activation='relu'),\n",
        "                                layers.MaxPool2D(pool_size =2, strides =2, padding ='same'),\n",
        "\n",
        "                                layers.Conv2D (filters =512, kernel_size =3, padding ='same', activation='relu'),\n",
        "                                layers.Conv2D (filters =512, kernel_size =3, padding ='same', activation='relu'),\n",
        "                                layers.Conv2D (filters =512, kernel_size =3, padding ='same', activation='relu'),\n",
        "                                layers.MaxPool2D(pool_size =2, strides =2, padding ='same'),\n",
        "\n",
        "                                layers.Flatten(),\n",
        "                                layers.Dense(512, activation='sigmoid'),\n",
        "                                ])\n",
        "\n",
        "    #save the features from the left and right network in two variables\n",
        "    feature_vector_left_output = vgg16(left_input)\n",
        "    feature_vector_right_output = vgg16(right_input)\n",
        "\n",
        "    # distance layer, which calculates the distance between both networks\n",
        "    distance_layer = layers.Lambda(lambda features: tf.abs(features[0] - features[1]))([feature_vector_left_output, feature_vector_right_output])\n",
        "\n",
        "\n",
        "    #fully connected layers\n",
        "    output = layers.Dense(1, activation='sigmoid')(distance_layer)\n",
        "    #create whole neural network model\n",
        "    model = tf.keras.Model(inputs=[left_input, right_input], outputs=output)\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    #Configurates the loss funciton, optimizer type and metrics of the model for training.\n",
        "    model.compile(loss=contrastive_loss,\n",
        "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "_agt1zOpWtEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define varibles\n",
        "height = 128\n",
        "width = 128\n",
        "dimension = 1\n",
        "batch_size = 32\n",
        "\n",
        "#define paths\n",
        "\n",
        "path_train_images_AD = \"/content/ADNI/AD_NC/train/AD/\"\n",
        "path_train_images_NC = \"/content/ADNI/AD_NC/train/NC/\"\n",
        "\n",
        "path_test_images_AD = \"/content/ADNI/AD_NC/test/AD/\"\n",
        "path_test_images_NC = \"/content/ADNI/AD_NC/test/NC/\""
      ],
      "metadata": {
        "id": "7CSRY79mWzYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load all image names of the different train and test folders\n",
        "list_train_AD, number_AD = image_list(path_train_images_AD)\n",
        "list_train_NC, number_NC = image_list(path_train_images_NC)\n",
        "\n",
        "list_test_AD, number_test_AD = image_list(path_test_images_AD)\n",
        "list_test_NC, number_test_NC = image_list(path_test_images_NC)"
      ],
      "metadata": {
        "id": "S3WunoWsXPOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#split data into training and validation dataset\n",
        "split_ratio_training = 0.8\n",
        "train_data_AD, valid_data_AD, number_train_AD, number_valid_AD = split_data(list_train_AD, split_ratio_training)\n",
        "train_data_NC, valid_data_NC, number_train_NC, number_valid_NC = split_data(list_train_NC, split_ratio_training)"
      ],
      "metadata": {
        "id": "ego7IqEEXRYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define order of validation images\n",
        "valid_data_order_AD = valid_order(valid_data_AD)\n",
        "valid_data_order_NC = valid_order(valid_data_NC)"
      ],
      "metadata": {
        "id": "UCH75rzEXT-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load all the images from the folder\n",
        "\n",
        "#define list\n",
        "train_images_AD = []\n",
        "train_images_NC = []\n",
        "\n",
        "#load images into list as a numpy array\n",
        "train_images = load_images_train_generator(path_train_images_AD, path_train_images_NC, train_data_AD, train_data_NC, number_train_AD, number_train_NC, height, width, batch_size)\n",
        "valid_images = load_images_valid_generator(path_train_images_AD, path_train_images_NC, valid_data_AD, valid_data_NC, number_valid_AD, number_valid_NC, valid_data_order_AD, valid_data_order_NC, height, width, batch_size)"
      ],
      "metadata": {
        "id": "pxUMERznXW5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define callbacks\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",min_delta=0,patience=6,verbose=0,mode=\"auto\",baseline=None,restore_best_weights=True,start_from_epoch=5)\n",
        "#define model\n",
        "siamese_model = siamese_network(height,width,dimension)\n",
        "#training of the neural network\n",
        "history = siamese_model.fit(x=train_images,\n",
        "                            validation_data = valid_images,\n",
        "                            steps_per_epoch = len(train_data_AD)//batch_size,\n",
        "                            validation_steps = len(valid_data_AD)//batch_size,\n",
        "                            shuffle = False, epochs=50, callbacks=[callback])"
      ],
      "metadata": {
        "id": "nvD0yMdsXcr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training and validation accuracy per epoch\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "# Get number of epochs\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(epochs, acc)\n",
        "plt.plot(epochs, val_acc)\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "plt.show()\n",
        "\n",
        "# Plot training and validation loss per epoch\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss)\n",
        "plt.plot(epochs, val_loss)\n",
        "plt.title('Training and validation loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zpkBtUCIXey-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#saves the model\n",
        "siamese_model.save('/content/drive/MyDrive/ColabNotebooks/model.h5')"
      ],
      "metadata": {
        "id": "OemBkl5gYEJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load the trained weights of the neural network\n",
        "#siamese_model = tf.keras.saving.load_model(\"/content/drive/MyDrive/ColabNotebooks/model.h5\")"
      ],
      "metadata": {
        "id": "9bareSwoYNNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# validate the model with the validate dataset\n",
        "metrics_valid = siamese_model.evaluate(valid_images,steps = len(train_data_AD)//batch_size)\n",
        "print('Loss of {} and Accuracy is {} %'.format(metrics_valid[0], metrics_valid[1] * 100))"
      ],
      "metadata": {
        "id": "EfBdhjPtYVd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test model with the test dataset\n",
        "#define the order of the test images\n",
        "test_data_order_AD = valid_order(list_test_AD)\n",
        "test_data_order_NC = valid_order(list_test_NC)\n",
        "\n",
        "#load test images\n",
        "test_images = load_images_valid_generator(path_test_images_AD, path_test_images_NC, list_test_AD, list_test_NC, number_test_AD, number_test_NC,test_data_order_AD, test_data_order_NC,height, width, batch_size= batch_size)\n",
        "\n",
        "#test the model with test images\n",
        "metrics_test = siamese_model.evaluate(test_images,steps = len(list_test_AD)//batch_size)\n",
        "print('Loss of {} and Accuracy is {} %'.format(metrics_test[0], metrics_test[1] * 100))"
      ],
      "metadata": {
        "id": "yyyjU4bJYYZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create a copie of the list and shuffle only the AD2 and NC2 list\n",
        "list_test_AD2 = list_test_AD[1:number_test_AD]\n",
        "list_test_NC2 = list_test_NC[1:number_test_NC]\n",
        "temp_AD2 = list_test_AD[0]\n",
        "temp_NC2 = list_test_NC[0]\n",
        "print(list_test_AD)\n",
        "print(list_test_NC)\n",
        "print(list_test_AD2)\n",
        "print(list_test_NC2)\n",
        "random.Random(42).shuffle(list_test_AD2)\n",
        "random.Random(42).shuffle(list_test_NC2)\n",
        "list_test_AD2.append(temp_AD2)\n",
        "list_test_NC2.append(temp_NC2)\n",
        "\n",
        "print(list_test_AD)\n",
        "print(list_test_NC)\n",
        "print(list_test_AD2)\n",
        "print(list_test_NC2)\n",
        "\n",
        "#counter variable\n",
        "number_of_correct = 0\n",
        "number_of_incorrect = 0\n",
        "\n",
        "#test loop for accuracy\n",
        "for a in range(0, number_test_AD//batch_size*batch_size, batch_size):\n",
        "  print(a)\n",
        "  #1 load 32 images\n",
        "  test_image_chunk_AD1 = list_test_AD[a:a+batch_size]\n",
        "  test_image_chunk_NC1 = list_test_NC[a:a+batch_size]\n",
        "  test_image_chunk_AD2 = list_test_AD2[a:a+batch_size]\n",
        "  test_image_chunk_NC2 = list_test_NC2[a:a+batch_size]\n",
        "  print(test_image_chunk_AD1)\n",
        "  print(test_image_chunk_NC1)\n",
        "  print(test_image_chunk_AD2)\n",
        "  print(test_image_chunk_NC2)\n",
        "  test_image_AD1 = load_images(path_test_images_AD, test_image_chunk_AD1, height, width)\n",
        "  test_image_NC1 = load_images(path_test_images_NC, test_image_chunk_NC1, height, width)\n",
        "  test_image_AD2 = load_images(path_test_images_AD, test_image_chunk_AD2, height, width)\n",
        "  test_image_NC2 = load_images(path_test_images_NC, test_image_chunk_NC2, height, width)\n",
        "\n",
        "  testimage1_list = []\n",
        "  testimage2_list = []\n",
        "  label_test_list = []\n",
        "  #2 create image pairs\n",
        "  for b in range(batch_size):\n",
        "    testimage1_list.append(test_image_AD1[b,:,:,:])\n",
        "    testimage2_list.append(test_image_AD2[b,:,:,:])\n",
        "    label_test_list.append(1)\n",
        "    testimage1_list.append(test_image_AD1[b,:,:,:])\n",
        "    testimage2_list.append(test_image_NC1[b,:,:,:])\n",
        "    label_test_list.append(0)\n",
        "    testimage1_list.append(test_image_NC1[b,:,:,:])\n",
        "    testimage2_list.append(test_image_NC2[b,:,:,:])\n",
        "    label_test_list.append(1)\n",
        "    testimage1_list.append(test_image_NC2[b,:,:,:])\n",
        "    testimage2_list.append(test_image_AD2[b,:,:,:])\n",
        "    label_test_list.append(0)\n",
        "\n",
        "  testimage1_list_array = np.array(testimage1_list)\n",
        "  testimage2_list_array = np.array(testimage2_list)\n",
        "  label_array = np.array(label_test_list)\n",
        "  #3 make prediction\n",
        "  test_pair = [testimage1_list_array, testimage2_list_array]\n",
        "  float_formatter = \"{:.2f}\".format\n",
        "  prediction = siamese_model.predict(test_pair, steps = 1)\n",
        "  print(np.round(prediction[:,0],3),(label_array))\n",
        "  #4 defines the classification of the image pair depending on the score\n",
        "  prediction_label = []\n",
        "  for c in prediction[:,0]:\n",
        "    if c >= 0.50:\n",
        "        prediction_label.append(1)\n",
        "    else:\n",
        "        prediction_label.append(0)\n",
        "\n",
        "  prediction_label = np.array(prediction_label)\n",
        "  print(prediction_label)\n",
        "  text_prediction = []\n",
        "  #5 compare prediction label with correct label\n",
        "  for d in range(0,len(prediction_label)):\n",
        "      if prediction_label[d] == label_array[d]:\n",
        "        number_of_correct = number_of_correct + 1\n",
        "        text_prediction.append(\"correct\")\n",
        "      else:\n",
        "        number_of_incorrect = number_of_incorrect + 1\n",
        "        text_prediction.append(\"incorrect\")\n",
        "  print(number_of_correct)\n",
        "  print(number_of_incorrect)\n",
        "  print(text_prediction)"
      ],
      "metadata": {
        "id": "tYdKhQR3YeoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate the accuray test result\n",
        "accuray_test = number_of_correct/(len(list_test_AD)*4)\n",
        "print(\"The overall accuracy of the saimese network is %.2f.\"% (accuray_test))"
      ],
      "metadata": {
        "id": "4pj2X1TKYpeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot the result of the first ten results\n",
        "for i in range(0,10):\n",
        "  titel = \"The prediciton of the saimese network is %0.f and the actual label is %.0f. \\nAs a result the prediciton is %s.\"% (prediction_label[i],label_array[i],text_prediction[i])\n",
        "  fig = plt.figure()\n",
        "  plt.suptitle(titel)\n",
        "  ax1 = fig.add_subplot(2,2,1)\n",
        "  ax1.imshow(testimage1_list_array[i,:,:,:])\n",
        "  ax2 = fig.add_subplot(2,2,2)\n",
        "  ax2.imshow(testimage2_list_array[i,:,:,:])"
      ],
      "metadata": {
        "id": "waS-cgaQYr_C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}